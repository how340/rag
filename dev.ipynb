{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fd15d0-9be3-4f91-a986-48fbdc7d924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama serves as the backend to host the LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# packages to help load in the pdf file. \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain.utils.html import (PREFIXES_TO_IGNORE_REGEX,\n",
    "                                  SUFFIXES_TO_IGNORE_REGEX)\n",
    "\n",
    "from config import *\n",
    "import logging\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de02ee8-8ee5-4f6e-87af-f5968b4e156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PERSIST_DIRECTORY = os.getenv('INDEX_PERSIST_DIRECTORY', \"./data/chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e0a385-fd63-4029-a9cb-7f4880372929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file storage for pdfs\n",
    "dir_path = \"file_storage/\"\n",
    "files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f)) and f[0] != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7b8487-0b9d-46c0-9771-4b57d795664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongkaiwang/.pyenv/versions/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/hongkaiwang/.pyenv/versions/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# define the embedding creator and the db initialization\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection_name = 'de-confluence'\n",
    "chroma_db = Chroma(collection_name, embeddings, persist_directory=INDEX_PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69af4f09-d3c7-444e-9f61-3a247cd6d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a462ab-c8d0-4e66-9132-e7470e9a4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dir_path:\n",
    "    document = PyPDFLoader(\n",
    "        file_path=dir_path + files[0]\n",
    "    ).load()\n",
    "    \n",
    "    document_split = text_splitter.split_documents(document)\n",
    "    chroma_db.add_documents(document_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da68d997-94ad-4b94-a05b-1e910138563b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['000002f7-2d5b-4b21-9ed9-81501becc53e',\n",
       "  '0000778a-88da-44db-9209-81f5c0fbe199',\n",
       "  '000615c2-e052-4bf4-98a1-a390330bdd5e',\n",
       "  '00064b86-22b9-4580-a33a-82bf680ccfc4',\n",
       "  '000dede0-96db-4683-a370-8605537f9dc6',\n",
       "  '001370a5-3ba3-4785-92ee-fef8e95ecbc3',\n",
       "  '0015c4a0-5ddb-4fbc-b318-2e50ccc8d46e',\n",
       "  '0016f63c-4865-4091-83ee-badc4d98c7f2',\n",
       "  '001ee087-2bca-4174-8bb6-fe4c4dfb0e77',\n",
       "  '002a11af-488f-44fe-ade3-163e5555c9d5',\n",
       "  '002e5602-c9d5-4f67-b3f5-22126454b0f4',\n",
       "  '0031565d-088a-4749-aef9-835e473f71d8',\n",
       "  '003e33b6-8b5b-4afa-9fd3-a88e7652fe76',\n",
       "  '003f7e31-dacb-43d5-b3b2-34d58697bd46',\n",
       "  '003f8bdc-ab13-41e0-8aea-c52e71a23d32',\n",
       "  '0044d7c2-fe66-4ea8-952f-65253425ff0e',\n",
       "  '00467ff0-82d0-4a27-bb57-97f5b071d427',\n",
       "  '0047d693-edd6-4b9a-bafe-a98035d9c548',\n",
       "  '0049df94-d25c-4753-97e7-9f88378681c3',\n",
       "  '00558c1c-c19b-46e4-b348-ccb795a1188a',\n",
       "  '00598b1f-3637-47d3-b0a9-f2fcb49080e6',\n",
       "  '005e19b5-7538-4a76-8fe4-0b24cd7bcffb',\n",
       "  '0060ae55-9ab8-4c01-8e28-f0b6eba9e01e',\n",
       "  '00622b89-d2d1-43c0-983d-f91b530774dc',\n",
       "  '006705ca-880a-4403-9029-ba55c6d01c91',\n",
       "  '0068945d-4f86-4130-bb28-87a7fb6f8694',\n",
       "  '00694671-be96-4dec-8317-5cc8d8d0e175',\n",
       "  '00697662-25ec-4be1-9294-6e8ba55c363e',\n",
       "  '0069cc5a-ea01-455d-a292-a6c0a82b5a2c',\n",
       "  '006f0eeb-d090-4920-8bf3-373c29c9418e',\n",
       "  '007036f0-9bb9-4601-9687-15fb06d07866',\n",
       "  '0076f4c6-a5c5-41ce-9430-0ccae257768d',\n",
       "  '00777816-9906-46eb-b3e8-99c60706a6ad',\n",
       "  '0078e570-8972-48b5-9ff7-d562333dd634',\n",
       "  '00792383-ca54-40d9-95d4-eb45edcd5e5b',\n",
       "  '00803e47-51eb-4354-b721-bae81ae1c991',\n",
       "  '008597c9-e7bc-4e78-9954-02d3a7032f29',\n",
       "  '008d7fe8-b0bd-4c2a-b2d5-2beb736de094',\n",
       "  '008f5e1f-3e67-4ccb-8045-8427c90a6b36',\n",
       "  '009278ce-80ce-4221-a2a3-09482d6b92de',\n",
       "  '0095f032-1c0f-45cc-98c0-6efc53b43e52',\n",
       "  '0096a4df-10b3-46cc-9124-0030c8046afe',\n",
       "  '0097a399-3365-4422-8667-733357433233',\n",
       "  '00a3379c-3374-4c69-bd19-1211d9c66453',\n",
       "  '00a4c084-5024-4a72-be42-ef6a9bd2c359',\n",
       "  '00a7e760-f30c-4666-a94b-094d2b3f7565',\n",
       "  '00ab0ddf-04fb-40f2-88b2-0d555430aa62',\n",
       "  '00b384a6-03ae-4c39-b537-f912eaf4b67e',\n",
       "  '00b96489-14ff-41f3-aa74-4368a996aa0f',\n",
       "  '00bc23c8-58a4-440e-904b-b4575448a261',\n",
       "  '00bcf286-68b1-42b5-ac8a-3fe67a4e77e7',\n",
       "  '00c1a398-2876-45bd-9734-c572ba88c52e',\n",
       "  '00cd1002-cb65-4dc7-b414-2fba06565cdf',\n",
       "  '00cd89b4-e85c-4eac-95df-75f1a32a221c',\n",
       "  '00ce866c-5ed2-42bf-945d-c11c0e9c1967',\n",
       "  '00cf48e6-7bae-4122-b2e4-e649bb82650b',\n",
       "  '00d0f75c-a401-49db-9930-3fb82c7f5cbc',\n",
       "  '00d1675d-f57b-49f3-bcd4-691d3d93eedc',\n",
       "  '00d4168a-b5bf-46c2-8167-10e4d636d759',\n",
       "  '00d7bc8a-1b44-47dd-9159-e54a7323476c',\n",
       "  '00d95444-deeb-4883-b81e-0aec9a7fe49d',\n",
       "  '00e61e6b-2887-4d92-aa86-0d4769fcea0b',\n",
       "  '00e91497-15b1-4a64-9e87-1deb165d74f8',\n",
       "  '00ed9162-40ce-4d09-bf06-60ba4d404cb2',\n",
       "  '00f911fb-e5fa-4bd6-ba56-befc8378b5f9',\n",
       "  '00fe45c8-3a0f-43c7-ae37-642c0a4ecca3',\n",
       "  '01015728-a1ef-4396-9070-133ec31fe4d6',\n",
       "  '01110dda-2e2d-4b1d-8056-d4d6332ee0d8',\n",
       "  '01174d35-a66b-4fa7-b3c7-8f180fba664a',\n",
       "  '01179f3a-c038-4cfe-b330-9858973568fd',\n",
       "  '011a7646-7537-4d74-b34d-89b42902a5c2',\n",
       "  '011aaf5c-6b6d-4dc6-b8e9-bea7e0604fdb',\n",
       "  '011b4410-55e8-44c0-a9a4-4cc14db2e152',\n",
       "  '011c46f6-26bd-4e62-a054-bd2ac279a755',\n",
       "  '012170b7-edb8-4de2-be13-c32c6f6f0db2',\n",
       "  '01274b75-3244-48d9-a3d9-bece97612a75',\n",
       "  '012b52d5-7a83-4b1c-af24-fe53d522d302',\n",
       "  '012e2a6c-cdac-462c-a2e5-0f927afed1bf',\n",
       "  '0130c363-6a75-40c3-b943-767ab1a3e88f',\n",
       "  '0132843f-4c7d-4c69-935c-c83cd5e5ca79',\n",
       "  '0132acfb-8b5d-432a-b752-1fc38026c96b',\n",
       "  '01368841-4622-48d8-b2b5-434017f55c51',\n",
       "  '013738e5-41c2-4984-a0b0-92c3d601c8ef',\n",
       "  '013a0668-6160-47b9-ae5f-7ba7ddd4fefd',\n",
       "  '014353ed-7de0-45a9-ac39-ad2067d1d455',\n",
       "  '014900f9-ba45-464d-9b6d-dfe8fba4e21c',\n",
       "  '01495ff3-23f5-4213-8f6c-b8ce3c43c2f2',\n",
       "  '01498f3f-4955-4be2-a0d2-087ba7b338c6',\n",
       "  '014aea64-1350-4549-9636-170234279059',\n",
       "  '014b66b8-f217-4e87-a0f1-c1e176b40989',\n",
       "  '014ece88-1318-429a-b84b-565d2005c1b9',\n",
       "  '015836c2-2b34-4b35-b7c9-b2db03506578',\n",
       "  '015a7cc3-7c7f-4a07-91ab-b4803b9ab410',\n",
       "  '015b028d-8bee-4e15-8eef-fdc65f423277',\n",
       "  '0160769f-ab88-478d-b1cb-24da8496f3cb',\n",
       "  '0162dfd2-2ff7-43b9-92a3-6aa0c6027b0d',\n",
       "  '016417d2-592f-45aa-9e98-6170b47d17ce',\n",
       "  '016561ee-77d7-4eeb-ab1a-96ef5d7c1c3b',\n",
       "  '016796a8-fd67-4a65-87b9-bc030375b19c',\n",
       "  '0167bab8-3aaf-4f19-9d3e-d8236fc3b1e9',\n",
       "  '0169e920-d9fb-4116-8377-86e1ac0489c7',\n",
       "  '016c4564-4d5e-4f67-bf47-f4b1f8f35c8a',\n",
       "  '016e5099-d471-4687-85b1-ab053cd4d36c',\n",
       "  '017b117f-b1c0-4290-b050-8010a6739651',\n",
       "  '017e4862-77e6-4fab-b0ce-e7001b47b77e',\n",
       "  '017e66ca-1af4-471d-9be4-99b74f0fb753',\n",
       "  '01864744-18c7-4e9b-b994-eaa596b92f1c',\n",
       "  '018df50e-3045-4f06-b636-b2a694a003da',\n",
       "  '01923e63-8510-4115-90d6-723db402595c',\n",
       "  '0194cf8d-f7ce-4eef-9100-7c18e96425d2',\n",
       "  '0194d3b9-e9ff-471e-8e33-65e031fdefea',\n",
       "  '019b969d-fd4d-4c27-a359-0d34045e16e5',\n",
       "  '019cf365-9306-42a8-8e0a-a86d2df19413',\n",
       "  '019e7b8f-7749-4d70-9052-99bec2aeeee3',\n",
       "  '01a1b54e-d308-4d5f-8946-d7f8b32f423c',\n",
       "  '01a6c8e7-dcf0-43d4-b76f-f8f5d50886e8',\n",
       "  '01a7fc48-b64e-4dd2-a25b-6b8bfcaccb1b',\n",
       "  '01aea078-b118-489b-96dc-8a0984f81120',\n",
       "  '01b4b4fb-b704-4ceb-9a2a-b0814f58f04a',\n",
       "  '01b5dc54-7fd4-43eb-adec-ef6ce1570b07',\n",
       "  '01b8b9be-c7c2-4967-bd3f-b1f5d0548074',\n",
       "  '01baf53f-18b8-449a-920e-ddd7908b0c4a',\n",
       "  '01cd7b00-1a8b-4a77-a452-92169519c93f',\n",
       "  '01d1e914-b42a-462a-ab29-99a2052f934b',\n",
       "  '01e50f15-ecb7-4fc1-9343-d0e712d8c75e',\n",
       "  '01e75da0-fbdd-4f9d-b271-ae3741810d14',\n",
       "  '01f04fe1-da90-4be2-b0c5-b32984839861',\n",
       "  '01f0f6d6-4b59-4fa8-a359-74d5485b00cd',\n",
       "  '01f3fca7-90a3-40df-9445-2de9c947d2ec',\n",
       "  '01f59155-8fcc-45c3-972a-81a3dc0ff8b5',\n",
       "  '01f7066e-9418-427a-89ba-85257c6ba54b',\n",
       "  '01f8d6f0-33c8-40d1-b8a8-9311a0c19901',\n",
       "  '01fdce29-396a-4223-be9a-4a8764b7b2d0',\n",
       "  '01fe5f52-b814-446d-b547-12cfb496f63f',\n",
       "  '01fed62c-bbe0-4bd0-88fe-f56c5990fe24',\n",
       "  '0200df61-4265-4c07-ab87-725d3d5b46f6',\n",
       "  '0200fdf6-0f7f-461c-b6fa-1477c3028a70',\n",
       "  '02015a1f-e9b5-4783-89d0-2ffc916f7372',\n",
       "  '02015e21-4e0f-4a72-9366-6055dfadc15e',\n",
       "  '02055abb-bdd7-4b59-ab99-33c71690d97d',\n",
       "  '02097d96-d098-4eb4-af2b-66beca7f7372',\n",
       "  '020ac10f-5e8b-44a6-821e-8a834a33c969',\n",
       "  '020fcc05-5183-4e3a-9164-f5f6ff92d0b8',\n",
       "  '02113d4d-cf94-4537-a691-097a741fde3a',\n",
       "  '02133ca7-244a-41e4-ac52-bc7822f204e9',\n",
       "  '02175094-fb50-496f-8874-25885762ac47',\n",
       "  '0217a0b3-8d75-4cb0-b120-11b42deab229',\n",
       "  '021bb3a0-c985-47cb-be28-5546e7295e2d',\n",
       "  '02257920-e8ac-43e7-b6d8-4f52a93598e1',\n",
       "  '0225f5c0-dca2-4d3b-a1fd-4a4d1188595e',\n",
       "  '022628a5-6af3-4970-acc5-bb9c8a86365a',\n",
       "  '0228081d-1dd6-45eb-8ea7-656385ce58ee',\n",
       "  '02298db1-4003-4986-b1f9-cb56ddbcd133',\n",
       "  '022b9585-03f0-4850-aae1-c451055f811c',\n",
       "  '023070c1-ba91-467f-972f-ab715751859a',\n",
       "  '02312204-786d-4209-843b-6990d8219543',\n",
       "  '0233723e-73d3-4054-80b9-cd7dccdbe28c',\n",
       "  '023c96f6-da56-4282-a976-30e28bf38c2e',\n",
       "  '023e0a9a-fefc-4654-b108-c142f5430623',\n",
       "  '0240a6ac-59db-4cff-98b1-01ee104e9fbc',\n",
       "  '0241fb9d-ae90-437a-9d4a-62286a74708b',\n",
       "  '02435e31-fe75-4d79-b691-4518b9cf22b1',\n",
       "  '02439bcf-2c36-4ba5-b33c-c57d3d2a690a',\n",
       "  '02448e2e-2dc0-4bcf-9e13-3dac146898ad',\n",
       "  '02457de0-0fd3-4fb5-9b81-b4ba456e4535',\n",
       "  '0253f91f-10ba-4f16-b21b-1d76c9b504da',\n",
       "  '02544131-2e5d-44f0-b250-7b37ba9f916b',\n",
       "  '0255cabe-3ad2-4771-a0c7-d529e7679e7d',\n",
       "  '02588f97-7e38-49b2-a6d2-8eaaf7807461',\n",
       "  '0260b1da-ccb3-45b8-b3e0-e94f46c35909',\n",
       "  '02640541-562d-4ca5-a11e-c6659ede5284',\n",
       "  '02657a2a-a8ba-43a9-90e1-83f337815a3c',\n",
       "  '0265e811-4523-4ee1-bc1c-abe0146833eb',\n",
       "  '026d74db-87ac-4035-b9dd-bc1a89d39d39',\n",
       "  '026eac9b-f797-4533-a244-755b53bc170d',\n",
       "  '02743939-2f0a-4c61-a469-a55b8cf5976b',\n",
       "  '02765559-7158-45c1-8782-bc7381156cde',\n",
       "  '02768fb4-9a0f-48c9-91cf-466c462233af',\n",
       "  '0277793f-451b-4d33-90ba-9fc89204f29e',\n",
       "  '027eca80-5e8a-42e5-8041-1c9049aa1ee3',\n",
       "  '027efa19-3128-4ed0-8d72-09d8f9935ff7',\n",
       "  '027f6297-3c2a-469c-ae7b-89105d48d604',\n",
       "  '027ff788-e75c-4ab3-8ea3-1565d31be6cb',\n",
       "  '0281593f-f189-4828-8f71-d91c1a561df9',\n",
       "  '02828334-0a51-47fb-ad7d-1d7149a989d5',\n",
       "  '02829959-a77a-4ad5-b101-74f914f4306c',\n",
       "  '0282a9d0-be75-4805-9574-c46cc1708683',\n",
       "  '0285178c-915b-45de-a217-206f03f94ef0',\n",
       "  '028662d0-20ff-4190-8c60-f1eedad5c121',\n",
       "  '0288c979-db53-4131-ae0c-faf6def69256',\n",
       "  '02953a68-6e3d-4b34-9439-7f655129713f',\n",
       "  '0296eef1-45e1-4cc5-99a8-a7711f9dc8ed',\n",
       "  '029c2c36-3d7c-430d-9825-b2a012b2b719',\n",
       "  '02a66856-b0fb-4c50-9f89-0e9d5253c2b0',\n",
       "  '02a6a19a-9e41-4f72-b2a7-f2b285ca6a99',\n",
       "  '02a8b195-b442-49d4-a716-e4280d536c51',\n",
       "  '02aee2bd-06d1-4275-8d99-83242bd07bb1',\n",
       "  '02af932b-f4a6-4b25-ae3d-b5534f8d707c',\n",
       "  '02b077f3-9af8-4098-9615-0af95c58ba8c',\n",
       "  '02b4630b-c274-45cc-8906-c557240c048e',\n",
       "  '02b8d65b-2b58-4989-933d-ac4ecba38aa6',\n",
       "  '02bfffba-8590-4d26-857f-bf4135ff3424',\n",
       "  '02c31ccb-dfa0-4e09-825f-6d7e7d75b0d7',\n",
       "  '02ce0872-a7d9-4a9d-b158-43bde6fb65ef',\n",
       "  '02cfb663-9e81-4743-a461-439e828a2719',\n",
       "  '02d03197-d18b-4969-8111-a1a7d62bf054',\n",
       "  '02d58336-27e5-4ce2-aa58-b10b433b139c',\n",
       "  '02d69f3c-8f41-4b47-9d28-a861513238aa',\n",
       "  '02da9f73-2058-46c8-a1c6-a47ca5f9aa4d',\n",
       "  '02dc4cd1-5a67-4539-95dd-9fd03b38702a',\n",
       "  '02dd86ec-749b-488b-93d1-f6c8806223a9',\n",
       "  '02e23109-a2b2-4707-9b47-e41739ab8813',\n",
       "  '02e5d7f3-cffe-49e6-bb67-894a6f744dda',\n",
       "  '02e60fcf-e2c1-43f2-9243-c13a4cfad065',\n",
       "  '02f1c82f-aea2-4cbd-9f36-45a48d4a0a68',\n",
       "  '02f57027-f97d-4ad0-91dc-a003f1bf8417',\n",
       "  '02fc90ca-a2aa-47f7-a4ec-6cf36367760b',\n",
       "  '03028f21-655b-4617-953e-797295cc47e6',\n",
       "  '030351cd-f49d-4987-99f7-6c5c50d2d9f4',\n",
       "  '0303de29-234c-4b58-aa99-06d179def76e',\n",
       "  '030415fb-2639-4d9e-bcf2-319dab835a21',\n",
       "  '030de45e-2787-414c-ae7e-dae25d71646c',\n",
       "  '0316a126-c320-4069-b60f-5c6f60fe7f28',\n",
       "  '0317adbd-66ce-468d-b39f-71cc8a8f9a29',\n",
       "  '031a09e2-a563-4754-9c7e-dc7d2fa6215b',\n",
       "  '031a9c42-d1d8-4a90-8376-acda9112481f',\n",
       "  '031b22b3-3118-471a-adfe-49bee6ce71e0',\n",
       "  '031d9131-050a-4885-aeac-d9a5b0ce94ae',\n",
       "  '0321d1e8-ef86-4fc7-83e3-ba893c576988',\n",
       "  '03243652-4201-4bc8-a823-b4885620b484',\n",
       "  '0324e199-d0f0-4e74-a865-5e63e484dd57',\n",
       "  '03270d3c-ddf7-4d1a-b725-8a75b3600368',\n",
       "  '0335d64f-02e5-4d0e-9427-caf6d7f62803',\n",
       "  '033893fe-b93a-4f7f-9e1a-c6b08443ba10',\n",
       "  '0340f7a9-dfe9-4704-87e2-7a0621e03c81',\n",
       "  '0345c151-afb0-44c0-b11d-338f3ce6a176',\n",
       "  '0347d5f5-b412-4d11-9458-5f388621ecfc',\n",
       "  '034b2b20-3914-4883-b18e-b3de72a33f3e',\n",
       "  '034d3d7c-c6d2-46f8-8f04-0dc637ee192c',\n",
       "  '034f568d-2ebe-45cf-a211-d5e39d4353a3',\n",
       "  '03514446-fa6a-4653-993b-2a9e6301dffa',\n",
       "  '03529d89-3eaf-456b-aeba-96c890bbbc3c',\n",
       "  '0354da5b-8c53-49eb-9bfb-91596cd302b1',\n",
       "  '03560d3f-727b-47b3-8e40-da4ff435301a',\n",
       "  '035a32e9-64f8-40de-bffc-829d963fd55f',\n",
       "  '035fa0c6-898f-468e-afcd-75eb985beb39',\n",
       "  '036203b2-1229-4884-8f86-cdfbf40e6c05',\n",
       "  '03631d7f-507f-471a-9614-4f7597e98fc7',\n",
       "  '036b8c4d-983c-4e06-9070-b1382ae023ca',\n",
       "  '036d4e5d-2d8b-4110-97a1-e1dbee1319fa',\n",
       "  '036e488d-b2fb-4146-9dba-b33d5160c99e',\n",
       "  '0370db5c-f088-420b-bb24-f0cf87bf0953',\n",
       "  '0371069c-2283-44da-9e6d-0e999877df9f',\n",
       "  '0372cf25-f2e5-4748-9756-705c5ece0f4f',\n",
       "  '03794d51-d13f-4a60-b58f-46a4b1ef25de',\n",
       "  '037d1a00-eb23-403e-9dc3-131afe1e33be',\n",
       "  '0385dcc9-d1bc-4d62-b011-a290a48bd141',\n",
       "  '038bc809-768f-4b7f-859e-6d967dae6282',\n",
       "  '038c56c8-3cf2-440a-8490-9129326b70b3',\n",
       "  '038d2c9c-6df9-4297-ad87-f1fab1c155a0',\n",
       "  '03918f00-4cc0-4bb7-a750-1224fb23baee',\n",
       "  '03938250-2288-4da1-a2e9-68e7653c9d8d',\n",
       "  '0398603c-7d5b-45f4-8174-d71be53711d7',\n",
       "  '039f46d8-b355-44b7-9e2b-c802357297f0',\n",
       "  '03a5b46d-10d5-402a-b43b-9918594da2f0',\n",
       "  '03a6d9ad-7205-49cc-ab64-040a931439f4',\n",
       "  '03ab7ab3-5600-42dd-ab0b-784121abbd4f',\n",
       "  '03ad1efa-2840-47ea-909d-fc86608b72c0',\n",
       "  '03afc410-48a1-4ed8-a673-2638294c16ad',\n",
       "  '03b3b6d9-860c-46cd-b926-26ba9a5ad684',\n",
       "  '03b48bd5-f5a4-463f-8d15-0f1335b93a65',\n",
       "  '03b50948-b639-4d3f-bd5b-5e041ee1e262',\n",
       "  '03b6fcdd-ddd5-4543-8d94-c625c5910b5c',\n",
       "  '03b86243-418f-43e7-9cb3-fc77e80dc619',\n",
       "  '03bd09eb-3ee8-4e3f-9649-0add21283bb7',\n",
       "  '03bf076d-7a28-4a89-891c-2ca177058df2',\n",
       "  '03bf7708-cf66-4d63-b647-0e17e4775144',\n",
       "  '03bf790c-a531-4ce5-8248-08dc57499b96',\n",
       "  '03bfa56b-21d1-4203-9ef3-95480f57ed58',\n",
       "  '03c2978c-b7a2-478c-8974-756608b75c74',\n",
       "  '03c4a874-c43f-49e9-91a8-ebebb7b30801',\n",
       "  '03c75851-5685-4e35-a252-78b73762f9b1',\n",
       "  '03c75c1e-9519-4ab3-9134-cfaf4c2c72bd',\n",
       "  '03cb33d3-8de7-4ff2-ab96-acfb35dcf210',\n",
       "  '03cf2338-b756-44ab-b413-95a3bd3fd21c',\n",
       "  '03d0dfef-2689-479d-a604-fd6c2e111a0f',\n",
       "  '03d17031-66ed-4d6e-8fc8-2935c12a0263',\n",
       "  '03d18b43-1448-43b4-ab48-75d3b54a6522',\n",
       "  '03d5645c-43ff-4e62-89bb-d27560795a59',\n",
       "  '03d6262a-fea1-48a8-8c20-d7b24cd5e648',\n",
       "  '03d79512-b165-4dc6-be85-b5f35477b31b',\n",
       "  '03dad3a0-638e-42a1-8c5b-b82f5a8dc5c1',\n",
       "  '03dd0697-190c-4117-accf-75e0a133d94c',\n",
       "  '03de4fe7-e743-4b9c-85da-a7f187139694',\n",
       "  '03dec314-853e-45a4-ae14-bcb91f8eeb88',\n",
       "  '03df04c7-7811-43ea-833f-e0d155944009',\n",
       "  '03e4fd79-4405-4f8b-87af-318d728d4b53',\n",
       "  '03edc4df-610e-4f7f-af3b-770463525fa3',\n",
       "  '03f19e1d-ebc2-4467-918d-5a3dd298cd4d',\n",
       "  '03f9f26c-b506-4b5e-8487-571a413be7e4',\n",
       "  '03fcb40a-e174-4dae-9d39-fba3b65c215a',\n",
       "  '03ff1e3b-6e92-4919-852d-b45a8e07cf66',\n",
       "  '0400393d-d394-4696-9626-a9cffdc46aa7',\n",
       "  '040a855e-d735-4771-bb05-17bd7a1a63d2',\n",
       "  '040e69f1-acf8-4515-a8a4-d2522f480294',\n",
       "  '040ebbfe-b0f0-4eb5-8ac2-a8b6d17d910d',\n",
       "  '0410aafc-5ff4-4fb0-94c8-26c3fd4af929',\n",
       "  '0418be2f-ace5-4bc0-a399-ebdd5d04b555',\n",
       "  '041a167d-1f09-463d-a632-680cbfa2bb00',\n",
       "  '04204b21-312e-46c7-986b-9845ad18d813',\n",
       "  '0421a8d5-3788-4433-9a29-3a98d2146c7b',\n",
       "  '0422fa54-cc2c-4d4a-ba9e-bc1c6f3a2f3a',\n",
       "  '0425a719-98db-4b47-b8a7-f15a13e5c1bc',\n",
       "  '0425f18b-a9f9-44fa-b854-31f06df86a86',\n",
       "  '04268d6c-9974-457c-8c08-113a513889a7',\n",
       "  '042a008b-836f-424a-b2df-021492353add',\n",
       "  '042a6a7a-c62d-4d14-9b8a-3d003fbb3666',\n",
       "  '0430e0be-385f-4d23-be4f-bee37a92981a',\n",
       "  '0430f4e0-326e-43ad-8908-53843b53fb80',\n",
       "  '0435dc86-3db8-43a7-ab69-ea65531adc2e',\n",
       "  '04378cad-d640-4ca1-8b36-d4ede538118a',\n",
       "  '04390ce3-c3cc-4f83-8e6c-f5127be52111',\n",
       "  '043cdec2-7b6f-4a6e-a2c3-0c90ae8844c3',\n",
       "  '043ed3e2-d718-4e8d-99ae-31bf37afc6d5',\n",
       "  '043f825a-fcaf-462f-801e-4fc30133a02e',\n",
       "  '044008fe-f1a5-4362-bb0c-7f5bf81f93c6',\n",
       "  '0441331c-5041-4dc5-9ed2-db939908b513',\n",
       "  '044a123e-cd38-447e-a48d-8125df16faa1',\n",
       "  '044a61b4-7641-44cd-88e8-605f22ce5485',\n",
       "  '044ff068-96c9-4d6e-bcc0-25268a15f2d0',\n",
       "  '04530635-b9a8-47b4-a86c-5d035360f08a',\n",
       "  '045b095a-580e-4ae6-bca0-eaff1062b138',\n",
       "  '04609ced-8a8a-45f2-9063-e0096e3d7140',\n",
       "  '04614fb0-30a9-47fe-8a13-b33248cb338b',\n",
       "  '0464c645-884b-47a4-87a2-26bebd58c44e',\n",
       "  '0465faaa-a2dd-421c-8652-5f7e6a9b6163',\n",
       "  '0469b7a0-398c-474c-a8d5-5c5ae375b249',\n",
       "  '046ab03e-f84f-4ced-b95d-970b185f80e4',\n",
       "  '046f0b85-34ac-428d-8f7c-a6bc6e26331a',\n",
       "  '046f3bba-ef36-425e-a174-67da22ebddf0',\n",
       "  '046fb8d7-e774-41a3-a8d1-34973b8f7711',\n",
       "  '04713d40-9fbd-4581-af7b-3c31be71ecb4',\n",
       "  '04723c8f-f131-488c-87a3-e557fb1b31eb',\n",
       "  '047c7492-5dfb-4112-a0f6-4a4c41479680',\n",
       "  '0481aeaa-a744-4a65-8085-26a581672113',\n",
       "  '04831775-d171-4ed9-a1f6-13afecb676ff',\n",
       "  '0487f50e-8175-4d48-a847-e13db0419f54',\n",
       "  '048a4431-53a0-4cad-8fbf-31b334dcee2c',\n",
       "  '048ceed8-a3b1-465a-80ff-2ccaa8c36aae',\n",
       "  '048efa58-b0b0-4f6d-9198-e1100918fc13',\n",
       "  '048fda9d-5384-4150-b7b8-ffc6b3942e17',\n",
       "  '0491248f-f227-4289-86ae-1c6ebf2e8b6a',\n",
       "  '04917e4b-db04-4cbd-b08d-5b614654e5b7',\n",
       "  '04922a75-6490-41e2-b784-b0ca8e826a7a',\n",
       "  '0495b5df-0e9f-4aee-a1ce-e8d331e16978',\n",
       "  '0495d9ee-3543-4e3c-8391-358277baa635',\n",
       "  '049b8fde-2b53-4549-93e0-13e99d73c6c7',\n",
       "  '049c7a3f-91f7-4c2c-90cd-20c9256e224b',\n",
       "  '049d4b55-5794-46e4-b576-8362b61401ae',\n",
       "  '049e8c22-e8a8-4d75-9edc-a6b43b9efcf9',\n",
       "  '04a3aae0-d6c6-402d-b909-65649d3a102f',\n",
       "  '04aa019f-cc31-4031-b109-99055872d519',\n",
       "  '04ab62df-b74b-4599-bf59-c63434b0cd29',\n",
       "  '04ab6705-d78f-4193-a881-78052db3320f',\n",
       "  '04aeaf3d-a282-4bbc-bda1-1f3deccdb970',\n",
       "  '04af03e1-4684-49e9-9d73-67c78e607e45',\n",
       "  '04afa185-c6c9-4ede-a8e9-fc7450f23390',\n",
       "  '04afbdb8-fb3a-43cb-ac6d-e829f7051e89',\n",
       "  '04afe574-0548-4fc4-8a6d-a53863c2b87a',\n",
       "  '04b2c71d-8fb8-4061-bf97-3cd4b3c9f30c',\n",
       "  '04b7cbce-eb92-47b7-bb11-4859a98ea05c',\n",
       "  '04b8526d-b50f-4d19-a97d-15d628047d8a',\n",
       "  '04bb12d2-5543-40a6-890f-45d4c919e2cc',\n",
       "  '04c705c9-1141-44e2-8995-6bd700c079c1',\n",
       "  '04c8454d-0bf7-4490-a91c-848b2af79436',\n",
       "  '04cb0d71-24f0-48d5-8948-0bec8be00eb4',\n",
       "  '04ccb28f-11f5-45c7-9cc2-1719ab641909',\n",
       "  '04d377ed-3d78-4303-a815-5aff01f079c5',\n",
       "  '04d586a5-6a9b-4b58-8cd8-2a3fec058bc5',\n",
       "  '04d7d8fa-849a-47ac-bdce-ee26be330ea6',\n",
       "  '04d88a28-88d4-4dc1-aa92-8a14795389cf',\n",
       "  '04dd0a66-c437-4cbe-bcc8-394446f17d6c',\n",
       "  '04e2006b-5fb7-42bb-97f1-6109fc5ff83f',\n",
       "  '04e7839b-f9c7-4649-9019-7d72dea1c4b0',\n",
       "  '04e8c852-0c74-4c91-9499-2ae465a4ea57',\n",
       "  '04e8d573-0f1d-41df-adfa-a124f403bcf4',\n",
       "  '04ea4495-7b87-432e-b3bc-6929a1c56d8c',\n",
       "  '04eb4b01-2561-4688-9e9e-a79b561cf1c4',\n",
       "  '04eff82f-cd7c-47e0-aa35-4d919360214f',\n",
       "  '04f7d9e3-91d7-4601-b9f9-38d51dd98a9a',\n",
       "  '04fa9ef0-6acb-4828-8438-93dce242ad55',\n",
       "  '04fdce67-9c29-4edb-93b0-91ee2071a833',\n",
       "  '0503484d-7a48-4651-9204-c71a3b505a94',\n",
       "  '050435d7-304e-4501-a6b4-371924ddf37f',\n",
       "  '0505907e-06df-414d-93c8-fdbe2a2aeed6',\n",
       "  '05075934-688f-4f33-aa00-a61c08eceb27',\n",
       "  '050c73ce-6bb2-4305-b778-00f6591167e9',\n",
       "  '05162caf-a9fd-48b0-96db-b851d5776823',\n",
       "  '051aa0b7-9d6e-4d8e-82eb-ab94c08b9830',\n",
       "  '051bb52c-877b-46ee-8e96-4a016c454a50',\n",
       "  '05267d88-097c-4af0-9a27-f9df1fefe8df',\n",
       "  '0528316f-7c1c-4b08-8415-f4a22abbbe35',\n",
       "  '05298895-4ec1-4106-958a-b2ae78491830',\n",
       "  '05299c50-adba-4fe8-9f1a-719e995657f8',\n",
       "  '052cc5cc-e207-462a-8172-efd476fae0c8',\n",
       "  '052d160a-4a9c-4e2d-a0d9-c4cf959635bd',\n",
       "  '05312c82-5323-4c86-a2cb-51b94cb29864',\n",
       "  '05353188-c16f-465d-8ccb-057558a0f0ba',\n",
       "  '05353d32-c869-464a-8a93-24bfb6d7548e',\n",
       "  '0536d37a-aaeb-41e1-b12c-cfb23d059b4a',\n",
       "  '0538d6c9-c089-4b8a-be4c-dfc7a07ca5e9',\n",
       "  '053bedc9-cdcc-4f6f-8842-1a59b7b3877b',\n",
       "  '053d56ea-d051-42c6-8e40-0a1360de3757',\n",
       "  '0542552c-feee-477f-83a8-a8210f6386e3',\n",
       "  '05469803-8e6b-48ad-bc0b-b6777892cdcf',\n",
       "  '054857ec-21cb-4e45-88dc-26ef568729fc',\n",
       "  '05492fc8-acbb-4bf1-a161-f2bc41978712',\n",
       "  '054a7e16-4f6c-4291-889d-920e5553faf2',\n",
       "  '054d0e8a-2fb7-47e0-9426-514a3f0c551a',\n",
       "  '054e5c9c-5cd7-4055-a009-a8aa9e510cd6',\n",
       "  '054eac69-c7ba-4f6b-a0ac-d3465fbf0619',\n",
       "  '05535ff4-456a-4b36-b11d-0394ad9ba53c',\n",
       "  '0557b3f3-9b13-490c-bdc0-6c239a395c6a',\n",
       "  '05594d8d-665c-49ee-a262-756d077ef0f3',\n",
       "  '05601501-ce8a-4cd2-8823-2ec55858c6f7',\n",
       "  '0561fea2-87a2-48f6-b91d-507480a3fafb',\n",
       "  '0562cba3-6601-4dcd-8686-14fecfd77b13',\n",
       "  '0564bd90-c10b-4f7b-af2b-55af2a0ac74a',\n",
       "  '05662238-19db-4e6a-9d4a-7a17ddf6fdde',\n",
       "  '05666b2d-f1d0-4608-b3f1-72f2cb462fbf',\n",
       "  '05670f1a-c671-49bf-b18c-10a6f0e3ba3c',\n",
       "  '05683cdd-bd3d-4475-bdb0-7054d31fff36',\n",
       "  '056fd83c-5b36-4709-89b0-b5e8c88629c8',\n",
       "  '05723558-bc1b-4f94-9d2b-2ac5039d1d93',\n",
       "  '0575306e-85d4-432d-aa59-9aecf4f737a3',\n",
       "  '057dcd18-15fe-4822-843c-0f1fd7682d8a',\n",
       "  '057de464-f2b5-46fa-a5d4-86221eb0bb0e',\n",
       "  '057ed08e-4bbb-463f-b776-bc6cba312d46',\n",
       "  '05929340-8390-48a7-b1f5-c0b3d14b21dc',\n",
       "  '05945ce1-1b45-4265-8363-4427aa8cb7e9',\n",
       "  '05972d07-df9c-471c-a0fe-19d5b432604f',\n",
       "  '05976e50-5d2d-486e-9a0b-9fd67391c939',\n",
       "  '05993307-3d47-4123-ab9a-3eaa24cc5a32',\n",
       "  '0599780c-575a-414a-a641-432af000c5a7',\n",
       "  '059c32b2-3534-42a0-b878-ed4f1b0db008',\n",
       "  '059e2422-54c1-4c27-89fb-2303ec26db03',\n",
       "  '059ea4a6-b28d-4b95-85dd-880445c5b6c3',\n",
       "  '05a0b5ab-f5a3-4dea-afef-01b386251fdc',\n",
       "  '05a36740-dd8c-4eb0-9fbf-a2d020efe831',\n",
       "  '05b0dee9-100d-4c6e-a97d-a1a38583d2f1',\n",
       "  '05b3b4f5-27fa-4efc-8db6-201453bc441c',\n",
       "  '05b47a82-082a-4e02-8d38-767a60ec5ce2',\n",
       "  '05b88081-b779-4a5e-b7be-a46b6c4a51b3',\n",
       "  '05b9db39-3e3c-41e8-9053-6d29277254ef',\n",
       "  '05bccbd8-fef6-418c-9f1a-42a9519fdeb0',\n",
       "  '05bd7ec0-7323-432a-b3ab-d88c66a85fb0',\n",
       "  '05c58060-710a-4b07-b992-941167cb4e75',\n",
       "  '05c5e653-c2cb-4129-bff1-68f13c9b2eef',\n",
       "  '05c945e7-6750-486c-9a64-74edc2d95d96',\n",
       "  '05d28760-f7ea-4e07-9b90-f8a13b2d3f88',\n",
       "  '05d68a28-7a6b-4d64-ae53-b44d36931e27',\n",
       "  '05d830fb-e984-46df-821c-afc4e95dfe85',\n",
       "  '05e36dc3-bbb4-4669-b48d-49bf7d181659',\n",
       "  '05e43e3e-bc66-4e6f-9da9-e677e70e93fd',\n",
       "  '05e474b0-428d-4dc1-9508-0780b1c05ed9',\n",
       "  '05e5b93a-0e98-4028-af8c-c4e21a307cfa',\n",
       "  '05e68680-53c7-488e-8154-ac0168c85097',\n",
       "  '05e727b2-1fc8-4f10-9315-5d44b53bd14a',\n",
       "  '05e9d341-c9ce-4625-ae68-f6e9863a31f0',\n",
       "  '05ee67e9-535d-4539-88b7-68b24af5c8f6',\n",
       "  '05ef8a43-4b8b-45b7-8359-6b9faf858d1d',\n",
       "  '05efc05b-a75d-4c6c-9ce5-71aea66a4665',\n",
       "  '05f7f327-fb10-4e85-a672-d77d1ea6f460',\n",
       "  '05fd5ee3-64e4-4bc3-9698-b0062fea0b9e',\n",
       "  '06004ce7-e2fe-4df0-acf2-d5acbca5d6b2',\n",
       "  '060ae599-395f-4329-b224-b10ebfef54b1',\n",
       "  '060b8348-aaa4-4f92-8aeb-6d7a49c28709',\n",
       "  '060e6270-5f59-4251-9641-233a985f6b31',\n",
       "  '060f7544-8510-4ed2-a12c-0277f321ba29',\n",
       "  '06107e88-614b-4664-a4be-368f8dd33767',\n",
       "  '06145f3f-8052-4a68-8300-ec8328cd1602',\n",
       "  '06157835-53d9-49d8-b536-a1fa1d0c3c1b',\n",
       "  '061e0a53-c2b1-46a9-9473-b7a40156f2c3',\n",
       "  '06202b11-5c1a-46c4-890a-361f25165824',\n",
       "  '06208d61-383c-4685-bd7e-ea1edafd9490',\n",
       "  '062ecfce-a8b5-4ea0-9d7c-500482f78d2e',\n",
       "  '0637b1c8-d431-4c46-a7a1-e90c1ea69235',\n",
       "  '063cae42-b0fb-4db7-98bb-8e96084ca2ad',\n",
       "  '0641921f-2bde-4595-ae27-5546e1ad4c89',\n",
       "  '064952f2-206a-453a-b26d-7fe411375547',\n",
       "  '064afbcc-8e1b-4f3c-9fe4-49244c270397',\n",
       "  '064b783b-ca8b-4622-a2c9-442975355060',\n",
       "  '064faba0-64ac-43d8-9a2c-1da66eca1a75',\n",
       "  '065107b8-fbba-4954-925f-4cae80b94a82',\n",
       "  '065545db-6d9f-40ff-b48d-b81c369487d0',\n",
       "  '0657a2b5-cc2a-4b95-bf7b-f9b46209bab5',\n",
       "  '0658e9a1-710b-47f3-a1cf-727c80519fd0',\n",
       "  '06597caf-dd55-4325-a791-bf70c5c7a731',\n",
       "  '065d1cc2-105c-41f8-b23e-b36ebf5eabe4',\n",
       "  '065d858e-d138-4ccf-9c2e-9d14ee478be3',\n",
       "  '065d9796-1507-4800-869f-ae6641b68e6f',\n",
       "  '065f4a3a-c39e-4b2b-8acf-07c8c51560d0',\n",
       "  '065ff045-701b-4ceb-ad45-67979699822a',\n",
       "  '0663cf65-65e1-43bf-81c6-c5cdd82717d5',\n",
       "  '066aaaed-20d2-4736-b648-2a19c5bc6e05',\n",
       "  '066c5bcb-adb3-46de-9de9-ab55f024884a',\n",
       "  '066d6923-9d10-49fc-a535-1b0dd8ac8153',\n",
       "  '066d7714-ece3-4ede-abf9-ee3a52e53511',\n",
       "  '067224a0-ec1e-4aad-bf41-5986181b5320',\n",
       "  '0676cea4-0327-4cb4-baa9-4750992cd777',\n",
       "  '06791242-ea61-4910-ba57-2709af5e5e16',\n",
       "  '067c20bb-e724-4111-ad54-5a4e5d7f9e8a',\n",
       "  '067fad3f-5fba-4732-b9f6-957eee3c578b',\n",
       "  '0682392b-853f-4367-b50e-ea82651c6bfa',\n",
       "  '06844dd3-7ac9-4daa-b795-8be310949606',\n",
       "  '0685afcb-55ef-481b-b598-0d7d363344f1',\n",
       "  '0685d560-a722-41fe-8ec3-ff82db2d3db5',\n",
       "  '068d07ef-47ec-49a1-a37f-fab987bcb63e',\n",
       "  '06910b10-afa7-4c71-9fc3-16f1f43ce425',\n",
       "  '06913961-515d-4cda-9675-12733b21db56',\n",
       "  '0693cef2-d151-4cdb-9aea-603818a13e8a',\n",
       "  '069565be-01cb-4fee-80f0-9063b2558437',\n",
       "  '069841bf-5e48-4919-8b23-bd855f772361',\n",
       "  '069d36e2-5a92-4d36-8f24-eb5470df6e28',\n",
       "  '069dda5c-3a67-4965-883b-ce0b8fec3f7a',\n",
       "  '06a026c5-ee16-490d-96bc-c21ce108e355',\n",
       "  '06a11415-26d2-4ac4-be3f-1278b755f20e',\n",
       "  '06a19e36-6892-4767-9224-88da917267a3',\n",
       "  '06a1c11d-1c2f-4250-814e-fe659328feb7',\n",
       "  '06aa6d95-662c-484e-b751-42dde5f937c9',\n",
       "  '06ad32b5-1497-477d-accf-eadb017911dd',\n",
       "  '06ae21b9-52f1-4022-9b22-f049037de752',\n",
       "  '06b031aa-40bc-4472-826b-b69c2fc4d41f',\n",
       "  '06b240a6-0393-4bea-b72c-027c2d5ac192',\n",
       "  '06b72d89-c43c-4b90-92f3-c5467ace6c8b',\n",
       "  '06bb4792-b77f-435c-bda7-c9993b0ad398',\n",
       "  '06bc7fba-1383-4e26-b7bc-fd806a81c5bb',\n",
       "  '06be0124-c009-4334-9fa4-524951c8b631',\n",
       "  '06c10882-01ba-42fe-ac62-7841f06af222',\n",
       "  '06c1c8c3-f148-43f2-9cac-72fe09748054',\n",
       "  '06cee182-4df2-4576-88a9-b0a19360b8e3',\n",
       "  '06d00eb2-c24c-4dfe-8350-d65e211401dd',\n",
       "  '06d65365-3fa4-46f7-894f-c6fa8c772f47',\n",
       "  '06d85d5a-c250-4ff5-89ad-e31217284bf3',\n",
       "  '06d8f7dd-d3b8-4a43-81df-d15ad797e2db',\n",
       "  '06da7768-2510-47da-8f93-5dcb2787ab28',\n",
       "  '06dab57e-a4c9-4a54-9c99-d18fc389e25a',\n",
       "  '06e73b54-28dd-472b-81f8-24b3c7bc7218',\n",
       "  '06e7e680-865c-4a64-8c4f-7f87952c3679',\n",
       "  '06f01062-76a6-4843-aba1-ffcdab881479',\n",
       "  '06f0fea0-2705-451e-bb41-3bef5490a346',\n",
       "  '06fb7844-e102-45e6-9642-d15426612675',\n",
       "  '06fe0aac-ce62-4699-af5f-357b215fe34e',\n",
       "  '06ff271d-3807-409c-a568-8bf9a5c5971c',\n",
       "  '07036c02-6dd6-4254-80ce-b150911224fa',\n",
       "  '070d04c4-ddc0-4bc4-bec6-2a9f2aca2828',\n",
       "  '070d9150-f5cb-42e6-acf2-692de786e111',\n",
       "  '0712b006-6acf-4220-b7e1-f9178fa6dd50',\n",
       "  '07175468-1fdf-4d40-8ca6-61854404502e',\n",
       "  '071a5f0f-93f3-4985-941b-04e060592650',\n",
       "  '071a90c1-0c3c-4c3a-9266-6e60a7f2cb9f',\n",
       "  '071c90a1-b1af-453e-93ce-300d9182e7ca',\n",
       "  '071fa18b-d378-4f9e-a615-d0c79ecb7796',\n",
       "  '07289d69-2659-416f-b27e-8bc7b95bbc3f',\n",
       "  '0728ab50-2820-4037-9b13-e1022cf36655',\n",
       "  '072e6530-cdc0-47e9-8714-b3b244b3c5e8',\n",
       "  '073253fc-ad56-407d-b41e-1c7792e86bb1',\n",
       "  '07358ae2-5948-462f-9662-53de7f0c2166',\n",
       "  '07362fab-2d56-4ffd-bcc0-58508bb17a1c',\n",
       "  '0738f5e4-b580-4559-95a9-9255160228dc',\n",
       "  '07404487-115f-4888-a5e6-6c14794a9bad',\n",
       "  '0748d22f-45db-4d02-ae8e-6d361daf89ab',\n",
       "  '0749f51c-cccf-485a-9cc3-4f872c445d50',\n",
       "  '074b50aa-deef-497f-bd02-7c6549dc824f',\n",
       "  '074d761e-d4b3-48a6-a7de-2751d06f7fe7',\n",
       "  '074e4c88-d4e0-4882-97ef-2778127b26ff',\n",
       "  '074ebbd8-b264-4d2f-8d4e-ef8e34bf26a8',\n",
       "  '074fbc62-61ae-429e-9cf7-4300b2767ea1',\n",
       "  '075264c7-db96-42d2-b2f3-560763d2876f',\n",
       "  '0752a428-d470-4c3e-8ba0-5a5cc91f0e2c',\n",
       "  '07558a24-d46f-4b5c-8883-6b881dd1d578',\n",
       "  '07567aa0-9ac3-493c-9b9b-7c5e7b5c1304',\n",
       "  '0757a5cc-8950-4338-8514-8f73dfaec2cb',\n",
       "  '075a3c59-7491-44c1-b3ac-d992b0962bf6',\n",
       "  '075f436e-3d49-4c0b-a3a9-b305ccf58ebf',\n",
       "  '076065dc-f578-43cc-8eea-8e370eddb4b0',\n",
       "  '076a40aa-8dd3-4529-afcb-c4533a9f1a55',\n",
       "  '076aad94-ca42-4bdf-8406-14a9242c4c3e',\n",
       "  '076b7c4c-2c62-4f83-be4c-c03bd0803aef',\n",
       "  '076f6da8-526d-4e49-aa90-8cd197acd511',\n",
       "  '07700388-f719-4212-97cd-27f99c2d603b',\n",
       "  '077a337f-5091-484f-a3b3-d42152ead3a1',\n",
       "  '077aa1d2-dace-465b-af09-2f3cddec9f35',\n",
       "  '077ba8f2-5a87-4747-8698-2cd37c530078',\n",
       "  '077f221d-9ef4-4637-b367-db7cb6df3c52',\n",
       "  '0780c801-f03b-4855-9ffa-a05dce5b14c4',\n",
       "  '07816b58-dd38-4be0-a0af-fcbd8719ddb9',\n",
       "  '0786be7b-f6c0-41b9-8ccf-f766d634ba68',\n",
       "  '078a6467-4db4-4c5d-9cdc-0561427f50bb',\n",
       "  '07949719-fc06-4271-ab17-65e064d08ae5',\n",
       "  '0795e03b-7fe3-4400-86b9-0bb6d28a619d',\n",
       "  '07a5ce08-511a-4d86-bd5f-8db904b94266',\n",
       "  '07a8127d-a1ee-4c48-bb31-4f0139fc1305',\n",
       "  '07ae34a9-50e4-4411-9280-0d39d4b599d1',\n",
       "  '07ae9497-c2df-43f9-89ae-25cfabfa25da',\n",
       "  '07b1841d-7036-448d-8842-ae0c86c1fdf6',\n",
       "  '07b22be7-4403-450a-a06e-7155771d5303',\n",
       "  '07b4a139-fc76-4476-a6f9-78a6982e3136',\n",
       "  '07b564ac-3f33-43e3-8bc0-6b343f4534c1',\n",
       "  '07b9c93d-c204-41bb-bc21-2062a76998ae',\n",
       "  '07be1735-0f09-41a8-8829-fc85ddeea9c4',\n",
       "  '07bed769-987b-4366-a1f2-da2944ae8a47',\n",
       "  '07c74398-b1cb-4b42-9ce2-38c21c6ff48c',\n",
       "  '07c819d2-91d0-4121-b180-d0c6cf38c0d9',\n",
       "  '07c99e39-6aa5-4a07-a27b-c7c8a01b597b',\n",
       "  '07cb5097-babc-4eda-b31c-cd8102e04b91',\n",
       "  '07ce2c56-8457-4211-9d8b-4d1755b0ad56',\n",
       "  '07cee32c-ea11-45c8-b7d5-8891efea9cce',\n",
       "  '07d1bbd3-f646-4295-8843-cd43611b22e4',\n",
       "  '07dc4ab8-e1ef-4cf5-8d5c-58f9989e3161',\n",
       "  '07e2f594-1b88-4b22-9201-d5ed83b012a3',\n",
       "  '07e4281b-4e4d-4325-b33d-cc4c30d0529c',\n",
       "  '07e8c817-0ef9-43f9-99f7-9355d25d0c37',\n",
       "  '07ee8ec6-5870-4c83-b50f-b7c6740254e1',\n",
       "  '07f0ae9d-3a02-41be-ab1e-e5bcabb00818',\n",
       "  '07f240bb-8120-42b5-8ca5-b677a40801cd',\n",
       "  '07f567e6-270f-4b11-bf89-cd8a43a94751',\n",
       "  '07f5d95f-5dbb-42af-bf57-3094201695ff',\n",
       "  '07fb66b5-59fc-4216-b7c5-4a48c3a34039',\n",
       "  '080323ef-3c18-4abf-a832-d0d8c6692f40',\n",
       "  '08087979-fea0-4b08-b4ca-ebfd7da3d696',\n",
       "  '0808e7dc-d75c-4acc-b9a2-d9798ad62231',\n",
       "  '080bd598-f87b-4c6e-82aa-54f611ca44ea',\n",
       "  '0812e352-8588-4085-abe6-d7c05bc760d1',\n",
       "  '081735ce-eb7f-4181-9039-76fb96e5d0c9',\n",
       "  '081c3f0f-1796-4a80-99cb-2c9bcf344de8',\n",
       "  '081cf330-7ada-4e62-b8b7-3bc5b57adea8',\n",
       "  '081d1552-8b12-4f69-9c82-cf7ee9b28db1',\n",
       "  '08204883-43d1-4c0c-b378-6cf8506a7370',\n",
       "  '08228483-1824-4908-b247-9fc75004e313',\n",
       "  '082a81f6-fe4c-48f4-a1e9-433753ec09de',\n",
       "  '08311dce-d920-4b94-b27b-fee5cb9cf79a',\n",
       "  '0831d8ac-d367-46d5-bb20-71dd51df4fdd',\n",
       "  '0832bf84-a905-4551-88c6-20c14189a961',\n",
       "  '0833c6d6-daab-4624-9ef2-a90693449938',\n",
       "  '08391c24-a892-476c-82c4-63951261449b',\n",
       "  '083cc1e3-6175-4a03-9000-61d6db46afa8',\n",
       "  '083f097a-e198-4916-b0cf-ed5cf394f212',\n",
       "  '0841a1c2-1414-4ccd-b493-61bef0d17efd',\n",
       "  '084206f4-13fe-48f4-beba-8e2a582bad4b',\n",
       "  '084540bf-e686-4cfa-b1c5-9c65f5255ec8',\n",
       "  '0845b6dd-91f4-496f-9c8d-64117ef6fec3',\n",
       "  '0845e360-bc09-41a7-b596-1437d548ad84',\n",
       "  '084698e0-f837-4f2f-be9c-6aee07b7d9d2',\n",
       "  '085739ef-bcf0-45ab-931e-c9a81c28eb60',\n",
       "  '08584c6a-e344-4aef-85f0-39809ff82b40',\n",
       "  '0858a784-32ed-4cec-a914-50331fdc12df',\n",
       "  '0858d658-4d78-408b-b294-40a6053a9a41',\n",
       "  '085dd676-1ed9-4c61-8494-146d3cce0b70',\n",
       "  '085f1370-9288-46f9-ab6f-09d59c75061a',\n",
       "  '0860338e-380a-4c88-aaa8-42a7d40f184a',\n",
       "  '0860e170-c760-4b60-9e79-50fdbc8b9fc7',\n",
       "  '0862bbea-62ce-4380-8b3d-bed229a3a24b',\n",
       "  '0864377f-4d7c-4dd0-8c28-54b6ba658b25',\n",
       "  '086ba606-7562-44cc-bac0-4da182f5e1db',\n",
       "  '086c92f4-7ec8-49d0-bc8c-6a2b50bf7928',\n",
       "  '0872edc9-a65f-4c8b-a577-6a15cdc74ad0',\n",
       "  '08899f23-8d3a-43c5-9d9b-1707003b69f3',\n",
       "  '088be369-e654-43ea-bbb5-89b043b3377b',\n",
       "  '08948317-2851-467d-9487-90786d415437',\n",
       "  '089fbef7-3e6e-4325-999e-26d25614a141',\n",
       "  '08a14bd8-5c0f-40e2-8518-f01012fc7c85',\n",
       "  '08a4752a-0f9c-43f3-b986-9a4874fe43ad',\n",
       "  '08a6fe57-896c-4aa6-b14b-cb49727329f5',\n",
       "  '08b3d680-1780-4440-8438-1f3ff2fc406d',\n",
       "  '08bc12cf-538e-4ec9-a6ea-552ec3058ea1',\n",
       "  '08bf8113-550f-44b2-b93b-37fc1fd003d0',\n",
       "  '08c3b94e-dc3d-42ab-adcf-6445c346aa7f',\n",
       "  '08c6f4cd-c6d6-4b0f-8688-1d2652ae11a3',\n",
       "  '08c854ee-cd57-4c64-9e15-3d51744b8c9e',\n",
       "  '08cab5e1-8256-44ac-88ef-45383fb7ce99',\n",
       "  '08cbcb40-5711-4d0b-9bfd-0cf6c1baf072',\n",
       "  '08cc2f79-4a78-4b53-a15d-f0db40b45bbe',\n",
       "  '08ce7933-dc2a-4f3a-9857-6c8db1f855ac',\n",
       "  '08d0cf28-3154-41e2-bfde-740be51740b9',\n",
       "  '08d383d0-8598-4318-866b-700499753327',\n",
       "  '08d54f4b-af86-43f4-b048-496001cd8cfb',\n",
       "  '08d7657d-29e9-4254-b9bc-f2038b33e570',\n",
       "  '08d923ca-8439-423c-bbae-a508bd08cd00',\n",
       "  '08d971c1-c7d0-4803-b5cd-38e845d35d3c',\n",
       "  '08dc5eb3-bfd4-4e26-a5fa-56c2f8ef8168',\n",
       "  '08ddbab2-bebb-486a-a790-abfe2319fa3b',\n",
       "  '08deca6a-8051-4100-be02-4ea1d05e67e9',\n",
       "  '08e42f82-5327-4fba-9041-1a80f99b81ab',\n",
       "  '08ec6d36-cdd9-4111-9f82-68c187f79b1d',\n",
       "  '08f332a5-dd2f-45e4-be15-4fbd426606f6',\n",
       "  '08fa4868-d61e-4673-a6e6-f43562821ee7',\n",
       "  '08fe09eb-9d10-49be-949f-90972665d2b3',\n",
       "  '0902bf0c-a29f-408f-9805-2776ca202bda',\n",
       "  '0906b0f0-047a-4271-b2e8-5f708ddeac1a',\n",
       "  '090708a5-3c5e-4c34-8c5a-5f2d0b552066',\n",
       "  '090881bd-33d6-445b-9ed9-a5e98e08e4b4',\n",
       "  '090b6173-6049-457a-aab9-ece5db1535c6',\n",
       "  '090b9db3-dc25-4888-a116-0cc3953df942',\n",
       "  '090eacb7-08b4-459e-aa9a-62347307047d',\n",
       "  '091793c7-5881-4a79-8dd6-98bd3d4459e7',\n",
       "  '0919ea9b-254e-4c95-a3dc-e00bd365d845',\n",
       "  '091ad833-7cb5-46bf-855c-c64a03bcd840',\n",
       "  '091e96a7-8622-4add-8e90-5c45c3f5bb50',\n",
       "  '09230793-6b86-434b-b9fb-fe2900d736d2',\n",
       "  '09237749-b015-42df-bc7f-b082fc15ba62',\n",
       "  '09239a31-6e6a-44e3-90e9-9f11861eebb1',\n",
       "  '09246937-9828-494c-9072-17c2ffcc4d29',\n",
       "  '0924d942-8009-4fdb-ac2a-474f47064a7b',\n",
       "  '092559e5-b43f-4d6a-9a5a-cc3f9fac7485',\n",
       "  '0926912f-4d18-4651-8c2d-4941b1f4b7fd',\n",
       "  '0928dd2f-6f71-4ada-87df-18baddcf31a8',\n",
       "  '092ceac2-ad02-489d-a99b-456ecf89993e',\n",
       "  '09316694-0117-4b4a-a8ce-5e857d3f2b56',\n",
       "  '0933b21d-8639-4326-984e-8d8156bb3962',\n",
       "  '0935941d-7346-4cd7-8e91-3fba9fd65154',\n",
       "  '093e5822-bdc8-4c0d-81e8-259b06b5212c',\n",
       "  '093f8cd4-4da4-4ec6-8865-1f2abda6aae8',\n",
       "  '09421f6a-d230-4f3e-8161-77873a5e605f',\n",
       "  '0944b3ed-57f4-4440-b180-4a5b7ff75594',\n",
       "  '094ea68e-29db-4aeb-aefd-ac7b3ec383f1',\n",
       "  '09502b20-59a5-47a7-ba9e-0a84790fcde5',\n",
       "  '0950c974-5f64-48c4-9c9f-b290631e6173',\n",
       "  '0958fa05-bd92-410b-a6fd-e971df4c9c1e',\n",
       "  '095e479e-942b-4991-a48c-bc447bdc0ab9',\n",
       "  '095ea41c-c154-4044-9e86-f6e44bc825bc',\n",
       "  '09606ac4-e0e3-4f88-a774-7758496fb743',\n",
       "  '09675497-53d2-4cac-b016-553ef40b208c',\n",
       "  '096c98cb-5adc-417a-886c-01f5117ba327',\n",
       "  '096f41bf-0134-4549-ab07-3a93727b0c2d',\n",
       "  '0971d044-b915-4c1d-8d53-c8d290e5b90f',\n",
       "  '09720b56-1a01-4ea2-990d-8e4ca45cc14a',\n",
       "  '097a2d35-8c6f-43c4-8dcd-c77de9e55293',\n",
       "  '097add55-f482-40f1-9be0-b4f1be161549',\n",
       "  '097dfaf4-3262-48c3-ae0d-45de20c82fc7',\n",
       "  '097f5217-8d45-4edf-842f-8276c815a1f2',\n",
       "  '0983ebbf-1a33-455e-87d7-c200b9c29752',\n",
       "  '09912f2a-3afe-4d2f-a741-7a9e492c15ad',\n",
       "  '099217f1-69ff-41d6-a163-046e599a501f',\n",
       "  '0994a16a-3b9f-4c24-a406-973fad976394',\n",
       "  '09981c17-4b3e-47ed-b715-c487a311e1aa',\n",
       "  '09991122-3b00-4964-95bb-76678104ca9f',\n",
       "  '099aa02e-6a11-450a-86de-26987a7ffa42',\n",
       "  '099aaf39-3095-4ccf-8981-00565230a2d5',\n",
       "  '099ad278-2130-4d12-b95b-2b49f05c6673',\n",
       "  '099b65ba-389d-4545-b1a6-0ced03a6a82c',\n",
       "  '099bffaf-f962-4740-8688-f78cc71c2385',\n",
       "  '099c7724-c298-4d32-9194-df5714f9a082',\n",
       "  '099f087c-1568-40b0-af22-831a3a53de4f',\n",
       "  '099f4bb4-d96a-4138-a462-208ef795844b',\n",
       "  '099fdb41-462b-4399-982c-98fffc08d4fe',\n",
       "  '09a1528a-ef91-4c39-a6fa-315b8f9f83c9',\n",
       "  '09a3427d-342d-4529-80b5-fa5bc772f185',\n",
       "  '09a4112a-d861-4aa7-9d94-0a968d0e1479',\n",
       "  '09a7c197-897b-491c-9c1c-49ffc809797e',\n",
       "  '09a99f77-4d52-43bd-a279-4f98d81f3159',\n",
       "  '09ac3c37-49b6-4b0d-b5e1-20b4dd631a8b',\n",
       "  '09af5098-0114-4232-914e-711853c93dee',\n",
       "  '09af5aa0-590f-4333-ac9d-4c3f40f7e320',\n",
       "  '09afefb8-6865-41a5-a093-91d00e4533d2',\n",
       "  '09b03d5d-29b6-443d-9fa8-6ff1171c83c2',\n",
       "  '09b2969b-c645-44f6-aea0-3221a543f3da',\n",
       "  '09b8b838-a9db-4ca4-b094-a5d39f5d69a4',\n",
       "  '09bf63ed-3857-4e99-a046-1cf6df09636d',\n",
       "  '09bf829f-6b8a-497e-83f6-084333bdf63b',\n",
       "  '09c83f96-872a-44cb-877d-4c36471e2e51',\n",
       "  '09c8a857-7ea2-408a-a33b-f37eb9e1b20f',\n",
       "  '09c9ba11-97cf-4620-b8e4-48bb002e7f4e',\n",
       "  '09c9f2af-5baf-4874-96de-455a774f5b31',\n",
       "  '09cb4032-0f66-42b5-8948-73bb7b643ba7',\n",
       "  '09ccb6a4-2eb1-4598-83be-30a65bba3657',\n",
       "  '09d1f56e-7a0e-4db9-8d62-8c59d39e88d1',\n",
       "  '09d3362b-97c4-4ee7-8ead-af5564bd93a9',\n",
       "  '09d678c5-c331-45ac-9c13-f9c09199b8e8',\n",
       "  '09daa2a1-68f4-45be-a7a3-29a34e4b1d9f',\n",
       "  '09de8f82-88f2-4597-8571-b81fa49bbcac',\n",
       "  '09eecfbd-64f6-4962-86df-965fa1f5e14e',\n",
       "  '09f0cda5-98a8-47bd-8c24-95f6a84e523a',\n",
       "  '09f1a7ea-1fe1-49e6-a5fe-ddbc992552ac',\n",
       "  '09f1fcbc-8741-4075-a729-52d4b97de87e',\n",
       "  '09f7f81f-bc36-469e-a43b-8dfca5cb0980',\n",
       "  '09f8eb4b-0918-4869-a6a4-a34bc21c7bd2',\n",
       "  '09fbaf53-21a6-4256-8cca-1238336e014a',\n",
       "  '09fd1100-6982-407c-833b-23c65d4e9349',\n",
       "  '09ff6ae6-353b-4426-9fce-4cb7c1c2a19c',\n",
       "  '0a0318b4-5713-44d3-ba2b-a89f5115f2a1',\n",
       "  '0a06a8a5-77f7-440b-af21-d016be547580',\n",
       "  '0a073c0b-e22e-4043-bc5b-87dfe1fc8a7e',\n",
       "  '0a10356b-aa91-4142-9e04-fe4762767513',\n",
       "  '0a134de3-2399-49ed-ba87-cd4f0a3ecf0c',\n",
       "  '0a142572-17db-4add-8cb9-aaf1ce459f01',\n",
       "  '0a165dd3-fb01-4c17-be51-0c47f9ab3c36',\n",
       "  '0a180b37-b06a-47be-a6df-d4b3276dd2a0',\n",
       "  '0a186db6-351e-44f8-a93f-9c79675c50fb',\n",
       "  '0a255d32-898b-4d73-8359-6cd3e624de03',\n",
       "  '0a29c419-8e86-45c5-b183-af17d036817b',\n",
       "  '0a2c4e8e-59a5-4f7b-aad5-9c0b75e1323a',\n",
       "  '0a2d86fe-f121-49a4-8ba6-8707cd617eeb',\n",
       "  '0a350eda-1a2c-4c21-bf64-14efd89a899e',\n",
       "  '0a377462-6475-4d85-a78c-113bf9da8879',\n",
       "  '0a3eca41-247c-4054-b0d1-67bf9f7b1022',\n",
       "  '0a44ba23-8fc1-46bc-9cc3-583fd9a5b496',\n",
       "  '0a4b3e87-1fb3-4871-afab-ef0250e0b141',\n",
       "  '0a510f74-f616-4670-b479-c5f76e0e02d8',\n",
       "  '0a52ef73-ce70-4b84-9c5d-af911856acaf',\n",
       "  '0a536048-4ea3-40de-9252-0bd1b53d8ff7',\n",
       "  '0a54c148-3553-4d51-ba87-319499a815f8',\n",
       "  '0a5f58b3-01e4-4be9-9c2f-768bf7e32abf',\n",
       "  '0a6a65a1-7e79-4cd3-a2de-92d38c1e67bc',\n",
       "  '0a6afadf-d89f-4666-b595-7e92d0b5760f',\n",
       "  '0a6d410e-1c0e-4aea-904d-bda53f20884b',\n",
       "  '0a719b56-eb3b-4850-bec6-a58cfa0403f4',\n",
       "  '0a72aeac-16c0-4397-88d1-74f70ca6f20c',\n",
       "  '0a72d0c4-e13b-4cae-b1e6-71244c754eaf',\n",
       "  '0a73c11a-2d7a-4887-ad60-9fab8fb620c4',\n",
       "  '0a7955bd-e261-4232-8abe-b82faf2785dc',\n",
       "  '0a7ad4c5-7684-478e-949b-31450fc6143b',\n",
       "  '0a7bf8e1-01e2-453e-8289-e9d6c74884e0',\n",
       "  '0a7cf183-68a0-4923-bf2c-949f2161ef56',\n",
       "  '0a7d3210-f2e2-4205-ad5b-f1f3e13285ad',\n",
       "  '0a809c79-85cd-4d09-b4ee-dc25012076ed',\n",
       "  '0a8e6513-8cb7-4f6f-b6b3-5df156a04651',\n",
       "  '0a8f752b-53c7-4e84-8482-c45402524623',\n",
       "  '0a962fb9-d6d8-4979-8aef-bf6d004acc36',\n",
       "  '0a969782-3eaa-4f5a-a6b1-0d026642947f',\n",
       "  '0a970ed6-b653-41a2-9cfc-0bbc05d61c09',\n",
       "  '0a9a2297-772c-4345-8fda-946ba90819df',\n",
       "  '0aaaa7a7-ecbe-4c7a-bb4e-86b4191836bc',\n",
       "  '0ab08f52-4a60-481d-bb32-2d32e65e9592',\n",
       "  '0ab14320-9ed1-451f-ae5a-bd78868f518e',\n",
       "  '0ab27fcf-3055-4377-85af-5d59c5f3daaf',\n",
       "  '0ab2cf68-02db-4e18-a815-9ca112e36f92',\n",
       "  '0ab6b6fa-caff-4a42-b7a2-93517f72b1d3',\n",
       "  '0abb0e04-b2a4-412c-ae77-3d6ad0438491',\n",
       "  '0abb4948-beac-49c2-aa52-d6c6ce4587f7',\n",
       "  '0abbcc33-a6a5-4bfd-b9d0-c6bbfdcb34e6',\n",
       "  '0abcea0b-6a6f-4453-ad9c-c8abda47620a',\n",
       "  '0ac0214d-181c-4d97-8a07-61931ea88f8b',\n",
       "  '0ac5420d-07e1-439d-806a-b27f44a168ba',\n",
       "  '0ac6b5f0-1af5-4056-8945-4dc52363f45d',\n",
       "  '0ac7ae21-e488-4820-96e3-c41e508ed553',\n",
       "  '0ac8cf84-0898-410f-8d9e-76eddd33dc66',\n",
       "  '0aca71c0-dfe6-4b09-8325-25725b173d8b',\n",
       "  '0acc222d-b129-47a8-9d56-0b4b1596b849',\n",
       "  '0acc319b-0836-4e91-a32f-ec586e26e1fd',\n",
       "  '0acf19ff-f75f-4fbd-920e-1fd4ccd137e9',\n",
       "  '0ad31e8b-a44b-4ec0-9745-1246b4410b40',\n",
       "  '0ad57a3e-4033-4edd-b315-d76113c186f5',\n",
       "  '0ad9d7ec-8010-40c3-8d1f-916c2d3e1e8e',\n",
       "  '0addefa7-ebe3-4b1c-8987-d2a845a3fdf5',\n",
       "  '0addfefc-7093-44ca-8500-29c40d871889',\n",
       "  '0ae2faf0-7196-444c-8755-132c837e5635',\n",
       "  '0ae386c3-2858-4976-b2d4-6b9fc65ccf38',\n",
       "  '0ae50d49-1f34-437b-8cc1-46e36079503a',\n",
       "  '0ae60b10-2ccb-4b63-a5d1-27ed8a8b2a45',\n",
       "  '0ae662cf-a523-4b0d-b78a-b1a68b254113',\n",
       "  '0aeaa959-a20e-43ea-802e-2f560bf58c50',\n",
       "  '0aecc29b-2727-40f1-b221-992a8b09c3f6',\n",
       "  '0af00432-d88e-4c06-abd0-f10db9963f1a',\n",
       "  '0af7d8e2-d061-456f-a7d4-3d99fc3d81cb',\n",
       "  '0afc753c-7df2-4a92-9201-997c6601ceae',\n",
       "  '0afc75a1-bd87-464f-b70b-f83d9898fcab',\n",
       "  '0afee6b2-230e-4b05-9c25-60b0d1da73c5',\n",
       "  '0b08caf6-e5ce-481e-8ac6-0c3ee08aafc7',\n",
       "  '0b0ea33d-8118-4160-b1e2-841d21c23bc5',\n",
       "  '0b1516cc-c7f4-4f2a-8a85-b06dcd7b71af',\n",
       "  '0b15492d-7c30-4ffb-972b-6a14bd02fe58',\n",
       "  '0b1553e6-24eb-4662-a332-0dc4878826c2',\n",
       "  '0b185bf7-27a9-473b-81cf-213597137760',\n",
       "  '0b19a954-05e3-488f-9095-40f2d1900ec6',\n",
       "  '0b1acbbe-b51e-4f6b-a779-2b565519c319',\n",
       "  '0b1b8922-feea-4687-92bf-d088d9b8c956',\n",
       "  '0b1c1356-81b0-4682-a7df-16b448c69e20',\n",
       "  '0b1e7109-e24c-4c8b-8e6c-ca37523463f0',\n",
       "  '0b21b44a-d9b9-4a30-9228-372c681506b7',\n",
       "  '0b22e73f-208e-42a7-b41c-ff7fbf036d28',\n",
       "  '0b259b8c-5c1a-4245-9e05-fa177eb2f105',\n",
       "  '0b25a95c-f489-4dee-88e2-cd00d0fcca41',\n",
       "  '0b29ddd7-60da-4cf5-87ce-1dbd60c57858',\n",
       "  '0b2c1c8e-0d6c-45a6-aecb-bc20d7d5c4aa',\n",
       "  '0b2c995b-3471-4c8c-b3ac-d31bfde76644',\n",
       "  '0b30c955-7da5-4f95-ae6f-40d5efd40953',\n",
       "  '0b3120f3-1676-40f9-b455-3701d75a9e0a',\n",
       "  '0b333d8c-7cdf-4a8a-978a-9f7765f3b909',\n",
       "  '0b36f7d5-9fa0-4c28-a269-1a2dccbe903b',\n",
       "  '0b37b80a-95ae-4271-a4d9-4b66e59cf5ac',\n",
       "  '0b39f5f9-c9d5-4144-a532-8c4204a3c496',\n",
       "  '0b3b0761-1eb9-4d72-ba82-234370db4e8c',\n",
       "  '0b3dbfa4-f929-45c8-bb4b-6a2cb570b314',\n",
       "  '0b3fd0da-b21d-4b60-ace6-64306fba6c5c',\n",
       "  '0b43bd53-8175-407e-9252-82767bf926e4',\n",
       "  '0b5484f8-1839-43f3-ade0-c42bb38ea9b0',\n",
       "  '0b548ce7-62e4-40e0-b7c0-bc6409dcd019',\n",
       "  '0b55f9c9-e565-4387-bafd-a3827ee4be4d',\n",
       "  '0b58122d-9eaa-4cfa-99fc-c0c3ac1a8903',\n",
       "  '0b5ee001-5e46-4782-9d03-56cfd3ee12d7',\n",
       "  '0b6a8427-4202-4497-9e85-2feace43f56d',\n",
       "  '0b6db014-b69b-479c-889f-f288e87b9a5c',\n",
       "  '0b73b15a-08ac-4083-8d04-5a49253194b4',\n",
       "  '0b753969-176a-47c4-a4ec-ecc37430cd0e',\n",
       "  '0b77a1b3-5c05-4044-bfba-24679772fde0',\n",
       "  '0b868f8b-a086-4232-bf67-75ee14cd1e58',\n",
       "  '0b871343-5c1b-4bc8-be4a-39634f5083b9',\n",
       "  '0b88caf0-661a-4b93-bd17-344e55615168',\n",
       "  '0b8d409f-e52f-479a-9500-0fd470349c29',\n",
       "  '0b8eed6f-8122-439e-b167-97ba813e28cb',\n",
       "  '0b8f5623-1b74-4aba-b29d-99e3568b241a',\n",
       "  '0b915c7a-f2a9-4fd3-8267-c7c87fc8b9ff',\n",
       "  '0b9646d9-1478-429e-a438-6c6f1e8f1eb2',\n",
       "  '0b96e4be-2fb7-4530-b05a-6b237b5aa844',\n",
       "  '0b976742-55dd-4a51-9c56-b61515afd53b',\n",
       "  '0b9c7856-7388-4f1e-b3f7-719ec3477ed4',\n",
       "  '0ba017bc-337a-44c9-8696-835438b7de43',\n",
       "  '0ba38ec5-b8eb-4370-bf7a-4d1aa1dc7dc9',\n",
       "  '0ba3d0ab-6619-4dd4-8734-14379a076a41',\n",
       "  '0ba7c944-6779-4430-90c2-715f02b65a7f',\n",
       "  '0ba9e8dc-7195-4206-9277-1b18ebdb79b8',\n",
       "  '0baddbde-c94d-4fd8-bf2a-49c9e02b6a7d',\n",
       "  '0badeeda-4d2d-4ac7-87e4-637090933cc8',\n",
       "  '0bb6c97d-e59c-4c24-be59-017b5c06e85f',\n",
       "  '0bb6cfcd-579b-49fd-a1f9-8c61f4f6b374',\n",
       "  '0bb72326-d48e-4f9e-98fc-942927d39676',\n",
       "  '0bb857b0-ab1d-4938-8f33-3ff6df58ef20',\n",
       "  '0bb965fa-abd1-43c8-b7f8-e817441310fc',\n",
       "  '0bc22582-62d3-4eae-9ae9-c6a06a7a6562',\n",
       "  '0bc230c8-3414-47a7-b622-3484841988e6',\n",
       "  '0bc92021-6796-4199-a17f-2417b7ad8002',\n",
       "  '0bc9eb2a-b976-4974-928e-9c9f13816ae2',\n",
       "  '0bcf3942-e52e-42f2-856e-56cdbdda4ceb',\n",
       "  '0bd084de-afa8-4bdb-bb03-060a9db94072',\n",
       "  '0bd47a44-643a-4f62-b61c-4c55c3a90b7d',\n",
       "  '0bd520f5-87e2-48fa-9cf8-eeffaea958e0',\n",
       "  '0bde369d-b4ea-4851-9704-807f79c2dd48',\n",
       "  '0be2976a-0637-4cb6-a55f-2e001ed67a19',\n",
       "  '0be3a0db-9afc-4931-bce3-a6843eb550ad',\n",
       "  '0be5e0a4-9370-45c8-a7d0-af4176d91ecf',\n",
       "  '0be65f8d-4d7f-4aa6-b9b7-f4a7185af1b7',\n",
       "  '0beafaf8-a0e2-43de-8d10-b1d46eb839ba',\n",
       "  '0bebda5a-2728-4e85-a1e6-7464288794e2',\n",
       "  '0beddec2-866e-427a-baac-4612e862d357',\n",
       "  '0bef4f1b-3ae0-44de-9412-2bfc185a9397',\n",
       "  '0bf831f8-8cd4-495d-8d6d-27e3f8da7edc',\n",
       "  '0bfa5726-0caa-4a4e-905e-c00a0ed59388',\n",
       "  '0bfb1ffb-3288-455a-a009-f9c5f523e350',\n",
       "  '0bfb725e-7203-40b6-950b-607f865156ef',\n",
       "  '0bfbe981-75e7-49c0-b774-2a2e980f2789',\n",
       "  '0bfde6ee-9028-46e5-9920-667292837d67',\n",
       "  '0bfe0ad8-0f98-422f-8574-99d204eef5a1',\n",
       "  '0bff9d1f-455e-495f-8346-5de41c467d5d',\n",
       "  '0c031259-e02e-4217-9825-de799497f889',\n",
       "  '0c04999e-f8fb-4ecc-9743-864eb3ddfacf',\n",
       "  '0c093801-f369-4b06-ac28-50e7e6d5a063',\n",
       "  '0c0e5c32-c76d-4d5f-b988-7187642fce73',\n",
       "  '0c11a7d9-9310-4964-a411-4dc870646d4f',\n",
       "  '0c124b23-e5ac-4d6a-9f90-c667f23cf26b',\n",
       "  '0c198adc-5297-4d45-a594-12bd1d656978',\n",
       "  '0c1b46d6-997d-496f-bf90-4d687dd93568',\n",
       "  '0c1d3404-112f-4599-af59-0214f0688a9e',\n",
       "  '0c22713e-807c-425a-86df-87559b5c230d',\n",
       "  '0c240ce2-68c0-4db0-9a1a-2b7618cc74b4',\n",
       "  '0c25b8e2-1466-47f3-909c-9b3f4ba7a95f',\n",
       "  '0c27b8e2-46cd-4de8-8c71-b94d04a39557',\n",
       "  '0c29fb78-14e5-4ad1-9be7-0899c365b6f2',\n",
       "  '0c33af1e-1e12-48dc-97db-25b930fb5f14',\n",
       "  '0c348692-470d-46be-bd74-31d022831c03',\n",
       "  '0c41c5b3-ee20-4b4d-b34b-64f267babe04',\n",
       "  '0c4b069a-f275-450e-8d0b-6b9be212b3b0',\n",
       "  '0c519c17-91f6-4a5e-8167-4465844a4c65',\n",
       "  '0c522635-a4b0-40c0-b81f-483fe193f8fc',\n",
       "  '0c52703d-e18b-47fd-a258-93ae843a7051',\n",
       "  '0c5293f8-c19e-4a42-a698-2fa0b5df240e',\n",
       "  '0c602471-7bf7-4e07-ab78-bb2699284c65',\n",
       "  '0c6a82ee-0aec-45cf-865e-785ed9a85dac',\n",
       "  '0c6b7d6e-1c36-4200-a65c-b2d59eeb922d',\n",
       "  '0c6df55f-a6ca-4508-9478-7c53039e3335',\n",
       "  '0c6f4e2a-47bb-4e54-b658-5244cba84fbf',\n",
       "  '0c6fef6e-e05f-4514-a7d1-28d009a5d141',\n",
       "  '0c704ad1-38d7-49cd-92ac-bcfb6370519d',\n",
       "  '0c79e802-5eb5-49c3-8257-23921f298ada',\n",
       "  '0c7a91fe-5bdc-419e-9a98-8b4502cb3461',\n",
       "  '0c7c394f-4da8-4af2-8275-ef1a77efaf6d',\n",
       "  '0c7f7023-0feb-40b7-a4ed-1a57d377fc62',\n",
       "  '0c7f8529-5f69-4241-97a3-90ec667d823c',\n",
       "  '0c813c46-877e-4d35-b155-e000af6f5a63',\n",
       "  '0c82477d-08df-4c85-bed8-dc4ae5ea53d7',\n",
       "  '0c834532-6755-42c3-bdf2-ddf650c9ad8f',\n",
       "  '0c905ffc-df82-45ca-81d6-dd9767616696',\n",
       "  '0c91299f-f8a4-4dc5-908d-987df545d8e4',\n",
       "  '0c963618-554e-4af0-a3d3-f4f59e14b91d',\n",
       "  '0c96d4dc-c67b-48ff-82dd-297b32b43d5a',\n",
       "  '0c9ae3b2-8d3e-428e-a1b6-e25ca08dec43',\n",
       "  '0ca419c0-f20d-4107-bf61-685086453b22',\n",
       "  '0ca43d73-7857-47d3-a0cb-ef20e2ff4104',\n",
       "  '0ca5ccf2-bd3c-4851-9906-428ad1c4b61b',\n",
       "  '0caa4f20-1ea0-4eb1-9760-46a4966e469c',\n",
       "  ...],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'page': 541, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 452, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 250, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 349, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 367, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 366, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 104, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 316, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 375, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 344, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 292, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 478, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 371, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 90, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 393, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 230, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 455, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 465, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 503, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 334, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 514, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 451, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 480, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 514, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 175, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 479, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 586, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 494, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 564, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 174, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 48, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 501, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 352, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 503, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 341, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 23, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 134, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 1, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 182, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 234, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 174, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 242, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 285, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 163, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 313, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 251, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 432, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 6, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 382, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 269, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 208, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 313, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 250, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 339, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 77, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 344, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 119, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 620, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 389, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 234, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 401, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 32, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 389, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 113, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 598, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 284, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 505, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 224, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 172, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 513, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 348, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 284, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 504, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 421, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 279, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 482, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 396, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 286, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 504, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 427, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 244, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 574, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 253, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 292, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 238, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 69, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 45, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 404, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 85, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 190, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 583, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 482, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 545, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 409, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 114, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 379, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 258, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 323, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 174, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 455, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 484, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 475, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 472, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 152, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 251, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 368, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 201, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 96, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 125, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 118, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 279, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 16, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 502, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 316, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 382, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 322, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 229, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 65, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 363, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 311, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 545, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 546, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 132, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 63, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 90, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 274, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 428, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 60, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 390, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 492, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 318, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 249, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 534, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 131, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 476, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 126, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 230, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 109, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 34, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 492, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 122, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 267, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 165, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 189, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 310, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 287, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 329, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 467, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 115, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 562, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 352, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 183, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 61, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 43, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 448, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 288, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 108, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 256, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 144, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 323, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 213, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 228, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 160, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 152, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 410, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 39, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 141, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 508, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 468, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 477, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 546, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 136, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 114, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 123, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 89, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 153, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 211, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 485, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 593, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 186, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 414, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 414, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 189, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 322, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 313, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 426, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 180, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 114, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 121, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 475, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 459, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 286, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 237, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 276, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 418, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 24, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 532, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 485, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 507, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 163, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 414, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 167, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 459, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 442, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 0, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 388, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 514, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 338, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 304, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 371, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 131, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 206, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 258, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 141, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 579, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 239, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 603, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 206, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 95, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 439, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 353, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 380, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 56, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 103, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 193, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 324, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 25, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 425, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 177, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 351, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 127, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 436, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 353, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 510, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 269, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 410, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 573, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 374, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 576, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 167, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 353, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 402, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 38, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 111, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 314, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 543, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 191, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 135, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 88, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 243, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 406, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 167, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 512, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 309, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 528, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 61, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 428, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 346, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 442, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 106, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 402, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 594, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 531, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 452, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 545, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 264, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 71, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 479, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 200, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 508, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 546, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 358, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 206, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 264, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 470, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 144, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 196, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 468, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 599, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 55, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 65, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 165, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 339, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 394, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 435, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 488, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 464, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 242, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 126, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 352, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 349, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 13, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 62, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 579, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 208, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 96, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 91, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 480, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 209, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 403, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 461, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 28, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 262, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 467, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 330, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 113, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 229, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 478, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 435, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 132, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 584, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 473, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 287, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 295, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 615, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 473, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 437, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 591, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 539, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 193, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 383, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 573, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 74, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 364, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 11, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 293, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 109, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 607, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 37, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 185, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 444, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 448, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 603, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 381, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 60, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 271, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 186, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 380, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 58, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 138, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 423, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 196, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 350, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 442, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 355, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 463, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 568, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 586, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 624, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 603, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 357, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 438, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 389, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 113, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 382, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 122, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 472, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 172, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 211, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 173, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 71, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 273, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 586, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 311, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 215, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 624, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 255, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 427, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 152, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 229, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 348, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 617, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 434, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 114, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 267, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 527, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 255, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 431, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 248, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 608, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 424, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 267, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 196, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 90, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 55, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 248, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 119, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 506, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 92, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 227, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 19, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 508, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 91, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 275, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 261, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 273, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 192, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 34, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 605, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 404, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 432, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 503, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 464, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 132, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 520, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 283, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 494, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 611, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 295, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 449, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 194, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 500, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 203, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 476, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 294, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 267, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 84, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 289, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 511, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 334, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 413, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 97, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 235, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 510, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 529, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 30, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 287, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 199, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 71, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 310, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 384, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 328, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 259, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 537, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 321, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 352, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 576, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 16, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 551, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 602, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 524, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 25, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 269, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 586, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 17, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 619, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 611, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 146, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 593, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 444, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 100, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 311, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 10, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 353, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 23, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 521, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 0, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 597, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 442, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 534, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 349, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 20, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 348, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 483, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 616, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 197, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 65, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 327, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 264, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 393, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 32, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 225, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 93, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 572, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 541, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 39, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 8, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 66, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 383, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 95, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 241, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 499, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 172, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 574, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 603, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 345, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 625, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 105, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 451, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 202, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 492, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 136, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 224, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 460, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 274, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 125, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 159, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 150, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 226, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 256, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 495, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 338, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 579, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 69, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 85, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 73, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 455, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 316, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 599, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 439, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 436, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 89, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 147, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 10, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 594, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 453, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 390, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 372, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 108, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 107, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 221, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 326, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 211, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 285, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 330, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 70, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 106, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 137, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 332, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 333, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 590, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 520, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 482, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 426, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 443, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 410, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 286, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 624, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 189, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 401, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 501, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 447, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 95, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 350, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 239, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 448, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 39, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 16, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 401, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 294, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 464, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 407, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 233, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 578, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 116, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 144, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 454, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 621, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 103, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 483, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 310, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 468, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 472, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 146, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 49, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 344, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 1, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 463, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 433, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 256, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 64, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 466, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 62, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 514, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 338, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 564, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 472, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 465, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 282, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 489, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 465, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 184, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 291, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 366, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 283, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 474, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 86, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 262, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 40, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 225, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 341, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 323, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 78, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 535, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 423, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 462, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 596, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 512, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 262, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 82, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 136, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 90, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 473, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 501, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 44, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 58, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 601, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 171, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 323, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 137, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 518, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 37, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 414, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 380, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 73, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 295, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 190, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 362, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 5, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 266, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 388, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 205, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 444, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 485, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 82, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 371, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 533, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 624, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 103, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 249, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 104, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 105, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 20, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 17, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 154, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 250, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 193, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 512, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 344, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 235, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 376, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 63, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 190, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 448, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 594, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 208, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 485, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 287, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 474, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 37, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 142, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 37, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 177, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 397, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 296, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 401, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 507, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 613, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 448, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 104, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 358, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 249, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 99, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 191, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 295, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 383, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 564, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 449, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 94, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 299, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 403, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 576, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 94, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 425, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 116, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 398, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 48, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 164, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 38, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 519, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 11, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 209, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 521, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 478, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 26, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 16, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 268, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 594, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 480, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 177, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 83, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 213, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 402, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 437, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 281, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 417, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 256, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 229, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 377, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 225, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 85, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 316, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 196, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 268, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 269, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 605, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 132, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 490, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 300, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 576, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 495, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 368, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 119, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 29, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 101, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 174, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 221, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 165, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 582, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 324, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 258, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 153, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 370, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 328, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 488, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 375, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 387, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 62, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 256, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 519, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 304, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 21, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 28, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 513, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 174, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 511, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 425, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 382, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 100, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 397, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 452, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 97, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 568, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 249, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 410, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 86, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 310, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 329, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 39, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 370, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 33, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 71, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 11, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 396, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 598, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 483, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 367, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 96, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 165, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 160, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 199, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 148, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 103, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 613, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 228, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 330, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 232, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 566, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 155, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 568, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 404, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 37, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 2, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 204, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 196, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 59, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 109, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 405, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 4, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 228, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 520, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 88, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 589, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 105, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 16, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 152, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 249, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 425, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 511, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 340, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 318, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 99, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 107, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 193, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 251, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 64, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 266, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 134, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 278, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 204, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 0, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 209, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 397, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 115, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 529, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 35, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 478, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 328, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 139, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 460, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 534, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 11, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 67, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 165, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 439, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 70, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 510, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 319, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 225, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 41, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 125, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 106, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 625, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 62, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 465, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 464, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 451, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 205, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 443, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 504, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 445, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 462, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 134, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 247, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 565, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 31, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 460, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 275, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 17, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 70, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 285, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 193, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 383, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 185, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 161, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 287, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 253, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 323, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 445, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 576, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 195, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 127, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 402, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 457, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 67, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 604, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 444, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 102, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 228, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 138, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 373, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 15, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 9, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 443, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 394, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 58, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 544, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 394, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 454, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 477, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 69, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 427, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 312, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 423, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 535, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 107, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 54, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 212, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 216, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 103, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 35, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 147, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 422, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 118, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 4, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 214, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 309, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 48, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 331, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 247, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 125, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 264, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 285, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 122, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 366, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 511, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 318, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 235, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 32, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 533, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 560, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 361, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 283, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 72, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 267, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 370, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 207, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 507, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 579, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 328, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 408, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 73, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 214, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 239, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 223, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 276, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 591, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 275, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 367, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 393, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 586, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 556, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 236, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 317, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 35, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 598, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 3, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 265, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 506, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 238, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 111, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 0, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  {'page': 124, 'source': 'file_storage/DE_catalog.pdf'},\n",
       "  ...],\n",
       " 'documents': ['the conversationPositive Integers\\nconversation_created_ts The timestamp of conversation \\ncreationTimestamp\\nlast_message_ts The timestamp of the latest \\nmessage in the conversation Timestamp\\nloans_entering_repayment_45_da\\nyThe number of loans entering \\nrepayment within 45 days of a \\nconversation message that can be \\nattributed to the message \\naccording to the attribution rules \\ndescribed abovePositive Integers\\namount_entering_repayment_45_\\ndayThe amount of the loans entering \\nrepayment within 45 days of a \\nconversation message that can be \\nattributed to the message \\naccording to the attribution rules \\ndescribed abovePositive Float\\ncurrent_borrower_status The status of the borrower based \\non their latest valid depositChargedOff, Complete, Current, \\nB1, B2, B3, B4, B5\\ncurrent_risk_tier The borrower’s risk tier T1, T2, T3, T4',\n",
       "  \"3select * from public.card_account_application where borrower_id = 3857277; --- also exists as we'd expect\\n4select * from public.card_card where card_account_id = 5078;\\n5\\n6-- looking at the enabled CTE\\n7select * from public.feature_enrollment where borrower_id = 3857277; -- DOES NOT EXIST! makes sense because borro\",\n",
       "  \"453\\n1...\\n2create table {{temp_schema}}.daily_change as\\n3  with \\n4  daily_change_1 as (\\n5    select\\n6      borrower_id,\\n7      user_id,\\n8      account_id,\\n9      src_dt,\\n10      deposit_amt,\\n11      cash_refund,\\n12      refund_to_loan,\\n13      returned_payment_amt,\\n14      payments_to_loan,\\n15      credits_applied_to_loan,\\n16      payments_less_credit,\\n17      awaiting_payment_bal,\\n18      awaiting_payment_applied,\\n19      withdrawal_amount,\\n20      adj_net_change as net_change\\n21  from {{temp_schema}}.ap_check\\n22  ),\\n23  -- bring in Perpay+ and other feature revenue which first impacts --\\n24  daily_change_2 as (\\n25    select \\n26      cast(fp.created as date) as created,\\n27      fp.borrower_id,\\n28      f.type,\\n29      -- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! This is where the enrollment table gets used. \\n30      sum(case when f.type in ('perpay_plus', 'perpay_plus_v2') then fp.amount else 0 end) as perpay_plus_revenu\",\n",
       "  'a. The send dag only takes data from the raw data table with the most recent timestamp\\nb. The compliance check dag runs before the send as well, just in case \\nSample Generation Query\\n \\n1------------------------------------\\n2-- ANY BAD PAYMENT HISTORY\\n3-- Look at a few with status 93 or 97 (charged off)\\n4select consumer_account_number,\\n5       date_of_account_info,\\n6       enter_date,\\n7       payment_rating,',\n",
       "  'needs to change for all (some might just be grabbing columns for reports, others for DS, etc), but I’ll list them all so we’re aware and can \\ncheck. After the list I’ll highlight the files I believe are most important/ should be updated first, so keep reading down.\\nList from command F search:\\nconstants.py\\nbatch_jobs_dag.py\\nloan_attribute.py\\nplatform_payments.py?\\nplatform_deposits.py\\ncard_ach_payment_tracking.py\\ntest_card_ach_payment_tracking.py\\ncommit_card_ach_payment_tracking.py\\nmetric_layer_dag possibly\\ndomo_daily_summary_data_model.py\\ncollections.py (data_model_daily_summary/lib)\\nupdate_rows.py (data_model_borrower_hist_daily/lib)\\ninsert_rows.py (data_model_borrower_hist_daily/lib)\\ndomo.py (data_model_borrower_hist_daily/lib)\\nbalance_delq.py (data_model_borrower_hist_daily/lib)\\nreconcile.py (data_model_accounting/daily_account_reconciliation_data_model/reconcile.py)\\ncore_account_daily_rollforward.py\\nreconcile.py (data_model_accounting/account_reconciliation/reconcile.py)',\n",
       "  '368\\nMarketplace Credit Reporting Review (Current Deployment)\\nThis document outlines the process at which the current (launch until at least 2022-12-02) credit reporting automation was launched. As \\nPerpay is launching new credit products (ex: the credit card) there is a need to perform many updates to this logic. Years of learnings is \\nleading to the conclusion that an entire re-build should be performed in order to add additional flexibility, observability, and validation. \\nHowever, getting our understanding completely up to date with what is currently deployed is paramount before any future action. The below \\nis from the perspective of the entire DAG run…\\nGeneral Notes\\nAll construction appears to always be in reference to Experian. Only at time of sending the data is there bureau-specific code/references\\n1. Build the external table for finding the borrowers (here) that need to be sent to the credit bureaus and then populate (here) - this is a',\n",
       "  'effectively get rid of any time saved from using CTEs. To check whether a query includes any disk-based steps:\\n1. Run the query\\n2. Retrieve the Query ID from sql_qlog:\\na. select query, elapsed, substring from sql_qlog order by query desc limit 5;\\n3. Check for which steps is_diskbased is true\\na. select * from svl_query_summary where query = MyQueryID order by stm, seg, step;\\nMore on the information available in the svl_query_summary table\\nIn addition to using CTEs, the execution plans of each query were analyzed to reveal particularly inefficient operations. JOIN statements that \\nwere especially costly were rethought to be more efficient. To identify the most problematic JOIN statements the relative cost indicated by \\nthe execution plans was used in conjunction with the general rule that nested loop JOIN statements are the most inefficient, followed by \\nhash JOIN statements, with merge JOIN statements being the most efficient. Where two large tables were being joined with each other,',\n",
       "  'applicable.\\n2. If a column is removed we won’t delete it outright from your data warehouse but “soft-delete” it and mark future records NULL so \\nthat the old data remains as a historical point of reference.\\n3. If a column’s data types change, we will do our best to losslessly accept both the old and new data by retaining the old column \\nand creating a new column with a data type accommodating both the old and new data. You can then create views off that table.\\n4. If your source adds a new table, we will begin syncing it like any other.\\n5. If a table is deleted in your source, we will leave it alone in your warehouse.',\n",
       "  '17    card_transaction_id ,\\n18    deserve_id ,\\n19    created_ts ,\\n20    transaction_status ,\\n21    rank () over (partition  by card_transaction_id order by created_ts asc) as rank,\\n22    case\\n23        when created_ts = first_value (created_ts ) over ( partition  by card_transaction_id order by created_ts d\\n24        else 0 end as current_ind\\n25from status_1\\n26where ((transaction_status <> transaction_status_previous ) OR (transaction_status_previous IS NULL));\\n27\\n28\\n29select\\n30    count(*) as cnt,\\n31    card_transaction_id\\n32from users_frank .card_transaction_status\\n33group by 2 having cnt > 2 order by cnt desc;\\n1select *\\n2from public.card_transaction_snapshot\\n3where card_transaction_id = 129165\\n4order by created desc;',\n",
       "  '376\\nextract the key/value pairs and create the CSV name for S3\\nextracts the line items and create CSV name for S3\\nmake sure to add the layer for the pandas package if used. → this is the photo in the above section\\n2: Probably a good idea to increase the lambda function timeout from 3s to 2m',\n",
       "  '6        - create or update card account from api data\\n7        - if ACCOUNT_CREATED create card from api data\\n8    APPLICATION_TYPE = \"application\"\\n9        - create or update application status from webhook data\\n10        - update application approval type\\n11    STATEMENT_TYPE = \"statement\"\\n12        - create or update statement from webhook data\\n13        - update internal representation of auto pay schedules\\n14    PAYMENT_TYPE = \"payment\"\\n15        - get or create card payment status\\n16    USER_TYPE = \"user\"\\n17        nothing',\n",
       "  '293\\n20\\n23-\\n11-\\n09 \\n21:\\n50:\\n30    0               14\\n36\\n9.5\\n79\\n1.1\\n28\\n37\\n43     ajs\\n-\\nne\\nxt-\\n4a\\nb8\\n9b\\ncc\\nc1\\n48f\\n80\\ncf6\\n93\\n8fa\\n6e\\n9c\\n94\\n5c\\na Ap\\npro\\nve\\nd 0                 0       0                     0           0          0            0    0           0      0        Ka\\nsh\\nKic\\nk        13\\n06\\n57\\n9    On\\nbo\\nard\\ning \\nVe\\nrify \\nPh\\non\\ne \\nNu\\nmb\\ner    26\\n80\\n0         0           On\\nlin\\ne \\nTra\\ncki\\nng \\nLin\\nk                           11\\n35\\n26\\n2  O\\nNL\\nIN\\nE_\\nTR\\nAC\\nKI\\nN\\nG_\\nLI\\nNK Ca\\n3c\\n18\\n82\\n9-\\n7f7\\n3-\\n11\\nee-\\n98\\n1f-\\n6fe\\n27\\n33\\n1f8\\n19/\\n13\\n06\\n57\\n9/1\\n13\\n52\\n62/\\n50\\n04\\n10\\n11\\n4    \\n20\\n23-\\n11-\\n09 \\n21:\\n50:\\n25    0               14\\n36\\n9.5\\n79\\n1.1\\n28\\n37\\n74     ajs\\n-\\nne\\nxt-\\n24\\n22\\n07\\nb1\\n16\\n66\\ne0\\ndf8\\nbbf\\n9e\\n73\\n81\\n50f\\ncd\\n9 Ap\\npro\\nve\\nd 0                 0       0                     0           0          0            0    0           0      0        Bri\\ngit           20\\n48\\n46\\n7    On\\nbo\\nard\\ning \\nVe\\nrify \\nPh\\non\\ne \\nNu\\nmb\\ner    26\\n80\\n0         0           Bri\\ngit\\n_C\\nobr\\nan\\nde\\nd \\nLP\\n_3.\\n7.2\\n3                       16\\n26',\n",
       "  \"Question: Attached first is the record of the payment in the deserve_payments_eod_report sftp. It shows that the payment was \\ncompleted on 2022-09-29 and returned on 2022-10-03.\\nLooking at the second attachment, Engineering's table (data received via webhooks), it looks like the COMPLETED status was created \\non 2022-10-03, but we do see that Deserve modified the record on 2022-09-29, which happens to be that same date Deserve said the \\npayment was completed in the sftp...\\nthis is funky and we're not sure which date to believe/whether to rely on the webhooks data or the SFTP. Is there any way for you to \\ntell/ask how this difference between the webhooks and sftps might have occurred so we can gain insight on which is more accurate?\\ndeserve_payment_14488.csv \\nengineering_payment_14488.csv \\nResponse: Our current practice for ACH payments is below:\\n1: Customer initiates ACH payment\\n2: Perpay reports ACH payments to Deserve immediately upon initiation (corresponding to initiated_at field)\",\n",
       "  'them or update them incorrectly, causing the parser to capture incorrect information. This leads to the underpayment of bills and \\nwork on DE’s end to fix broken DAGs. Our invoice matching v2 DAG also checks that Docparser pulls in very specific fields, which \\ndoes not need to be the case for every vendor. Thus, the removal of this will be a lot less touchy. I will obtain an updated list of all \\nrequired fields from Preston, which we can check for downstream in the code before pushing the data to QB. \\nSimilar to CSVs, we can utilize Zapier for all attachments into S3, including the PDFs.\\nAmazon Textract uses ML to extract data from documents and forms. One of its specialties is analyzing expense documents like \\nInvoices (check out this invoice extraction example using their dummy invoice- you can also upload your own to see how it’ll parse). \\nI want to verify all our current invoices look good through their test parser, but if that’s the case and it captures everything we need, I',\n",
       "  'Network A network monitor checks whether a \\ngiven endpoint is active. Monitors also \\ncan alert over a percentage on a cluster \\nbased on custom network tags defined \\nin the Agent and host tags. \\nOutlier Outlier monitors detect when members of \\na group (e.g., hosts, availability zones, \\npartitions) are behaving unusually \\ncompared to the rest. They are useful for any metric collected via \\nthe Datadog Agent or API for which a given \\ngroup should behave uniformly.\\nProcess Check For each process, a single service check \\nstatus is produced. \\nService Check Service check monitors are useful when \\nmonitoring user-defined checks, such as \\nthe status of an application. The Datadog',\n",
       "  'possible_partial_returned_p\\nayment_indInteger An indicator for loans that have had \\nassociated returned payments. This value only Loan Level \\nVariancesColumn Data \\nTypeDefinition Additional \\nDetail',\n",
       "  'accounts_payable_int.ecomm_po_data are finished.\\nii) accounts_payable_int.variance_sheet_output_history can be created once \\naccounts_payable_int.variance_sheet_output is finished.\\n3:\\naccounts_payable_int.po_matches and accounts_payable_int.po_variances may be run concurrently, once everything \\nmentioned in 2 completes (and once accounts_payable_int.qb_bills completes).\\n4:\\nDump variances into sheet & create accounts_payable_int.existing_bills_for_qb_upload and \\naccounts_payable_int.new_bills_for_qb_upload. Send contents of these new tables to quickbooks.\\n5:\\ni) accounts_payable_int.po_matches is inserted into accounts_payable_int.qb_history and \\naccounts_payable_int.po_matches and temp_marketplace_int.po_matches are dropped and it’s ddl is recreated.\\nii) Tests run once the tables they’re testing have finished updating. \\nDiagram\\nHere’s a diagram which offers a visual perspective. Tables are in yellow with their creation indicated by dotted lines.',\n",
       "  'perpay_plus_opt_in_ts and perpay_plus_status, specifically looking for pp+ v1 or v2 enrollment.\\n8left join {{public_schema}}.feature c on b.feature_id = c.id\\n9group by cast(a.created as date), c.type;\\n10...\\n1with card_cte as(\\n2    select u.uuid as user_uuid, u.email, fe.status\\n3    from {{pub_schema}}.feature_enrollment as fe\\n4    left join {{pub_schema}}.borrower as b on fe.borrower_id = b.id\\n5    left join {{pub_schema}}.user as u on b.user_id = u.id\\n6    where fe.feature_id = 67 and fe.status = \\'enabled\\'\\n7)\\n8...\\n9<card_cte never gets called!>\\n1    def test_users_in_ppp_v1v2(self):\\n2        # test for if any users have any status in BOTH perpay plus v1 or v2, should be one or another\\n3        query = f\"\"\"select\\n4                        borrower_id, \\n5                        count(*) as cnt\\n6                    from public.feature_enrollment\\n7                    where feature_id in (35, 133)\\n8                    group by 1\\n9                    having cnt > 1;\\n10                \"\"\"',\n",
       "  '466\\nmonthly_fee_adjustment\\nlate_payment_feeThe sum of all MONTHLY_FEE transactions associated with a given card account on a given src_dt. The transactions are associated \\nwith the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\nmonthly_fee field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nMONTHLY_FEE.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:',\n",
       "  \"catch, that is, the scenario where a payment is initiated at \\n23:59:59.9 on one day, and the payment details are generated \\njust milliseconds later on the following day. This situation will \\nnecessitate a workaround by DE, which is likely to be a \\npermanent solution, given that it's a reasonable timing-related \\nissue.Abby is testing a PR on staging to \\nresolve this issue. The issue only \\naffected two loans in the current \\ntable, loans 2561427 and \\n4816842. It is an edge case, so \\nthe PR is going in to prevent it in \\nthe future. Technically this is a \\nworkaround, but because it is a \\nmillisecond timing issue upon \\nrecord creation in the public \\ntables, there’s nothing that can be\",\n",
       "  '335\\nData Discrepancies v2 (11/29/22)\\nPerpay Core Issues:\\n12 auto_pay_payment id: \\n891 \\ndeposit id: 2744109Payments that are \\nmarked returned in the \\npayment_achtransacti\\non table are not marked \\nreturned in the deposit \\ntable. 542 returned \\ntransactions (~41% of \\nreturned transactions)Appears to be resolved \\nas of 10-29-2022.\\n Backfill complete! - \\nTaylor\\n13 auto_pay_payment id: \\n416 \\ndeposit id: 2685390Deposit amount is \\ndifferent from auto pay \\npayment & ACH \\nTransaction amount. 1 payment This one’s weird!  I will \\nexplain and we can \\ndecide if any action \\nneeds to be taken.  This \\nscenario should not \\nhappen again as we’ve \\nmade changes to the \\nACH payment \\nprocessing.Issue # Example Issue Impact Resolution',\n",
       "  \"515\\nBefore we get too deep, I’d like to consider renaming all of the fields to better reflect what they capture. Strong names will avoid \\nambiguity and increase readability and maintainability for our team.\\nI’ve put 500 variances here I’ve identified, and am now doing the same for 1,000 variances here. I think Google Sheets provide a \\nuser-friendly format for referencing example cases, which could aid in better understanding the situation as a group. I'm open to 4 COMPLETED abs collections_newly_pending_deposits: newly pending \\ncollections deposits shouldn't be includedputting in pr1 - trying to get the table to a \\nbetter base state\\n5 COMPLETED abs withdrawals_newly_pending: This is p much the \\nopposite of 1. It’s when eng removes withdrawals from the \\nbalance. withdrawals newly pending need to be \\nsubtractedputting in pr1 - trying to get the table to a \\nbetter base state\\n6 COMPLETED abs commerce_borrower_credits_for_awaiting_payment_\\nnew: don't subtract\",\n",
       "  '452\\nIterable Send\\nThe Iterable Customer Data Table / Generate Data Task\\nThis impacts what we’re sending to iterable in the card dict. Particularly, the enrollment table dictates the 1 or 0 flag, enabled_ind, for the \\ncard. \\nIn another spot, it’s used for vantage score. I think this part will be unaffected by the cards in the wild, as we look for feature_ids of 35 and \\n133 which seem specific to scores (I think are pp+ features only).\\nMarking generate_data (iterable send) as ❌  since it’s affected in at least one way by the enrollment table. \\nAccount Rec\\nAccount Balance\\nThe only time enrollments are used are for pp+ revenue v non pp+ revenue as fields perpay_plus_revenue and \\nother_feature_revenue. Any other feature revenue, whether involving in the wild cards or not, would fall under other_feature_revenue, \\nso this file should be unaffected. \\n1...\\n2    card_dict_cte as (\\n3        select\\n4            b.id as user_id,\\n5            p.card_account_id,',\n",
       "  'commensurately.\\n2. borrower_id = 4251822 / card_account_id = 14777 / card_payment_id = 250680\\ndeserve_payment_250680.csv \\ndeserve_account_balance_14777.csv \\nVariance 10: Missing International Transaction Fees\\nAlready resolved- won’t attach examples',\n",
       "  \"subtractedputting in pr1 - trying to get the table to a \\nbetter base state\\n6 COMPLETED abs commerce_borrower_credits_for_awaiting_payment_\\nnew: don't subtract \\ncommerce_borrower_credits_for_awaiting_payment_newputting in pr1 - trying to get the table to a \\nbetter base state\\n7 COMPLETED abs card_payment_returns: should subtract \\ncard_payment_returns from the balanceputting in pr1 - trying to get the table to a \\nbetter base state\\n8 COMPLETED abs commerce_loan_payments_for_canceled: add \\ncanceled loan paymentsputting in pr1 - trying to get the table to a \\nbetter base state\\n9 COMPLETED \\nremoved \\nfrom eqtn\\nIN PROGRE… \\ncomplete \\nunderstand\\ningabs/andr\\newcommerce_cash_refunds: don't include \\ncommerce_cash_refunds- when should these be taken \\ninto account?putting in pr1 - trying to get the table to a \\nbetter base state\\nfollow-up questions put in channel & \\nwaiting for response\\n10 COMPLETED abs/erne\\nst withdrawals_canceled: should be added to balance committed straight to master bc such a\",\n",
       "  '106\\n107        # Load parameters from config.py if params argument is not provided\\n108        if not params:\\n109            params = load_params (renderer .file_path )\\n110\\n111        renderer .render_query (query_template , params=params)\\n112\\n113    else:\\n114        print(f\"Unsupported file type: {file_extension }\")\\n115\\n116if __name__ == \\'__main__\\' :\\n117    main ()',\n",
       "  \"480\\npayment of $100 on the 18th which is why the total decrease in account balance was $850 total). Do you know why Deserve \\ndecremented that card account balance by $750 the day after we'd expect? Let me know if any other data would be useful for you.\\naccount_rollforward_847f677-1d00-4fb4-84e2-23a5fd612da5.csv \\npayments_d847f677-1d00-4fb4-84e2-23a5fd612da5.csv \\nResponse: Nothing jumps out. I believe the SFTP files are generated by Deserve whereas the processor (Corecard) actually maintains \\nthe ledger, so it’s possible there’s a delay between the two that caused this. I’ll raise to them in a ticket and lyk of their response.\\nFollow-up: Bit in the weeds but in these situations it looks like Deserve is reporting the payment to corecard immediately, but there is a \\nslight delay in corecard updating the balance (delay of circa 2 min). Apparently the slight delay is causing the EOD reporting that we\",\n",
       "  '→ Triggers Zapier\\nZapier extracts attachement, dumps in s3 bucket (in the Rymax Excel folder)\\ns3://ana-stage-s3-accounts-payable-d3tcw/rymax_statements_excel/\\n→ Triggers Lambda function\\nLambda function converts excel to csv, dumps in another folder\\ns3://ana-stage-s3-accounts-payable-d3tcw/rymax_statements_csv/\\nThis triggers the lambda function trigger-rymax_csvs_statements_glue_database-crawler, which triggers the next glue crawler\\nGlue runs and creates/updates the table in Glue Database\\nrymax_csvs_statements_glue_database-crawler\\nhttps://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#crawler:name=rymax_csvs_statements_glue_database-\\ncrawler\\nAn event rule (rule-rymax_csv_to_parquet_glue_job) will trigger a lambda function that triggers the next glue job\\nGlue job runs to convert csv to parquet, places parquet in rymax_statements folder in perpay_accounting_datamart folder\\nrymax_csv_to_parquet_glue_job',\n",
       "  \"po_matches table and the 'No's are still removed from the po_variances table. The sheet will clear on whichever following run has \\nnew data to insert into it.\\nNew data is added to the accounts_payable_int.variance_sheet_output_history table using an INSERT INTO. This way, we \\navoid recreation of the table each time and maintain an accurate historical ledger. If this pipeline is ran for the first time in prod, it \\nrequires the table to exist.\\nThe code to ingest data from the variance sheet into accounts_payable_int.variance_sheet_output lies in the class \\nManualPOMatching, within the manual_po_matching_class.py file (PATH: \\ndags/analytics/accounts_payable_revamp/lib/po_matching/manual_po_matching_class.py).\\nThe code to insert accounts_payable_int.variance_sheet_output data into \\naccounts_payable_int.variance_sheet_output_history lies in variance_sheet_output_history.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/ingesting_data/variance_sheet_output_history.py).\",\n",
       "  '495',\n",
       "  '565\\nInterviews- Questions\\nFull-time\\nRemote challenge \\nproblem set\\nOn-site interview\\nSlides can be found here\\nWhiteboard questions\\nBuild the Reddit/twitter/Facebook/ect data base architecture\\nCreate a function that reverses the key value pair\\nData Model questions\\nFamiliarity with data model structures?\\nStar/snowflake schemas? When to use?\\nHow wide should a table be?\\nAt Perpay, we track users/borrowers/marketplace/core - what kind of datamodels would be appropriate?\\nHow do you ensure that the data is accurate?\\nValidation techniques\\nWhat is better, known incorrect values in tables or stale data?\\nWhat levels of proficiency do you require from analysts as they add new tables\\nHave you worked with 3rd party marketing platforms before?\\nHave you dealt with rate limits?\\nHow would you make data stored in S3 available to analytics who mainly work in SQL editors\\nWhat are some mythologies to writing clean and efficient SQL code?\\nWhat are some considerations for data governance and permissions?',\n",
       "  '61        # Get the schema based on the environment and other conditions\\n62        from utils.config_local import ConfigLocal\\n63        import utils.constants as constants\\n64\\n65        if schema_name in (constants .TEMP_MARKETPLACE_SCHEMA , constants .TEMP_SCHEMA_DEV ):\\n66            return constants .TEMP_SCHEMA_DEV\\n67\\n68        local_env_vars = ConfigLocal ().generate ()\\n69        return local_env_vars .get(\\'REDSHIFT_DEV_SCHEMA\\' ) if not external else constants .USERS_EXTERNAL_GENER\\n70\\n71def load_params (file_path ):\\n72    # Load parameters from config.py located in the same directory as the file_path\\n73    config_path = Path(file_path ) / \"config.py\"\\n74    spec = importlib .util.spec_from_file_location (\"config\" , config_path )\\n75    config = importlib .util.module_from_spec (spec)\\n76    spec .loader.exec_module (config)\\n77    return config.params\\n78\\n79def main():\\n80    # Set up argument parser\\n81    parser = argparse .ArgumentParser (description =\"Render SQL query from template\" )',\n",
       "  '4. They request that these tables have no index or column headers and while columns can be blank, they strictly require the correct \\nnumber of columns (and therefore commas) to be present.\\na. Even though they do not want column headers, a perpay-specific header is added by request so they can automatically process \\nwhen the tables are loaded to their sftp server\\n5. They require this data on a daily basis, so code is set up to pull single day data\\na. Code is built to run also by command line where a data range can be supplied if necessary\\nLogic Summary:\\n1. Code queries redshift to create driver file for all loans for users we have pulled DataX data for that went to complete/repayment or only \\nmade a payment towards their loan.\\n2. All required attributes are added to the driver table and then its pulled down for further processing\\n3. A dataframe is created (as a subset of the driver) for only users that just became FTB and ones that completed their loan for the \\nupdates table',\n",
       "  'applied to our calculated account balance on the correct date in \\nall cases (or not at all). In addition, I’m seeing inconsistencies \\nbetween the amount of the payment that it is returned and the \\namount the balance increases.The returned_payments field was \\nbeing duplicated by a similar issue \\nto variance no 2. Thus, this was \\nfixed by referencing the new \\nloan_id column in \\nplatform_payments as well. In \\naddition, the ending_balance_calc \\ntable hadn’t taken Status Primary \\nDE \\nMember(\\ns)Summary Status Detail',\n",
       "  \"previous page the user was on) -- if this is null, then its likely that the user visited the invalid page by mistake (eg they manually \\ntyped in the invalid URL into their browser's navigation bar, which brought them to the invalid page)\\n1/dashboard\\n2/search\\n3/checkout/onepage/success/\\n4/\\n5/orders/\",\n",
       "  \"catch, that is, the scenario where a payment is initiated at \\n23:59:59.9 on one day, and the payment details are generated \\njust milliseconds later on the following day. This situation will \\nnecessitate a workaround by DE, which is likely to be a \\npermanent solution, given that it's a reasonable timing-related \\nissue.Abby is testing a PR on staging to \\nresolve this issue. The issue only \\naffected two loans in the current \\ntable, loans 2561427 and \\n4816842. It is an edge case, so \\nthe PR is going in to prevent it in \\nthe future. Technically this is a \\nworkaround, but because it is a \\nmillisecond timing issue upon \\nrecord creation in the public \\ntables, there’s nothing that can be\",\n",
       "  '342\\nCard Statement Calculations\\nDate logic in statements:\\nIn UTC (database):\\nFor the first statement:\\nstart_ts = timestamp card entered ‘pending’ state\\nend_ts =\\nif day(start_dt) = 30 or 31 → 29th of the following month\\nif day(start_dt) = 1 → 29th of the current month\\nelse → day(start_dt) of the following month\\ndue_dt =\\nend_dt + 23 days**\\nThe 30th, 31st, and 1st do not count as days\\nnext_statement_ts = day(end_ts) of the following month\\n \\nOnce the day values of these are set, they do not change month to month. So…\\nFor the 2nd statement (and so on):\\nstart_ts = next_statement_ts from previous statement\\nend_ts = day(start_dt) of the following month\\ndue_dt = day(due_dt from previous statement) of the following month\\nnext_statement = day(end_dt) of the following month\\n \\nIn EST (i.e., actual statements):\\n \\nHow are the fields calculated?\\nprevious_balance\\nThe balance on the statement prior to the current statement. If it’s the first statement, this value is 0.00.\\ntransaction_total',\n",
       "  'Pages Events353\\n           Group Permissions355\\n      Projects356\\n           Deployed357\\n                Amplitude Event Reduction358\\n                Accounting DAG Delay365\\n                Marketplace Credit Reporting Review (Current Deployment)368\\n                Accounts Payable Revamp - Q3 OKR369\\n                     Setting Up AWS Lambda w AWS Textract375\\n                Account Reconciliation V2377\\n                     Core Account Balance Daily Ledger379\\n                          Engineering Core Account Daily Rollforward380\\n                          Accounting Core Account Daily Rollforward386\\n                          DE Notes on Initial Core Discrepancies390\\n                     Commerce Loan Balance Daily Ledger393\\n                          Engineering Commerce Loan Daily Rollforward394\\n                          Accounting Commerce Loan Daily Rollforward397\\n                          DE Notes on Initial Loan Discrepancies400',\n",
       "  'schemas.\\nreporting_daily\\ndata_model_finance_planning\\ndata_model_intercom_conversations\\ndata_model_ecommerce \\ndata_model_platform_credits \\ndata_model_daily_summary \\ndata_model_ecommerce \\ndata_model_messaging_daily \\ndata_model_payment_plans \\nmarketing_product_catalog (writes to staging S3 bucket)',\n",
       "  \"5: Log '[InvoiceMatchingV2] Removing invoices marked as delete from DB - table: po_invoice_overview \\n...'\\n6: update the perpay_accounting_datamart_ext.po_invoice_overview table\\n- Join the info from the sheet (from temp_int.invoice_matching_sheet_temp) onto \\nperpay_accounting_datamart_ext.po_invoice_overview where delete = 'Y' and save that to temp_int schema \\nas temp_int.po_invoice_overview\",\n",
       "  '13    left join public.card_cardpaymentstatus b\\n14        on a.id = b.card_payment_id\\n15            left join public.card_account c\\n16            on a.card_account_id = c.id\\n17            left join public.borrower d\\n18            on c.borrower_id = d.id)\\n19select * from payment_status_1 where payment_status is null or created_ts is null;\\n1select * from temp_int.card_payment_status where payment_status is null or created_ts is null;',\n",
       "  'Creating DAG Tasks119\\n      Troubleshooting122\\n           Common Errors123\\n                (Error) Duplicate Rows created by DMS124\\n                (Error) Marketing Daily Row Count126\\n                (Error) Iterable ShortIO Column Length130\\n                (Error) Accounts Payable/Invoice Matching131\\n                (Error) Company Switch Mid-Run133\\n                (Error) Card Payment Status Nulls134\\n                (Error) Card Metric Layer Failures137\\n                (Error) Card DM Full Failure142\\n                (Error) Nulls in Card ACH Payment Tracking table143\\n           Common Warnings144\\n                (Warning) Multiple Primary Jobs145\\n                (Warning) Parent/Child Multi-sourcing relationship147\\n           Infrastructure Issues152\\n                (Issue) DMS Failures153\\n                (Issue) WLM Configuration for Parameter Group154\\n                (Issue) Hanging Airflow Task(s)156\\n                (Issue) Killed Workers157',\n",
       "  '183\\nGreat Expectations\\nOverview:\\nThis document outlines the general guidelines to be used to create great expectation suites for data validation at Perpay.\\nPurpose:\\n1. Great Expectations is an open source tool to run and document data validation tests. These tests include validation column values to be \\nbetween certain values, null checks, and general column statistics like mean/median.\\n2. We use Great Expectations at Perpay to alert when data is out of the norm. This prevents us from sending inaccurate data to our 3rd \\nparty integrations (such as Iterable) as well as our internal reporting tool (i.e., Domo).\\n3. We would like to standardize the validation cutoffs used to determine alerting thresholds for the expectation suites. They are outlined in \\nthis document below.\\nExpectation Guidelines:\\nWe currently run the following tests:\\nUniqueness (expect_column_proportion_of_unique_values_to_be_between)\\nTo be run on join key for the table, and any other column that should have unique values',\n",
       "  'Testing label. After that, you can play around with the Zap Abby Test PDF Invoices. This one is already set up to pull from the \\nAbby Testing Gmail label, uses the AP Revamp-Stage folder in Google Drive, and plops invoices into the Staging S3 Bucket.\\nIt’s a good idea to clear out the S3 folders, Google Drive, and reset the tables here before you start testing specific invoice \\nbehaviors. This will remove any prior test invoices from the process so you can start fresh.',\n",
       "  '61        # Get the schema based on the environment and other conditions\\n62        from utils.config_local import ConfigLocal\\n63        import utils.constants as constants\\n64\\n65        if schema_name in (constants .TEMP_MARKETPLACE_SCHEMA , constants .TEMP_SCHEMA_DEV ):\\n66            return constants .TEMP_SCHEMA_DEV\\n67\\n68        local_env_vars = ConfigLocal ().generate ()\\n69        return local_env_vars .get(\\'REDSHIFT_DEV_SCHEMA\\' ) if not external else constants .USERS_EXTERNAL_GENER\\n70\\n71def load_params (file_path ):\\n72    # Load parameters from config.py located in the same directory as the file_path\\n73    config_path = Path(file_path ) / \"config.py\"\\n74    spec = importlib .util.spec_from_file_location (\"config\" , config_path )\\n75    config = importlib .util.module_from_spec (spec)\\n76    spec .loader.exec_module (config)\\n77    return config.params\\n78\\n79def main():\\n80    # Set up argument parser\\n81    parser = argparse .ArgumentParser (description =\"Render SQL query from template\" )',\n",
       "  \"46);\\n1select * from users_rehmet.variance_sheet_output_history_additions;\\n1insert into temp_marketplace_int.variance_sheet_output_history\\n2    select * from users_rehmet.variance_sheet_output_history_additions;\\n1select\\n2    *\\n3from temp_marketplace_int.variance_sheet_output_history\\n4where invoice_number in (\\n5    -- add your list of invoice_numbers here\\n6    '2459777-IN',\\n7    '2459847-IN',\\n8    '2460142-IN',\\n9    '2460079-IN'\\n10)\\n11order by invoice_number, created_ts desc;\",\n",
       "  'status will still carry with them a non-null initiated_at timestamp. \\nEnding Balance\\nThe result of the equation should produce the ending balance for the given day that is reported by Deserve.\\nThe ending balance to be checked against is first sourced from the Deserve web-hook data. If there is no balance reported for the account \\nin the Deserve web-hook the data, then the Deserve SFTP data is checked.\\nCommerce Report Definitions & Nuances\\nDaily Equation\\nFor commerce account reconciliation, on every day the following equation should hold true for any given marketplace loan:\\nstarting_balance\\n– payments\\n– borrower_credits\\n– refunds\\n+ returned_payments\\n= ending_balance = ending_balance_calc\\nNuances\\nStarting Balance\\nThe table captures loan balances on a daily level. This is the balance of the loan at the start of the day, before any funds have been moved \\nin or out. For this process, data is pulled from the public.loan and public.payment_loanprincipalbalancehistory tables. The starting',\n",
       "  \"164\\n10/24/2\\n3https://g\\nithub.co\\nm/Perp\\nay/perp\\nay-\\nairflow/\\ncommit/\\n741ca0\\n8f11ce8\\n8a6a7e\\n421495\\n18ab54\\nc0fc8a8\\n78 10/22/23 Withheld \\nunique_key \\n'8548232_2651\\n582_response' \\nfrom \\nreferral_fact.there's 4 instances \\nof the \\n8548232_2651582_\\nresponse \\nunique_key i the \\nreferral_attribute \\ntable. This is from a \\nduplicate in the \\npublic.referrals\\n_referralrespons\\ne table. Because I \\nknow about this \\nissue and it’s the \\nweekend, I will \\nreach out to \\nengineering \\nmonday. But I don’t \\nwant the error to \\nkeep firing and I \\nwant the dag to \\ncomplete tomorrow \\nmorning, so I’m \\ntaking out currently \\nerroneous \\nborrowers.  https://github.co\\nm/Perpay/perpa\\ny-\\nairflow/commit/\\n5e07f4de413fe\\n2390d9baa123\\n6cff12efed9285\\n5 select * \\nfrom \\npublic.ref\\nerrals_ref\\nerralrespo\\nnse where \\nreferral_i\\nd = \\n8512486 \\nand id = \\n2651582; \\n  11/19/202\\n3Held out a card \\npayment id for \\nthe card select \\n* from \\ncard_int.paym\\nent_fact \\nwhere \\npayment_id = \\n368930;I thinkkkk that two \\nstatuses occurred at\",\n",
       "  'open if engineering gave it an active or suspended status\\nclosed if engineering marked it anything other than active or suspended as a status (either charged_off, terminated, closed)\\nA card account is tied to a single borrower. \\nA card account may have multiple cards. However, the cards must be of a single type, and only one card under the account may be \\nACTIVE at a time.\\nborrower: a Perpay user who has been approved for a credit card. \\nA borrower may have multiple card accounts, with multiple cards under each account.\\nuser:\\ntransaction: a transaction between a borrower and merchant via the card\\nA transaction may be PENDING, CLEARED, or SETTLED.\\nTransactions have not been run through a process yet are considered PENDING transactions.\\nTransactions that make it through the clearing process are considered CLEARED transactions.\\nTransactions that make it through the settlement process are considered SETTLED  transactions.\\nThe term transaction and purchase are used synonymously.',\n",
       "  \"252\\n8       payment_history_profile,\\n9       account_status,\\n10       credit_limit,\\n11       highest_credit_or_original,\\n12       terms_frequency,\\n13       actual_payment_amount,\\n14       current_balance,\\n15       amount_past_due,\\n16       original_charge_off_amount,\\n17       date_last_payment\\n18from perpay_risk_datamart.equifax_raw_data as a\\n19left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n20where (payment_history_profile like '%1%' or payment_history_profile like '%2%' or account_status != '11' or pay\\n21and enter_date = max_enter_date\\n22order by account_status;\\n23\\n24------------------------------------\\n25-- OPT INS\\n26-- The min_enter_date filter is getting perpay plus opt ins that are newly reported\\n27select\\n28    distinct\\n29       a.consumer_account_number,\\n30       date_of_account_info,\\n31       enter_date,\\n32       payment_rating,\\n33       payment_history_profile,\\n34       account_status,\\n35       credit_limit,\",\n",
       "  'During our planning, we came up with several questions relevant to this project:\\n1. Does dropping and recreating tables remove their permission settings? The answer seems to be YES.\\n2. Is there a difference between DROP and DROP CASCADE we need to consider? DROP CASCADE helps remove the views created \\nbased on the table being dropped. As far as I can tell, we do not use views, so DROP CASCADE isn’t something we need to worry \\nabout.DE DS DA Risk Engineerin\\ngAccountingMarketing Product E-\\ncommerce\\nAdmin         \\nSuperuser Superuser        \\nUser User User User User User User User  \\nInternal \\ninternsInternal \\ninternsInternal \\ninternsInternal \\ninterns     \\nExternal \\ninternsExternal \\ninternsExternal \\ninternsExternal \\ninterns     Risk& Analytics External Teams',\n",
       "  '7',\n",
       "  'amount is considered to be a completed direct deposit on the date of the created timestamp, and it is attributed to the associated \\naccount_id.\\nReturned Direct Deposits: Rejected deposits are sourced from rows in the deposit table with a content_type_id of 46 and a status \\nof returned. The amount is considered to be a rejected direct deposit on the date of the modified timestamp, and it is attributed to the \\nassociated account_id.\\nPayment Tracking\\nThe following are the seven types of payments that impact the core balance and how DE derives the amount of that payment on a given day \\nfor a user’s account.\\nInitiated Credit Card Payments: Initiated credit card payments are sourced from Deserve’s SFTP data. Each unique payment_id with a \\nnon-null initiated_at timestamp is considered to be an initiated credit card payment of the amount indicated in payment_amount on the \\nday of the initiated_at timestamp for the associated account_id.',\n",
       "  'A great resource you can reference is the Github repository of Datahub, especially their docker-compose files (the linked page is for v0.13.1 \\nspecifically, you should update this as we bump versions) for the quick start option. In case there are any changes to the image versions \\nused for external packages like ElasticSearch, they will be obvious here.\\nSecrets\\nSecrets in AWS Secrets Manager need to be updated as necessary. The ARNs for these secrets will then be used in the task definition in \\nthe Terraform stage.\\nLog Group\\nFor the MSK cluster, we also manually created a log group name /msk/datahub/.\\nTerraform StepsFor the first prod deployment, the ana-prod-datahub-environment-variables bucket and the ECR repositories for the Datahub \\nimages have been created manually. Might be worth Terraforming this in the future. Bucket versioning is enabled.\\nOnly for the first prod deployment: When deploying to prod for the first time, we will need to prepare ALL secrets. This won’t be',\n",
       "  '2: Logs \"[InvoiceMatching] Getting reviewed matches into redshift table...\"\\n3: Pulls data from Invoice Matching Google sheet into temp_int.invoicing_level_match_reviewed\\n-> exits if there is no data in the sheet (empty sheet = there’s nothing to do!)\\n4: Logs \"[InvoiceMatching] Adding ready_for_qb=Y from sheet into QB table, ready_for_qb=N to pending \\nsheet...\"\\n5: Takes the data from last run’s external table (perpay_accounting_datamart_ext.invoicing_new_quickbooks_entries), \\ncopies it to a temp table (temp_int.invoicing_new_quickbooks_entries). Then appends new data (only those where \\nready_for_qb was \\'Y\\') (from temp_int.invoicing_level_match_reviewed, the table we just pulled from the sheet) to the \\ntemp table. Then saves the final result back to the external table again, \\nperpay_accounting_datamart_ext.invoicing_new_quickbooks_entries(which overrides the initial table). \\nRemember- nothing up to this point has been deleted in the sheets',\n",
       "  'Transactions that make it through the settlement process are considered SETTLED  transactions.\\nThe term transaction and purchase are used synonymously.\\npurchase: synonymous to transaction\\nA lot of our data models have fields named <transaction status>_purchase_count and <transaction status>_purchase_amount. \\nThese are synonymous to transaction count and transaction amount, as described in the DM documentation for those fields.\\nWe should remove this to be consistent with transaction naming.',\n",
       "  'a. The send dag only takes data from the raw data table with the most recent timestamp\\nb. The compliance check dag runs before the send as well, just in case \\nSample Generation Query\\n \\n1------------------------------------\\n2-- ANY BAD PAYMENT HISTORY\\n3-- Look at a few with status 93 or 97 (charged off)\\n4select consumer_account_number,\\n5       date_of_account_info,\\n6       enter_date,\\n7       payment_rating,',\n",
       "  'ssBilling/mailing address for the primary \\nconsumer. Cleaned for special \\ncharacters, malformed strings.user_attribute.street_addr, or if \\nnull, last created \\nanalytics_magento_sales_flat_\\norder_address.streetuser_attribute.s\\ntreet_addr\\nSecond Line of \\nAddress\\n second_line_ad\\ndressContains 2nd line of address such as \\napartment number. This can also be \\nincluded in the first line of address.Blank  \\nCity\\n  city Contains city name for address of \\nprimary consumer. Truncate rightmost \\npositions if city name is greater than \\n20 characters or use standard 13-\\ncharacter U.S. Postal Service city \\nabbreviations.city of user_attribute, or if not \\npresent, magento address. \\nTruncated to 20 characters \\n(field max). Not truncated to \\nUSPS 13-character \\nabbreviation, as there are \\nsome invalid cities in need of \\ncleaning. \\nState\\n  state  state of user_attribute, or if not \\npresent, state of magento \\naddress. Magento state \\nconverted to state abbreviation \\nusing \\nperpay_general_datamart.mag',\n",
       "  'limiting events to Amplitude before this approach is taken.\\nFront-End Tracking Caveats:\\nThe Amplitude and Segment data must be used with the following caveats in mind:\\nThe data will never be 100% complete due to things out of our control, such as users using ad-blockers.\\nThe user mappings in Amplitude will never be 100% accurate due to things out of our control, such as two visitors sharing a computer \\nand signing up or logging in with two different accounts.\\nThere should be more QA done around UTM tracking using the above sources, as they should match the marketing sources directly \\ndescribed below, but do not.\\nRelevant Redshift Schemas and Tables:',\n",
       "  \"345\\nCredit Card Data Engineering Resources\\n \\nConfirming accuracy of Perpay Deserve data for “provisioned at” timestamp:\\nNotes: provisioned at comes from the public.card_cardstatustable where status = ‘pending' . When activating a credit card for the \\nfirst time, this status is determined by Perpay’s engineering team - confidence in this data \\n \\n \\n \\nConfirming that all transactions in Perpay Engineering’s Database exist in the SFTP settled file \\n:\\n \\n \\nWebhooks Received by Engineering:\\n1with cte as(\\n2    select account_id,           \\n3    account_opened_at    \\n4    from perpay_accounting_datamart_ext.deserve_accounts_eod_report\\n5    group by 1, 2)\\n6select a.borrower_id,       \\n7a.created_ts as perpay_created_ts,       \\n8b.deserve_id,       \\n9c.account_opened_at,       \\n10datediff('second', cast(account_opened_at as timestamp), perpay_created_ts) as difference_in_seconds\\n11from card_int.card_borrower_fact as a\",\n",
       "  'Here is one example of a custom task group that is used in the metric layer. It rebuilds the card activation event descriptions table if a rerun \\nparameter is passed in the DAG config.\\nFile Structure\\n1. Create a folder for your DAG that contains the below files\\n2. Create a config.py file that contains your params JSON object and any other config variables you may want in your dag\\n3. Create a {your dag}_dag.py file to define the searchpath for your sql files, create the DAG object, initialize the task groups, and define \\nthe dependencies. An example of creating a DAG object for the metric layer is below (although for most dags a single searchpath is \\nsufficient):\\n1card_key_relations_task_group = TempTestCommitTaskGroup(\\n2    dag=dag,\\n3    table_name=\"card_key_relations\",\\n4    temp_schema=redshift.get_schema_by_env(constants.TEMP_CARD_SCHEMA),\\n5    commit_schema=constants.METRIC_LAYER_SCHEMA,\\n6    sql=\\'card_key_relations.sql\\',\\n7    params=config.params,',\n",
       "  '621\\nRedshift\\nVideo topic ideas:\\nwhat is a redshift cluster?\\nwhat is an internal vs. external schema?\\nwhat is WLM and what is our current WLM configuration?\\nWhat is a table lock?\\nWhat is a CTE and why do we use them?',\n",
       "  '390\\nDE Notes on Initial Core Discrepancies\\nPayment Detail Amounts Do Not Sum to Payment Amount\\nCases where the sum of all payment details associated with the payment does not total to the actual payment amount.\\nMissing Micro Deposit on Balance\\nCases where a micro deposit was made by the user, but no corresponding payment was made with the micro deposit and the deposit does \\nnot reflect on the user’s core balance.\\nMissing Payment Detail for Out of Stock Credit\\nCases where an out of stock credit was redeemed by a user, but there is no corresponding payment detail row for the use of the borrower \\ncredit.1 0 0 4 8 6 7 4 1 6 9 4 6 2\\n2 0 1 0 1 6 0 3 5 6 5 3 9 9\\n2 5 2 7 7 5 6 4 1 2 1 6 5 5\\n4 8 2 3 8 0 0 4 6 5 2 6 9 3User ID with Discrepancy Payment ID\\n1 8 3 1 3 5 0 1 4 6 8 3 1 5\\n2 6 1 7 1 5 3 1 4 7 6 1 8 6\\n8 0 0 0 2 0 8 4 1 0 7 9 5 7 \\n8 3 3 4 5 7 6 4 3 6 0 2 9 6 User ID with Discrepancy Deposit ID\\n8 0 5 7 3 0 8 1 2 8 6 9 2 8 3\\n8 0 7 9 9 5 4 1 2 7 0 7 3 6 2\\n8 1 1 9 7 6 6 1 2 9 4 1 6 3 9',\n",
       "  'incorrect\\nFivetran (use Gmail account for login)\\nThe DAG\\nTo test, turn the accounts_payable_revamp DAG on in staging. The code handles environment checks, so nothing needs to be modified \\nthere.\\nThe Quickbooks_Upload task may fail with an error related to a refresh key. If it does, re-access the tokens with this link (make sure you \\nselect the Sandbox company) and update the Quickbooks Secrets/Tokens for Invoice Matching Staging 1pass (Admin).\\nThe Zaps\\nStaging Zaps for each vendor are found in the AP Revamp Stage Zapier folder.\\nWarning about pre-created zaps:\\nAll Zaps but the Abby Test.. ones will pull invoices through the same Gmail labels as prod’s zaps do. Because the labels are the \\nsame as prods, you shouldn’t send test emails there (prod will pick it up; sorry, these are set with prod’s configs from initial testing). \\nOr if you’d like, switch its current source to a different label for testing, and then send test emails there.',\n",
       "  \"402\\nCard Account Balance Daily Ledger\\nData Dictionary\\naccount_id Integer The associated account ID for the card \\naccount \\nborrower_id Integer The associated borrower ID for the card \\naccount \\nuser_id Integer The associated user ID for the card \\naccount \\ncard_account_id Integer The engineering ID of the card account \\nbeing observed \\ncard_account_uuid VarcharThe Deserve ID of the card account being \\nobserved \\nemail VarcharThe associated email for the card account \\nsrc_dt Date The observation date in EST  \\nobservation_day Integer The total days of observation for the card \\naccount at the src_dt \\nstarting_balance Numeric(\\n38,2)The starting balance on the observation \\ndate. On the first day of observation, it is \\n0. On every other day it is derived from \\nthe previous day's ending balanceCalculating \\nDaily \\nBalances\\ntransactions Numeric(\\n38,2)Sum of transactions of type REGULAR for \\nthe card account on the observation dateTransaction \\nTracking\\naccount_opening_fee Numeric\",\n",
       "  '103        left join card_int.card_account_attribute as f on p.card_account_id = f.card_account_id\\n104        left join card_int.card_account_performance j on p.card_account_id = j.card_account_id\\n105        left join minimum_payment_2 o on o.borrower_id = c.borrower_id\\n106        left join public.card_account q on q.borrower_id = c.borrower_id;\\n107\\n108-- create the new cdp table\\n109drop table if exists users_herr.iterable_customer_data_save;\\n110create table users_herr.iterable_customer_data_save as select * from perpay_marketing_datamart_ext.iterable_customer_data;\\n111\\n112-- Update the card object\\n113update users_herr.iterable_customer_data_save as a\\n114 set card_dict = b.card_dict\\n115from users_herr.card_update b\\n116left join public.user c on b.user_id = c.id\\n117where a.user_uuid = c.uuid;\\n118\\n119-- Make the switch - uses renaming to make the switch super fast\\n120create table census_source_int.iterable_customer_data_temporary as select * from users_herr.iterable_customer_data_save;',\n",
       "  'reorganize the DAG to be much more aligned with the necessary functionality as well as updating how we are handling tests and error \\nreporting/alerting. The goal of this first update is to enable the marketing updates to be as minimally impacted by unnecessary errors as \\npossible, decrease the DAGs run time, and make error correction easier.\\nDMS Issue Mitigation\\nDMS has created issues for us as it migrates data, such as duplicate rows. Our tests pick up the dups and rightly fail, however, this then \\ncauses downstream slow downs. DMS is set up in default fashion and there are many capabilities we are not taking advantage of. One \\nis the ability for DMS to validate its own processes… which is not turned on. Additionally, enforcing strict rules on primary keys would \\nkeep spurious dups from being added to our database (Note, this will absolutely require our Data Dog integration to be complete so we',\n",
       "  '390\\nDE Notes on Initial Core Discrepancies\\nPayment Detail Amounts Do Not Sum to Payment Amount\\nCases where the sum of all payment details associated with the payment does not total to the actual payment amount.\\nMissing Micro Deposit on Balance\\nCases where a micro deposit was made by the user, but no corresponding payment was made with the micro deposit and the deposit does \\nnot reflect on the user’s core balance.\\nMissing Payment Detail for Out of Stock Credit\\nCases where an out of stock credit was redeemed by a user, but there is no corresponding payment detail row for the use of the borrower \\ncredit.1 0 0 4 8 6 7 4 1 6 9 4 6 2\\n2 0 1 0 1 6 0 3 5 6 5 3 9 9\\n2 5 2 7 7 5 6 4 1 2 1 6 5 5\\n4 8 2 3 8 0 0 4 6 5 2 6 9 3User ID with Discrepancy Payment ID\\n1 8 3 1 3 5 0 1 4 6 8 3 1 5\\n2 6 1 7 1 5 3 1 4 7 6 1 8 6\\n8 0 0 0 2 0 8 4 1 0 7 9 5 7 \\n8 3 3 4 5 7 6 4 3 6 0 2 9 6 User ID with Discrepancy Deposit ID\\n8 0 5 7 3 0 8 1 2 8 6 9 2 8 3\\n8 0 7 9 9 5 4 1 2 7 0 7 3 6 2\\n8 1 1 9 7 6 6 1 2 9 4 1 6 3 9',\n",
       "  \"6    raise Exception (f'[ImpactApi] Issue with Action Report - no data returned for yesterday!' )\\n7\\n8  # Write df to the impact_actions table\\n9  self.spectrum .write_df_to_internal (resulting_df , self.spectrum .get_schema_by_env (constants .TEMP_SCHEMA ), 'impact_actions' , varchar_length =65000)\\n10  self.spectrum .create_external_prod_table_as (\\n11      table_name ='impact_actions' ,\\n12      write_schema =self.spectrum .get_schema_by_env (constants .PERPAY_MARKETING_DM_SCHEMA ))\",\n",
       "  \"apache-airflow-at-lyft-6e53bb8fccff\\nPandora uses Airflow in their stack, and uses Prometheus & Grafana to monitor its state. The shown Grafana dashboard tracks at the \\ntask level, looking at the number of queued tasks and tasks up for retry. It also monitors Airflow database size:  \\nhttps://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee\\nGojek, the Indonesian Uber, uses Airflow and uses InfluxDB and Telegraf to get metrics into Grafana for visualization. They monitor \\nheavily at the tasks level looking at number of queued and running tasks. (Side reading: there is less documentation on Airflow → \\nTelegraf connections then there is documentation on Airflow → Prometheus connections): https://blog.gojekengineering.com/how-we-\\nmonitor-apache-airflow-in-production-210f9bff9e71\\nTechnical Resources\\nDocker compose example with all the containers wired up with Airflow: https://github.com/ankxyz/airflow-prometheus-grafana-exampl\\neCan't find link\",\n",
       "  'account reconciliation. \\nThe effect of the Deserve ACH holdout takeover on returned payments was not always understood correctly. This was the cause of the \\nsince resolved Variance 4 in the known variances documentation.\\nPayments Completed Before 2023-05-08\\nWhen a borrower makes a payment, engineering will record it and ping Deserve to generate the payment on their end as well. When \\nengineering pings Deserve to generate this payment has changed over time. The actual account balance changes when Deserve creates \\nthe payment on their end. \\nPrior to the Deserve ACH holdout takeover on 2023-05-08, engineering would wait until a payment reached a completed state to tell \\nDeserve to create the payment on their end. This would occasionally cause an undesired behavior in that a borrower could make a payment',\n",
       "  '227\\nRe-ingesting data from the Manual PO Matching Sheet\\nBut wait!! Just before the data from accounts_payable_int.po_variances gets put into the sheet, we need to account for information that \\nmay already be in the sheet. To do so, we ingest the data into a table, accounts_payable_int.variance_sheet_output and insert it into \\nan accounts_payable_int.variance_sheet_output_history table with an attached timestamp called created_ts (the ts which the \\nrecord entered the table). Then we decide what to do with it based on its ready_for_qb status. Following the descriptions above,\\nIf ready_for_qb is…\\nmarked as Yes: we insert the record in the accounts_payable_int.po_matches table. We also remove the record from the \\naccounts_payable_int.po_variances table.\\nmarked as No: we remove the record from the accounts_payable_int.po_variances table. We don’t add it anywhere because \\naccounting will manually upload the data. Thus, it is removed from the automated process altogether (will not exist in',\n",
       "  \"me to believe we're missing something in the table (need to track more than the list I just mentioned). Can someone on Engineering help me \\nidentify what that is? Thank you!\\nvariance_2_examples.csv \\nResponse: For the first loan, there was only a single payment detail of $2.49. Same case for the second loan, total Payment made that day \\nis $12.00, only $6.00 of it was applied towards this loan, other $6.00 was applied towards loan 1794571 – Looks like payments towards the \\nloans are doubled in the CSV\\nVariance 7: Old Borrower Credits\\nNo questions associated with this variance\\nVariance 10: Payments = Borrower Credits\\nQuestion: Andrew and I are looking into a new variance for the commerce rollforward table and have a question. Are payments made via \\nborrower credits included in the payment_detail table?\\nWe don't know much about the borrower credits process, but we noticed that of the 339,111 instances where borrower credits were\",\n",
       "  'issues arise. Because the e-commerce PO data is on the item-level, we expect each commerce PO sku and po_number to match invoice \\nsku and po_number. These are the attributes used to join the tables together. In addition, we expect invoice line items to match invoice \\nsummaries on file_name_<hash_unique_to_file>, which is used to join invoice data together.Zapier access keys can be found in 1pass under AWS Access Key- prod-zapier-external-user. These allow Zapier to connect \\nto the Amazon S3 PROD account.\\nThe data is brought into a table named ecomm_po_data in the accounts_payable_int schema.\\nThe query that is executed to create the table is in ingest_ecomm_po_data.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/ingesting_data/ingest_ecomm_po_data.py).\\nThe data is brought into a table named qb_bills in the accounts_payable_int schema.\\nThe query that is executed to create the table is in qb_bills.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/ingesting_data/qb_bills.py).',\n",
       "  \"26\\n27    def get_schema_by_env (self, schema_name , force_user_schema =False, external =False):\\n28        if schema_name in (constants .TEMP_MARKETPLACE_SCHEMA , constants .TEMP_SCHEMA_DEV ):\\n29            return constants .TEMP_SCHEMA_DEV\\n30\\n31        # Now that paths are updated, local config can be imported\\n32        from utils.config_local import ConfigLocal\\n33        local_env_vars = ConfigLocal ().generate ()\\n34\\n35        return local_env_vars .get('REDSHIFT_DEV_SCHEMA' ) if not external else constants .USERS_EXTERNAL_GENERA\\n36\\n37\\n38if __name__ == '__main__' :\\n39    # Takes the first argument after the file name\\n40    file_name = sys.argv[1]\\n41\\n42    # Set up the paths so that all files can be imported\\n43    rndr = EasyRenderer (file_name )\\n44\\n45    # Now that the paths are known, import constants and the file\\n46    import utils.constants as constants\\n47    module = __import__ (rndr.file_name )\\n48\\n49    # Render the sql logic\\n50    module .rendered_query (rndr)\",\n",
       "  \"514\\nTying out Engineering's Account Balance Flow\\nGoal\\nThis document tracks issues within the TEMP_core_account_daily_rollforward table. It enables us to record discrepancies in our \\naccount balance, calculation inaccuracies, or inadvertent additions. Additionally, it assists in identifying discrepancies resulting from manual \\nerrors. This table is built by the TEMP_data_model_accounting DAG, which can be initiated manually.\\nContext\\nWe are stepping back from the core process to understand how Engineering determines account balance at the end of the day. Our aim is \\nto monitor every fund transaction that occurs within the day before we establish the accounting department's version of the balance. Once \\nwe have a complete grasp of all the elements contributing to Engineering's balance, we can proceed by incorporating new calculations for \\nthe starting and ending balances to view variances on a higher level like these. When this happens, we can remove the\",\n",
       "  '4. Engineering sends something to Deserve letting them know a payment has been made with a flag whether or not to hold the payment \\nout (whether it’s an ACH payment)\\n5. Deserve receives the message, determines how long the payment should be held out for, and then sets payments in hold-out to \\nPROCESSING state for that duration\\n6. Post-hold, the payment in the public.card_cardpayment hits a COMPLETED or RETURNED state\\na. If the payment is completed, nothing happens to the card account balance.\\nb. If the payment is returned, the corresponding fees will post the following cycle\\nWhat this means\\nAll this means is the holdout period logic lies on Deserves side now, and the public.card_cardpayment table gets populated earlier \\nthan it did before. Even though the holdout period logic is variable now instead of 2 days, I don’t think this changes much of anything. In',\n",
       "  '285\\nTransactions\\nDaily transactions are sourced from the Deserve web-hook data. In all cases, a transaction is reported on the date that Deserve reports the \\ntransaction reached a SETTLED state. This is a current source of variance as not all transactions are posted to accounts on the date that \\nthey are settled. Transactions that take a borrower directly to or over their credit limit will in some cases post to an account before the \\ntransaction actually settles. We have reached out to Deserve for clarifications on their exact rules for when a transaction posts to the \\naccount in these circumstances. More information is available under Variance 3 in the known variances documentation.\\nAccount Opening Fee\\nAccount opening fees are sourced from the Deserve web-hook data. They come through as transactions with a transaction category of \\nACCOUNT_OPENING_FEE. There is a $9 fee associated with opening an account. Aside from a handful of exceptions, a card account will incur',\n",
       "  '10\\na. loan data mode, marketplace user data model, card charge off data model, etc\\n2. Data Sources\\na. What is/are the source(s) of the data that are used to populate the table? Is the data coming from Core, Deserve SFTP, segment, a \\ncombination?\\n3. Use Case\\na. What is the purpose of the table? Is it used for credit reporting, domo reporting, general analytics, tracking lapsed signup \\nassignments?\\n4. Description\\na. A medium length high level description of nature of the table that highlights any additional context/caveats of the data within that \\nwould help the stakeholder understand the information within.\\nColumn Description (here)\\nColumns are the cornerstone of any data modeling enterprise, as they contain the exact values necessary for analysis or manipulation. It is \\ncrucial that users have a clear understanding of the context behind each feature to avoid any ambiguity. For instance, knowing whether a',\n",
       "  '505\\nCurrent Variances - Specific Cases\\nThese are examples tied to variances that have not been resolved. At max, we’ll attach two examples per variance, unless asked for more. \\nEach variance is numbered and matches the number in the overview table above. \\nVariance 1: Null Source Dates\\nThese borrowers have no src_dt in the rollforward table. Because all have variances, I believe this is because we can’t tie out the account \\nbalance correctly with null dates. Will dig further into the code before asking any specific question to another team, but logging these done to directly resolve it, other \\nthan engineering going in and \\nmanually updating the rows \\n(which is not preferred). \\n*Side: there are two cases that \\nmagically resolved that we need \\nto look into. These were cases in \\nthe list I gave Talha. This PR \\nhoped to resolve one, but when I \\ntested it, the initial variance was \\ngone (this list was from variance 5 \\nbut I’m adding the message here \\nas it aligns more with 11).',\n",
       "  '422',\n",
       "  '280\\nInfo',\n",
       "  \"public.deposit\\ndeposit_id’s are NULL if the deposit doesn’t happen or the ACH details are stale \\npublic.auto_pay_payment\\nThis table contains deposit_id's which end up in public.deposit\",\n",
       "  'On the first day of observation, it is sourced \\nfrom the balance ledger. On every other day it \\nis derived from the previous day’s ending \\nbalanceCalculating \\nDaily \\nBalances\\npayments Numeric(\\n38,2)Payments applied to the loan balance on the \\nobservation date (not including borrower \\ncredits)Payment \\nTracking\\nborrower_credits Numeric(\\n38,2)Borrower credits applied to the loan balance \\non the observation datePayment \\nTracking\\nreturned_payments Numeric(\\n38,2)Payments returned on the observation day \\nthat will cause the loan balance to be \\nincremented by the returned amountPayment \\nTracking\\nrefunds Numeric(\\n38,2)Refunds for the loan applied to the loan \\nbalanceRefund \\nTracking\\nending_balance Numeric(\\n38,2)The balance resulting from all balance \\nmovement being applied to the day’s starting \\nbalanceCalculating \\nDaily \\nBalancesColumn Data \\nTypeDefinition Additional \\nDetail',\n",
       "  '287\\nbalance on the first day of observation comes from the loan table. On all subsequent days, the starting balance is equated to the ending \\nbalance of the previous day in payment_loanprincipalbalancehistory. \\nThere are a handful of cases (~3,343) where the loan amount in the loan table does not match the ending_balance in the first record of \\nloanprincipalbalancehistory. These cause some variances in the table. Engineering believes these are one-off cases, likely incorrect \\nmanual deletions from batch processes. They have not been addressed yet, as the most recent variance stemming from this issue occurred \\non 2022-11-28, and the earliest was in 2014. \\nLoans before 2019-04-11 enter the table upon the first loan balance movement, and loans after 2019-04-11 enter the table upon the date \\nthe loan is issued itself. This is due to a cutover on Engineering’s side that date (~19:36 UTC).  \\nEnding Balance',\n",
       "  'never was. \\nReturned Borrower Credits: We’re missing a \\nreturned_borrower_credits field. I was correct in assuming \\nprincipal balance should never go below zero. But, this occurs \\nthrough a flow of funds- what happened to borrower_id = \\n774799 on 9/22/23 is that Engineering first applied borrower \\ncredits to this loan, but when it got canceled, they returned the \\ncredits. See the brwr under the sheet here.Flagged the variance in the sheet \\nand am beginning a PR to resolve \\nit based on this assumption. Will \\nnot mark the PR as ready for \\nreview until I receive confirmation \\nthat engineering does not have \\nbalances < 0.\\n^ This has been confirmed. Looks \\nlike instead of a check for <0, we \\nneed to incorporate borrower \\ncredit returns.\\n14 COMPLETED\\nPR hereAbby Miscalculated Refunds: There are just a few cases where \\nwe’re miscalculating the refund amount. See the examples \\nunder the sheet here.figured out why these were \\nincorrect. PR up for review to \\nresolve them.',\n",
       "  'in Jun 2021. The historical record prior to the introduction of the waffle flag tables is held in \\nrefrence_archive_int.pinwheel_payroll_provider_legacy and reference_archive_int.pinwheel_company_legacy. In addition, the \\npublic.waffle_flag table contains a column modified which indicates the timestamp of the last instance that a company or payroll \\nprovider’s eligibility status changed. Because there are some companies and payroll providers with a timestamp in modified that does not \\nhave a corresponding record in public.waffle_flag_history, this column was also used to help form a more complete historical record. \\nAssembling Critical Dates\\nPerpay understands a company to be eligible for Pinwheel if the company is unverified, if the company’s payroll providers is on Pinwheel’s \\npayroll provider eligibility list, or if the company is on Pinwheel’s company eligibility list. In addition, Perpay began directing unverified',\n",
       "  '245\\nSolution\\nThere are two ways to resolve this issue\\nOption 1: update the code where we ingest the invoice (either cleaned_pdf_summaries.py or cleaned_csvs.py) and rename the \\nvendor name when there’s cases of the unwanted name.\\nOption 2: ask Accounting to update the vendor name in Quickbooks.\\nBoth options are quick, but I prefer to do the first unless it’s a new vendor because there’s a chance the vendor switches the name more \\nthan once or we need to re-upload old invoices with the other name. Updating the code is more flexible.',\n",
       "  \"575\\n \\nThere are some costs that we cannot parse out from Engineering/DS costs (example: EC2 instance). Implementing cost allocation tags \\nwould allow us to tag our resources by department/environment, which would help separate costs further.\\nIf you filter on LinkedAccountName = ‘Production Account’ and ProductCode = ‘AmazonRedshift’, all UsageType categories aside from \\n‘USE1-DataScanned’ are regular Redshift costs. 'USE1-DataScanned’ is the additional cost of Spectrum, scanning objects in S3 via \\nRedshift. \\nOther resources:\\nRedshift Pricing\\nRedshift Spectrum Query Charges\\nRedshift S3 Pricing\\nCost Efficiency @ Scale in Big Data File Format (Uber blog post)\\n \\n4where invoiceid is not null)\\n5select sum(totalcost) as product_cost\\n6from remove_totals\\n7where linkedaccountname = 'Segment Account';\",\n",
       "  \"is no action needed to correct the reporting. \\nIf there is found to be an error in reporting, Ops corrects the error via an admin tool. \\n1select b.email, c.*\\n2from perpay_general_datamart.borrower_fact as a\\n3left join perpay_general_datamart.user_attribute as b on a.borrower_id = b.borrower_id\\n4left join perpay_general_datamart.borrower_attribute as c on a.borrower_id = c.borrower_id\\n5--where lower(b.email) = 'santiagobella2018@gmail.com'\\n6where a.borrower_id = 2681039\\n7--and c.days_past_due >= 30\\n8limit 10000\",\n",
       "  \"accounting will manually upload the data. Thus, it is removed from the automated process altogether (will not exist in \\naccounts_payable_int.po_matches and will not exist in accounts_payable_int.po_variances). We will never delete records from \\nthe accounts_payable_int.variance_sheet_output_history table, so that table acts as a historical record for every row that has \\nbeen in the Google Sheet with its corresponding timestamp.\\nmarked as Awaiting Decision: we update the status of the record in accounts_payable_int.po_variances to be 'Awaiting Decision'. \\nThat way, it will reenter the sheet, but with the updated status.\\nmarked as Not Addressed: we don’t take action. These records are still in the accounts_payable_int.po_variances table, so will \\nreenter the sheet as is.\\nThe chosen_payment_option operates independent of ready_for_qb. While ready_for_qb dictates where the data will travel in the\",\n",
       "  '293\\n20\\n23-\\n11-\\n09 \\n21:\\n50:\\n30    0               14\\n36\\n9.5\\n79\\n1.1\\n28\\n37\\n43     ajs\\n-\\nne\\nxt-\\n4a\\nb8\\n9b\\ncc\\nc1\\n48f\\n80\\ncf6\\n93\\n8fa\\n6e\\n9c\\n94\\n5c\\na Ap\\npro\\nve\\nd 0                 0       0                     0           0          0            0    0           0      0        Ka\\nsh\\nKic\\nk        13\\n06\\n57\\n9    On\\nbo\\nard\\ning \\nVe\\nrify \\nPh\\non\\ne \\nNu\\nmb\\ner    26\\n80\\n0         0           On\\nlin\\ne \\nTra\\ncki\\nng \\nLin\\nk                           11\\n35\\n26\\n2  O\\nNL\\nIN\\nE_\\nTR\\nAC\\nKI\\nN\\nG_\\nLI\\nNK Ca\\n3c\\n18\\n82\\n9-\\n7f7\\n3-\\n11\\nee-\\n98\\n1f-\\n6fe\\n27\\n33\\n1f8\\n19/\\n13\\n06\\n57\\n9/1\\n13\\n52\\n62/\\n50\\n04\\n10\\n11\\n4    \\n20\\n23-\\n11-\\n09 \\n21:\\n50:\\n25    0               14\\n36\\n9.5\\n79\\n1.1\\n28\\n37\\n74     ajs\\n-\\nne\\nxt-\\n24\\n22\\n07\\nb1\\n16\\n66\\ne0\\ndf8\\nbbf\\n9e\\n73\\n81\\n50f\\ncd\\n9 Ap\\npro\\nve\\nd 0                 0       0                     0           0          0            0    0           0      0        Bri\\ngit           20\\n48\\n46\\n7    On\\nbo\\nard\\ning \\nVe\\nrify \\nPh\\non\\ne \\nNu\\nmb\\ner    26\\n80\\n0         0           Bri\\ngit\\n_C\\nobr\\nan\\nde\\nd \\nLP\\n_3.\\n7.2\\n3                       16\\n26',\n",
       "  \"guys take a look?\\nFor a similar case:\\nHello! Seeing a card account application (application ID: 6512 , borrower ID: 61097) that has a null card_account_id  in the \\npublic.card_account_application table, but the card account application status is provisioned.I can see that they do have a \\ncard account (ID: 4588 ), and in the past, we see the card_account_id field populated in the card_account_application table \\nwhen the card account is created. This null field is causing some tests to break on our end.Would someone be able to help \\ninvestigate if this was expected behavior or not? If so, we'll update the tests on our end. Thanks!\\nAnd another case:\\nHi! We’re seeing a card_account with id = 6697 but no card tied to the account (nothing in card_card table where card_account_id \\n= 6697). This is breaking some tests on our end. Can someone look into this?\\n22    coalesce (b.id, j.card_id) as card_id ,\\n23    f.id as card_account_application_id ,\",\n",
       "  'If the vendor is supplying PDF invoices….\\nupdate cleaned_pdf_summaries! You’ll want to make sure that the fields are being pulled in correctly for the new invoice, so this step is \\nin conjunction with step #6.2. Definitely update vendor_name and invoice_due_date, but the rest of the fields are probably be fine \\nuntouched! I like to check the pdf against the output of cleaned_pdf_summaries and cleaned_pdf_summaries after the changes to \\nsee if things are correct. If it’s not expected, look at the output of perpay_accounting_datamart_ext.ap_invoice_summaries to see \\nwhat the issue is.\\nMake sure there’s only one row per invoice for cleaned_pdf_summaries. If there are multiple, that means something was parsed twice, \\nso you need to limit its case statement to accommodate.Vendor \\nNameInvoice \\nNumbe\\nrPO \\nNumbe\\nrInvoice \\nDateDue \\nDateModel \\nNo/SK\\nUTrackin\\ng \\nNumbe\\nrQuantit\\nyUnit \\nCostShippin\\ng \\nCharge\\nsDrop \\nShip \\nCharge\\nsFreight \\nCharge\\nsTotal \\nCost',\n",
       "  '22            \"perpay_plus\": {\\n23                \"segment\": \"Not CR Eligible, Opted In\",\\n24                \"current_backend_status\": \"opted_in\",\\n25            }\\n26            \"activity\": {\\n27                \"date_joined\": \"2019-11-09\", //YYYY-MM-DD \\n28                \"last_login\": \"2020-05-10\", //YYYY-MM-DD \\n29                \"last_transactional_click_dt\": \"2019-12-11\", //YYYY-MM-DD \\n30                \"last_transactional_open_dt\": \"2020-03-09\", //YYYY-MM-DD \\n31                \"last_promotional_send_dt\": \"2020-04-09\", //YYYY-MM-DD \\n32                \"last_promotional_open_dt\": \"2019-12-20\", //YYYY-MM-DD \\n33                \"last_promotional_click_dt\": \"2019-12-20\", //YYYY-MM-DD \\n34            },\\n35            \"company\": {\\n36                \"company_name\": \"CVS\",\\n37                \"company_risk_level\": 2,\\n38                \"payroll_provider_name\": \"Gusto\",\\n39                \"payroll_provider_type\": \"Gusto\",\\n40                \"payroll_portal_url\": \"www.gusto.com\",',\n",
       "  \"- The invoicing_master_invoice table does string manipulation of the vendor data using the \\ntemp_int.invoicing_vendor_key_matching table (created in step 2.2) joined onto the \\ntemp_int.invoicing_docparser_output table (created in step 2.4).\\n- The invoicing_level_match table uses perpay_accounting_datamart_ext.invoicing_new_quickbooks_entries \\n(this gets created in job 2! So we use the table from our last run) joined onto the table we just created, \\ntemp_int.invoicing_level_match, to clean the data more and add a flag to denote whether or not it’s in quickbooks or \\nnot. It does this by creating a column called in_qb, which can have a value of 'Y' or 'N'\\nThe function then saves the result of the query from \\ndags/analytics/invoice_matching/lib/queries/get_invoice_matching_query.py to a dataframe that is returned. The query saved \\nselects all columns from temp_int.invoicing_level_match (the table we just created) and creates a new column called\",\n",
       "  '46\\nThird-Party API and Reporting Integrations\\nDescription:\\nThis space serves as the documentation source for all third-party integrations where data is either retrieved from or pushed to.\\nTable of Contents\\n \\nAutomated Audience Management\\nDataX Reporting Service\\nAmplitude Marketing GET  API\\nFacebook Marketing GET  API\\nGoogle Marketing GET  API\\nAccounts Payable Management\\nCredit Bureau Reporting\\nIterable User and Catalog Batch Updates\\nQuickBooks GET  API\\nPerpay Front-End Tracking and Marketing Stack\\nDomo Data Governace',\n",
       "  'Payment Tracking\\nPayments Completed Before May 8, 2023\\nThe original processing for payments involved a 2 business day holdout for every payment. A customer’s card balance would only be \\nupdated after this holdout period ended. The end of a holdout period was indicated by a payment entering a completed status. Payments \\nthat were initiated before 2023-05-08, but were completed after that date also follow this original paradigm.\\nThere are 2 exceptions to this rule for payments prior to 2023-05-08:\\n1. Deserve Customer Service Errors: There were several instances where Deserve customer service errors resulted in a payment being \\nerroneously applied to a customer’s balance. Deserve ended up paying this money to Perpay themselves, and the “payments“ remained \\npermanently on the customer’s balance. These payments are permanently in a pending status, but did apply to the balance.',\n",
       "  '86\\nMonitoring our Infrastructure\\nTable of Contents\\n1. Introduction to Datadog\\n2. Integration with AWS\\nCouldFormation\\nPrerequisites\\nPost-integration capabilities\\n3. Key Benefits of Using Datadog as a Monitoring Tool\\nComprehensive Monitoring\\nReal-time Alerting\\nEnhanced Troubleshooting\\nCost Optimization\\n4. Costs\\n5. Additional Resources\\n1. Introduction to Datadog\\nWe use Datadog to monitor our infrastructure! Datadog is a popular cloud monitoring and analytics platform that provides comprehensive \\nobservability of the performance and health of our infrastructure and services. With its extensive features and integrations, Datadog helps us \\ngain deep insights, troubleshoot issues, and optimize the efficiency of our systems.\\nOur dashboards: https://app.datadoghq.com/dashboard/lists\\nOur alerts: https://app.datadoghq.com/monitors/manage\\n2. Integration with AWS\\nDatadog integrates with AWS to provide monitoring and observability for our AWS resources. We aim to observe resources such as EC2',\n",
       "  'who started the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automated and runs \\non a schedule\\ncancelled_by varchar The username of the Domo user \\n(a perpay.com email address) \\nwho cancelled the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automatically \\ncancelled \\nstart_time timestamp The timestamp at which the \\nexecution began\\nend_time timestamp The timestamp at which the',\n",
       "  'understanding of the raw data. The following is a high level description of the analysis we performed before settling on our ETL approach for \\nthis project.\\nTo start, it was clear that we needed to perform some form of graph analysis to understand what was happening under the hood of the \\nengineering process. We used NetworkX to perform this analysis in python. After realizing that cyclic merges were common, we decided to \\nuse the multi-directional graph class to build the individual merge clusters. Initially, we only used merge events, but in later iterations we \\nadded the information regarding email updates and deleted user accounts. We went into the project assuming (hoping) that each cluster \\nwould have one user-node that is never merged into anything else which could be picked as the lead user. But even with our first pass at an \\nanalysis, a couple issues became immediately clear. Here, we list the final set of issues we arrived at after multiple iterations:',\n",
       "  '584\\nStep 4a: Create a glue crawler\\nGive your crawler a relevant name (csvs_statements_glue_database-crawler). Click next.\\nAllow the pre-selected options under “Specify crawler source type). Click next.\\nAdd a data store:\\nThe data store will be S3. Include the path to the bucket where your CSV lives. Click next.\\n \\nUnder “Add another data store”, click next.\\nChoose an IAM role - select “ana-stage-glue-role”. Click next.',\n",
       "  \"deposits. Engineering process these files and updates customers' Perpay account balances. The cleaned up version of the ACH file \\ncorresponds to the  public.achdetail table.\\nThis information is also downloaded by Accounting via the Perpay Admin dashboard.\\nThe ACH data is gathered from three sources: JP Morgan, TD Bank, and First Trust. \\nPreston matched the email addresses in the core rollforward table and the ACH files to see if the amounts match. (Email addresses are \\nthe only common piece of PII that appear in both the core rollforward table & the ACH files.)\\nRec process\\nEvery morning, Ops kicks off an automatic process (built by Engineering) as part of their daily reconciliation (aka “rec”) process. Ops also \\nperforms additional manual checks, e.g. checking that all required files are present and performing a fraud audit. \\nSee the Reconciliation page (Engineering) for a high-level overview of how this process works\",\n",
       "  \"546\\nTo understand the above command, see this StackOverflow link. The 172.17.0.1 IP is Docker's IP which you can find in the output of \\nifconfig. This SSH tunnel links all requests sent to Docker's IP through port 5439 to the production Redshift cluster through the jump box.\\nNotes on Permissioning and Connections\\nHad to manually edit Datasync task via the console to give it permission to write logs (there is a little checkbox at the very bottom of the \\nEdit page). Once I did it for one, it seemed to work for all. Could not find how to set this up via Terraform.\\nThe initial list of permissions I had to think about were the following:\\nStaging access to production Datahub,\\nStaging access to the production Redshift cluster,\\nStaging permissions for production Glue crawlers and Datasync tasks.\\nOther connections I had to add during the metric layer conversion:\\nAccess to the production Airflow bucket for the staging Airflow EC2 instance.\\nLet’s expand on the above.\\nAccess to Prod Datahub\",\n",
       "  '410\\nIn the following section, we go over the final process we built to separate healthy and unhealthy clusters.\\nData Preparation\\nWe start by retrieving the merge and email update histories along with each from and to user’s active/inactive status and their SSN \\nverification status and event timestamps. These two datasets are merged and sorted by the event timestamp and the final dataset includes \\nthe columns: to_email, from_email, event_ts, event_type, to_user_id, to_user_is_active, to_user_is_ssn_verified, \\nfrom_user_id, from_user_is_active, from_user_is_ssn_verified. This final dataset, a.k.a. the event table, is fed into an algorithm \\nthat builds the multi-directional graph of users.The logic is as follows:\\n1. For each row, check whether it’s a merge or email update event.\\na. If it’s a merge event:\\ni. Check if the from_email exists in the graph and if not add it along with the properties associated with the email (user_id,',\n",
       "  '8    def __init__ (self, config, customized_period =False):\\n9        self .config = config\\n10        self .customized_period = customized_period\\n11        self .spectrum = RedshiftSpectrum (config)\\n12        self .utils = ImpactUtils (config)\\n13\\n14    # Uses the ReportExport endpoint to get and retrieve Report results. \\n15    # Writes those results to the perpay_marketing_datamart_ext.impact_actions table.\\n16    def _get_actions (self, program_id ):\\n17        end_dt = datetime .now()\\n18        start_dt = datetime .strptime (\\'2021-10-01\\' , \\'%Y-%m-%d\\' )\\n19        resulting_df = pd.DataFrame ()\\n20        # Endpoint cannot handle requests that span multiple years, so we break the requests up based on year, starting at 2021\\n21        for year in range(start_dt .year, end_dt.year+1):\\n22            \"\"\"\\n23            The Impact Reports endpoint\\'s ...',\n",
       "  'balanceWithdrawal \\nTracking\\nrefund_marketplace_as_cash Numeric\\n(38,2)The portion of a marketplace refund \\nthat is returned to the user’s core \\nbalanceRefund \\nTracking\\nrefund_card_balance Numeric\\n(38,2)Negative card balance that has been \\ntransferred to the user’s core balanceCard \\nBalance \\nRefund \\nTracking\\ndeposit_ach_pending Numeric\\n(38,2)ACH deposits that have hit a pending \\nstatus, resulting in the amount being \\napplied to the user’s core account \\nbalanceDeposit \\nTracking\\ndeposit_ach_returned Numeric\\n(38,2)ACH deposits that have hit a returned \\nstatus, resulting in the amount being \\ndeducted from the user’s core account \\nbalanceDeposit \\nTracking\\ndeposit_check_completed Numeric\\n(38,2)Checks that have hit a completed \\nstatus, resulting in the amount being \\napplied to the user’s core account \\nbalanceDeposit \\nTrackingColumn Data \\nTypeDefinition Additional \\nDetail',\n",
       "  \"2select a.*, b.email\\n3from perpay_risk_datamart_ext .equifax_raw_data as a\\n4left join perpay_general_datamart_int .user_attribute as b on b.borrower_id = a.consumer_account_number\\n5where lower(b.email) like '[USER_EMAIL_ALL_LOWER_CASE]%'\",\n",
       "  '9\\nUsage and Contribution\\nDatahub (production location, staging location) serves as the central data catalog for not only the data teams but for Perpay as a whole. This \\nresource provides not only table and column-level information for data held within Perpay’s data warehouse, but also the lineage of the \\nupstream assets used to create the table/column and higher-level conceptual information. The platform is also connected to the data \\nwarehouse and orchestrator to ensure that what is being presented in Datahub is currently representative of the data environment.\\nAlthough schemas, tables, lineage, and other metadata can be automatically pulled in from the machinery that builds our data assets and \\nservices, the contextual information needs to be inputted by us measly humans (and/or AI-assisted). Therefore, this document outlines the \\nstandard operating procedure and style considerations for any DRAAFT team member to contribute to Datahub in order to maintain',\n",
       "  \"18       a.created as perpay_created_ts,\\n19       a.amount as perpay_amount,\\n20       a.tip_amount as perpay_tip_amount,\\n21       a.status as perpay_status,\\n22       a.type_category as perpay_type_category,\\n23       a.type as perpay_type,\\n24       a.transacted_at as perpay_transacted_at,\\n25       a.settled_at as perpay_settled_at,\\n26       datediff('hour', settled_at, created) as difference_in_hours\\n27from card_transaction_cte as a\\n28where a.status = 'SETTLED' and difference_in_hours > 2 and a.type not in ('FEE');\",\n",
       "  '213\\nadd_to_master_table: Appends the data to the master table\\n1: Unions the data from every vendor in the list of parsed vendors from the above function to a table \\ntemp_int.docparser_invoices_master_<todays date>\\n2: Copies the contents of perpay_accounting_datamart_ext.docparser_invoices_master into a \\ntemp_int.docparser_invoices_master table and and appends the temp_int.docparser_invoices_master_<todays date> \\ncontents to it\\n3: Uses the temp_int.docparser_invoices_master to create a new \\nperpay_accounting_datamart_ext.docparser_invoices_master table (overwriting the old one).\\ndrop_temp_tables: Drops any temp tables\\n1: Drops each individual temp_int.docparser_<vendor name>_<today> table.\\n2: Drops the temp_int.docparser_invoices_master_<todays date> combined vendor table. \\n** if there was a config passed in, all that would change is the timeframe which we collect parser data (i.e. get data parsed in',\n",
       "  \"175\\n25from pathlib import Path\\n26from jinja2 import Template\\n27\\n28class CombinedRenderer :\\n29    def __init__ (self, global_file_name ):\\n30        # Intialize with the provided global file name\\n31        self .global_file_name = global_file_name\\n32        self .base_path = self._find_base_path ()\\n33        self .file_path , self.file_name = self._find_file_path ()\\n34        self ._path_setup ()\\n35\\n36    def _find_base_path (self):\\n37        # Determine the base path of the repo\\n38        return __file__ .rpartition ('perpay-airflow' )[0] + 'perpay-airflow/'\\n39\\n40    def _find_file_path (self):\\n41        # Split the global file name to get file path and file name\\n42        path_list = self.global_file_name .rsplit('/', 1)\\n43        file_path = path_list [0] if len(path_list ) > 1 else ''\\n44        file_name = path_list [-1].rsplit('.', 1)[0]\\n45        return file_path , file_name\\n46\\n47    def _path_setup (self):\\n48        # Append necessary paths to sys.path for module loading\",\n",
       "  '6                    from public.feature_enrollment\\n7                    where feature_id in (35, 133)\\n8                    group by 1\\n9                    having cnt > 1;\\n10                \"\"\"\\n1...\\n2create table {{temp_schema}}.ab_test_backend_signup_report_1 as\\n3  select\\n4    a.*,\\n5    b.created as perpay_plus_opt_in_ts,\\n6    b.status as perpay_plus_status,\\n7    d.first_plus_eligible_dt,\\n8    d.current_eligible_ind as current_plus_eligible_ind,\\n9    f.number_of_payments as first_loan_duration,\\n10    g.first_pay_cycle,\\n11    case\\n12      when (g.first_pay_cycle in (\\'weekly\\') and first_loan_duration = 24)',\n",
       "  '(NB: We need to confirm whether same-day ACH reversals result in the creation and subsequent deletion of a row in public.deposit, or if \\nno rows are created in the first place.)\\nAlso, the following table illustrates how auto_pay_payment and achdetail are represented in the Platform Deposits metrics table \\n(atomic_int.platform_deposits). Note that when source = \"ACH\", the row originates from auto_pay_payment (and not achdetail).',\n",
       "  \"issue for investigation yet.\\n1 COMPLET… Andrew Credit balance refund (link): A customer pays towards their card \\nwithout ever receiving the card. After a specified amount of days, \\nthe negative balance is applied to the credit card balance, then the \\nmarketplace balance, and then the account balance for \\nwithdrawal.This was resolved before we \\npicked the project back up again \\nin September. \\n2 COMPLET… \\nPR hereAbby Dispute lost or withdrawn (link): Disputed that are lost or \\nwithdrawn are not being counted appropriately against account \\nbalanceThis fix entailed adding any \\nfunds from disputes that were \\neither lost or withdrawn into \\nending balance calc. The sign \\nwas flipped prior to the fix, so we \\nwere taking those out of the \\ncalculation instead of adding \\nthem in as they should be.\\n3 COMPLET… \\nPR hereAndrew Transaction timing (link): It seems there are still issues with the \\ntransaction data and when it should be reported. Deserve's data is\",\n",
       "  '473\\nending_balance\\nending_balance_calcIf the payment hits a RETURNED status after the holdout, that is when the system generated card account balance is increased by the \\npayment amount, as stated above. Likewise, if the payment never entered a holdout and gets RETURNED, the date of the RETURNED \\nstatus is the day the system generated card account balance is increased.\\nRollforward Table Logic:\\nSums all the payments that went towards the card and hit a RETURNED status within the day. \\nThis payment total is added to starting_balance in the equation for ending_balance_calc. \\nSource Tables:\\nThis directly comes from atomic_int.platform_payments, which captures card payments from the atomic_int.card_sftp_payment \\ntable, which pulls from the actual source of deserve_payments_eod_report SFTP.\\n5select * from perpay_accounting_datamart_ext.card_daily_rollforward where borrower_id = 7382228 order by src_\\n6\\n7-- CASE 2:\\n8-- atomic int\\n9select * from atomic_int.card_payment_status',\n",
       "  'in the console for the task and the instance it was clear the latency and errors were being driven by the high memory usage. The immediate \\nsolution was to increase the size of the DMS instance to where it had double the memory which has completely removed the issue (though \\ncosting and extra $15 a day). However, longer term, we need to not only re-balance the tables over all of the tasks but also explore the \\ntiming parameter for how long the batch tables are created on the instance. As of writing I am unsure how how drastic of an effect this could \\nhave but its worth exploring.',\n",
       "  '29       a.consumer_account_number,\\n30       date_of_account_info,\\n31       enter_date,\\n32       payment_rating,\\n33       payment_history_profile,\\n34       account_status,\\n35       credit_limit,\\n36       highest_credit_or_original,\\n37       terms_frequency,\\n38       actual_payment_amount,\\n39       current_balance,\\n40       amount_past_due,\\n41       original_charge_off_amount,\\n42       date_last_payment,\\n43       c.type,\\n44       d.min_enter_date as first_report_dt,\\n45       e.opt_in_dt as feature_opt_in_dt\\n46from perpay_risk_datamart.equifax_raw_data as a\\n47left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n48left join (select consumer_account_number, min(enter_date) as min_enter_date from perpay_risk_datamart.experian_\\n49left join (\\n50    select distinct borrower_id, b.type from public.feature_enrollment as a left join public.feature as b on b.i\\n51    ) as c on c.borrower_id = a.consumer_account_number\\n52left join (',\n",
       "  '369\\nAccounts Payable Revamp - Q3 OKR\\nThis Q3, we want to revamp the accounts payable process to ensure we pay our bills on time, maintain good relationships with vendors, \\nalleviate manual accounting work, and resolve errors from our current approach. \\nThis page gives a brief summary of the old framework and scopes out the new framework.\\nOld Process\\nPart 1: Invoice Ingestion\\nInvoices are received in PDF or Excel format to the email address invoices@perpay.com. The excel sheets are manually matched with \\nreports pulled from Magento, and then manually formatted & uploaded to Quickbooks outside of the automated process. \\nThe PDF invoices are forwarded to a 3rd party tool called Docparser, where there is a unique “parser” created for each vendor (this happens \\nevery day at 2am via the invoice_matching_v2 DAG). Invoice information is parsed from the PDF document and output into the Parser \\nOutput sheet.',\n",
       "  '202',\n",
       "  '97\\nmaterialized view (a time and energy consuming task) and find and drop tables that are dependent on the changed table (drop cascade), \\nwhich is both arduous and inefficient.\\nThis is where the source layer comes in. The source layer is meant to lie between the public and metric layer. The source layer’s main \\nfunction is to “numerically curate” the raw data from the public layer. This layer of validated data is meant to handle unexpected changes \\nthat may arise from public layer sources. This can play a role in the implementation of the materialized views which can smoothly carry out \\ntheir function of resolving timing issues. The placement of the MVs is further discussed in the section The source layer and materialized \\nviews. A good example of a source layer implementation is how DBT implements this structure.\\nMaterialized Views\\nWhat are they? Why use MVs? How do they work?',\n",
       "  'ones are fine. \\nIn summary,\\n[PROD] ERROR has occured!\\nDAG: dag_organizer\\nTask: data_model_full.Test_MarketingDailyDataModel_Domo\\nError: Bash command failed. The command returned a non-zero exit code 1.\\nFAILED \\n../../opt/***/dags/analytics/data_model_full/tests/test_marketing_daily_data_model_post_iterable.py::TestMarketingDailyDataModel::t\\nest_row_count_marketing_daily_data_model\\nUTM campaign problem\\nResult of a workflow change\\nIterable definition Iterable value Our definition (in the \\ntemp_dev_int.iterable_\\ndaily_insights tables)Our value Meaning',\n",
       "  'keep everything organized we encapsulate all of these tasks in a single task group:\\nTempTestCommitTaskGroup\\nSince typically creating a production table involves the same three step process (creating temporary tables, testing the tables to ensure they \\nare production quality, and committing the temporary tables to the production database), the TempTestCommitTaskGroup exists to abstract \\naway a lot of this boiler-plate code.\\nThe SQL statements required for this paradigm are:\\n1. drop the previous table in a temporary schema\\n2. create the new table in a temporary schema\\n3. drop the temporary table in the production schema\\n4. create the new temporary table in the production schema\\n5. rename the production table to indicate its deprecation\\n6. rename the temporary table in the production schema to indicate that it is no longer temporary\\n7. drop the deprecated production table\\nAn example of initializing an instance of this class is below:\\n1DROP TABLE IF EXISTS temp_card_int.card_key_relations',\n",
       "  '280\\nInfo',\n",
       "  'Without config change, ingesting a new schema and table will unhide it from the UI.   Each ingestion is tagged with a \"runId\" which uniquely \\nidentifies each ingestion run. While this hasn\\'t been utilized yet, it\\'s important for enabling stateful ingestion, which is an idea that was kicked \\naround for trying to keep UI state between ingestion runs. The thought here (that needs testing) is that after ingestion, we can do a one-time \\nmanual hiding of all datasets, keeping those we want. From then on, using the \"ignore_new_state\" tag in our ingesting config, it will update \\nsome data without changing the DataHub state of each asset. This would allow us incremental changes by manually unhiding data as it\\'s \\nupdated. We’re unsure which data changes from run to run here, but it should be tested.\\nWhile ingesting datasets, DataHub creates these concrete \"data units\" - called entities - that are identified by Uniform Resource Names',\n",
       "  'cleaned all in 2023 and 2022. \\nMost recent record with this \\nvariance is 2021-04-01, which is \\ngreat, because we’ve decided to \\ndisregard cases prior to 2022-01-\\n01!\\n7 COMPLETED\\nPR HereAndrew Old Borrower Credits (link): Borrower credits are not being \\ntracked in account reconciliation prior to 2019-12-01. I do not \\nknow the business reason for this (if there is one), and I am \\ndigging into it.There was a date filter on 2019-\\n12-01 causing variances for \\ninstances where refunds were \\nissued prior to that date, so the fix \\nentailed removing this filter. We \\nthink it was initially included for \\ntesting/to strip down the dataset \\nfor timing, but can’t be 100% \\npositive. Regardless of the past \\nreason, it resolved variances.',\n",
       "  '17    card_transaction_id ,\\n18    deserve_id ,\\n19    created_ts ,\\n20    transaction_status ,\\n21    rank () over (partition  by card_transaction_id order by created_ts asc) as rank,\\n22    case\\n23        when created_ts = first_value (created_ts ) over ( partition  by card_transaction_id order by created_ts d\\n24        else 0 end as current_ind\\n25from status_1\\n26where ((transaction_status <> transaction_status_previous ) OR (transaction_status_previous IS NULL));\\n27\\n28\\n29select\\n30    count(*) as cnt,\\n31    card_transaction_id\\n32from users_frank .card_transaction_status\\n33group by 2 having cnt > 2 order by cnt desc;\\n1select *\\n2from public.card_transaction_snapshot\\n3where card_transaction_id = 129165\\n4order by created desc;',\n",
       "  'amount is considered to be a completed direct deposit on the date of the created timestamp, and it is attributed to the associated \\naccount_id.\\nReturned Direct Deposits: Rejected deposits are sourced from rows in the deposit table with a content_type_id of 46 and a status \\nof returned. The amount is considered to be a rejected direct deposit on the date of the modified timestamp, and it is attributed to the \\nassociated account_id.\\nPayment Tracking\\nThe following are the seven types of payments that impact the core balance and how DE derives the amount of that payment on a given day \\nfor a user’s account.\\nInitiated Credit Card Payments: Initiated credit card payments are sourced from Deserve’s SFTP data. Each unique payment_id with a \\nnon-null initiated_at timestamp is considered to be an initiated credit card payment of the amount indicated in payment_amount on the \\nday of the initiated_at timestamp for the associated account_id.',\n",
       "  \"7    a.transaction_type,\\n8    a.type_category,\\n9    a.transaction_amount as sftp_amount,\\n10    b.amount as core_amount,\\n11    a.account_id,\\n12    b.deserve_id,\\n13    a.cleared_at,\\n14    a.sftp_dt,\\n15    b.created\\n16from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report as a\\n17left join public.card_transaction as b on a.transaction_id = b.deserve_id\\n18where b.status != 'SETTLED'\\n19--- if you want to look only at transactions after the 'fix' was deployed ---\\n20--and b.created >= '2022-10-17'\\n21order by sftp_dt desc;\\n1select\\n2    case when a.account_id in ('149eaf20-8288-4ce3-87da-425b0ed06934', 'bf085378-a8ad-4244-b921-9f926d1f7416', '\\n3          then 1 else 0 end as deserve_employee_ind,\\n4    b.status,\\n5    b.id as card_transaction_id,\\n6    a.transaction_id,\\n7    a.transaction_type,\\n8    a.type_category,\\n9    a.transaction_amount as sftp_amount,\\n10    b.amount as core_amount,\\n11    a.account_id,\\n12    b.deserve_id,\\n13    a.cleared_at,\\n14    a.dt,\\n15    b.created\",\n",
       "  'and data models it contains. Adding descriptive information to any schema accessed by stakeholders will give them a clearer understanding \\nof the commonality and representation of all data assets stored within. Below are the required pieces of information that should be added to \\nthe description field. If there are relevant glossary terms or subject matter experts/owners, please add them via the selection boxes.\\n1. Product Focus\\na. Card, marketplace, platform, perpay plus, etc\\n2. Data Sources\\na. What is/are the source(s) of the data that populate the tables within the schema? Is the data coming from Core, Deserve SFTP, \\nsegment, a combination?\\n3. Use Case\\na. What is the purpose of the data models/tables in the schema? Are they used for credit reporting, domo reporting, general analytics, \\ntracking lapsed signup assignments?\\n4. Description\\na. A medium length high level description of nature of the schema that highlights any additional context/caveats of the data within that',\n",
       "  'missed in the upload process. This should be double checked by doing a search in QB and manually uploaded if it’s true (you can \\nask accounting to manually upload them, just provide the invoice_numbers). A message will be sent to the alerts channel.\\n5: Backup Measures\\nWe have some backup measures in place for when things go awry. Specifically, we implemented a task that can be triggered using the \\n{\"delete_qb_duplicates\": 1} config to delete duplicated bills from QB. It’s logic lies in the delete_duplicated_bills() function in \\nquickbooks_api_class.py and utilizes a POST request. We rely on Fivetran data to decide which bills to submit through the POST request \\nfor deletion. If one or more bills in the duplication cluster have been paid, this deletes the remaining unpaid bills in that cluster. If no bills \\nhave been paid, all but one are deleted, where the one kept is chosen at random.',\n",
       "  \"Codezip_code Report 5 or 9 digit zip codes. zip of user_attribute, or if not \\npresent, 5 digit zip of magento \\naddress. \\nAddress \\nIndicatoraddress_indicato\\nrIf the address is reported, can be \\nconfirmed, military, or other statuses.Y = consumer’s known address  \\nResidence Coderesidence_code O = Owns, R = Rents code. Blank  \\nSegment \\nIdentifiersegment_identifi\\nerReporting segment on employer info: N1  \\nEmployer name employer_nameName of employer company_name if not like Other \\n> user_attribute.co\\nmpany_name\\nFirst Line of \\nEmployer \\nAddressemployer_addre\\nss_line1 Blank  \\nSecond Line of \\nEmployer \\nAddressemployer_addre\\nss_line2 Blank  \\nEmployer City employer_city  Blank  \\nEmployer State employer_state  Blank  \\nEmployer Postal \\n/ Zip Codeemployer_zip  Blank  \\nOccupation occupation  Blank  \\nReserved reserved3 Blank   \\n1 case when m.borrower_id is not null then 'DA' -- Delete merged account\\n2      when n.borrower_id is not null then '13' -- Paid in full, canceled Perpay Plus\",\n",
       "  '364\\nWe observe that many users associated with the same device ID are borrowers (authentication required). Since Amplitude automatically \\ncomputes the Amplitude ID using Perpay user ID + device ID, there are two possibilities: \\nAmplitude IDs are captured incorrectly\\nDevice ID field in Amplitude table may not be entirely reliable (this explanation seems more likely)',\n",
       "  '312\\nsomething is off here about credit limit. Either the rdd_config is getting rdds incorrectly or public.card_account_snapshot is \\noff (look at borrower_id = 21032; card_account_id = 4612)…. QA → fixed. something was off with the credit_limit \\ncalculation that was there prior (only for very few cases). Updated logic to use the RDD metric and coalesced with the \\nprevious logic as a backup.\\nreformatted\\n \\nCard borrower hist daily data model - waiting for column definition confirmation (at bottom) before testing to see if anything \\nthere should be modified\\nPR 2023-02-03: Update to the Card Borrower Hist Daily DM- logic & format changes\\nCard borrower hist daily fact\\nGood → reformatted\\nCard borrower hist daily status\\ndocumentation: Card Borrower History Daily Data Model \\nRight now, we take someone’s card status (in a field called deserve_card_account_status) from an SFTP',\n",
       "  'Other connections I had to add during the metric layer conversion:\\nAccess to the production Airflow bucket for the staging Airflow EC2 instance.\\nLet’s expand on the above.\\nAccess to Prod Datahub\\nSee the production security group named ana-prod-datahub-backend-sg in the production environment. Under sgr-\\n0617e2a94c05e1843, we have given access to Datahub GMS endpoint on port 8080 to the jump box that allows traffic from the staging \\nAirflow instance to the production environment.\\nAccess to the Prod Redshift Cluster\\nTake a look at the prod Redshift cluster security groups. Under the name ana-prod-redshift-sg you will see a security group rule \\nallowing access from another security group named ana-prod-redshift-access-sg  on port 5439. If you now navigate to the jump box \\nnamed ana-prod-stage-to-prod-jump-box-ec2 (the jump box we described above), you will see that one of the security groups attached \\nto the jump box is the very same ana-prod-redshift-access-sg security group.',\n",
       "  '6            \"Action\" : [\\n7                \"glue:*\" ,\\n8                \"s3:GetBucketLocation\" ,\\n9                \"s3:ListBucket\" ,\\n10                \"s3:ListAllMyBuckets\" ,\\n11                \"s3:GetBucketAcl\" ,\\n12                \"ec2:DescribeVpcEndpoints\" ,\\n13                \"ec2:DescribeRouteTables\" ,\\n14                \"ec2:DescribeNetworkInterfaces\" ,\\n15                \"ec2:DescribeSecurityGroups\" ,\\n16                \"ec2:DescribeSubnets\" ,\\n17                \"ec2:DescribeVpcAttribute\" ,\\n18                \"iam:ListRolePolicies\" ,\\n19                \"iam:GetRole\" ,\\n20                \"iam:GetRolePolicy\" ,\\n21                \"cloudwatch:PutMetricData\"\\n22            ],\\n23            \"Resource\" : [\\n24                \"*\"\\n25            ]\\n26        }\\n27    ]\\n28}\\nI would highly recommend going through the above and removing them one at a time to see if they are absolutely necessary for the',\n",
       "  'failed. \\n** something to keep in mind: now that dm full tasks are more independent as of [November 2, 2022], you need to check other data \\nmodels that have already ran to make sure they don’t need to be reran as well.  If they hadn’t hit a test yet they wouldn’t have fully errored \\nyet, so we want to rerun them before they do error soon\\ni.e. iterable_loan_dm fails bc company duplicate\\nso, we’d restart loan_fact, as that pulls from public.company, but we also need to restart user_fact, as it also pulls from public.company \\nbut hadn’t hit the slack error just yet (and will hit the error if it isn’t reset)\\nFollow-up\\nCheck that DM full tests that had initially failed now run to completion.\\n [PROD] - SPECTRUM - Test for data_model_full failed, please check the logs.\\n[PROD] - SPECTRUM - data_model_full DAG failed, please check the logs.\\n[PROD] - SPECTRUM - iterable_update DAG failed, please check the logs.\\n[PROD] - SPECTRUM - Test for iterable_update failed, please check the logs.\\n1select',\n",
       "  'status and current balance) must be \\nreported as of the date in this field. For \\nmonthly reporters, report the date within \\nthe current month that represents the \\nmost recent update to the account.If the account status is 62 (the \\naccount has been collected on, \\nzero balance), then it is the date \\nthe account was recalled from \\nTrueAccord due to 0 balance. If \\nthe account is settled with \\nTrueAccord, it is the date of last \\npayment to TA. Otherwise, the \\ndate is of 2 Saturdays ago \\n(EOW, displaced by a week).perpay_risk_dat\\namart.trueaccor\\nd_upload_log: \\nreason, \\nupload_dt, \\nrecall_dt, \\nfailed_upload_dt\\nFCRA \\nCompliance/ \\nDate of First \\nDelinquencydate_first_delinq\\nuencyField to unsure compliance with FCRA. \\nReport the day the account went 30 days \\npast due for not current and not-in-good-\\nstanding accounts.The most recent date that \\ndays_past_due = 30 for the \\nborrower.borrower_hist_d\\naily_status.days\\n_past_due',\n",
       "  'Network A network monitor checks whether a \\ngiven endpoint is active. Monitors also \\ncan alert over a percentage on a cluster \\nbased on custom network tags defined \\nin the Agent and host tags. \\nOutlier Outlier monitors detect when members of \\na group (e.g., hosts, availability zones, \\npartitions) are behaving unusually \\ncompared to the rest. They are useful for any metric collected via \\nthe Datadog Agent or API for which a given \\ngroup should behave uniformly.\\nProcess Check For each process, a single service check \\nstatus is produced. \\nService Check Service check monitors are useful when \\nmonitoring user-defined checks, such as \\nthe status of an application. The Datadog',\n",
       "  \"275\\nProcedure\\nCeltic wants to see a full example report for each quarter, so 100 randomly generated accounts for Quarter start-Quarter end dates. The query below generates a true random sample (not split particularly \\nbetween the 3% and 2% rewards multiplier groups).\\n1. Run the query, subbing in [Quarter start dt] and [Quarter end dt]\\na. E.g: for Q1 2024, this is '2024-01-01' and '2024-03-31'\\n \\nThis query uses the platform_payments metric layer for card payments. It’s the one I’d use to generate the report:\\n54    ORDER BY d.email desc\\n55),\\n56-- Sum the rows to capture the total amount of redeemed rewards\\n57rewards_redeemed_summed as (\\n58    SELECT\\n59        borrower_id ,\\n60        email ,\\n61        account_id ,\\n62        to_char (sftp_dt:: date, 'YYYYMM' ) as yymm,\\n63        sum(rewards_redeemed_usd )::decimal(10,2) as total\\n64    FROM rewards_redeemed_unsummed\\n65    GROUP BY 1, 2, 3, 4\\n66    ORDER BY email desc\\n67),\",\n",
       "  '429\\nperpay_general_datamart_int.core_pinwheel_eligibility contains a daily historical record for each company of whether or not the \\ncompany was eligible for Pinwheel. By serializing this table by company and date, a metric layer can be formed by selecting the first record \\nfor which a company becomes eligible for Pinwheel and every subsequent date that a company’s Pinwheel eligibility changes. If the logic is \\ncorrect for both the Pinwheel eligibility metric layer and the core eligibility table, then these two tables should be the same. After adding \\nadditional data sources to the core eligibility table that it originally was not taking into account, it was the case that the two tables were the \\nsame. \\nComputationally it would be less expensive to build the metric layer by solely using the information that is already stored in the core eligibility',\n",
       "  \"2select a.*, b.email\\n3from perpay_risk_datamart.equifax_raw_data as a\\n4left join perpay_general_datamart.user_attribute as b on b.borrower_id = a.consumer_account_number\\n5where lower(b.email) like '[USER_EMAIL_ALL_LOWER_CASE]%'\\nRDW rdw Character length of record   Experian NameTable Column Experian Definition Perpay Field Clarification Dependent \\nDatamart \\nTables/Fields\",\n",
       "  'Cases where there is a failed withdrawal, but the user’s core balance does not reflect this money being added back.5 9 7 1 9 1 2 1 9 3 6User ID with Discrepancy Withdrawal Request ID\\n1 1 1 0 5 9 8 1 5 5 5 1\\n1 2 0 4 8 3 8 3 1 6 5\\n1 3 3 2 8 0 7 1 3 3 2 6 0, 1 3 4 7 9 2\\n2 3 4 6 6 4 1 6 2 0 8\\n3 0 0 4 3 4 2 6 2 2 9User ID with Discrepancy Card Payment ID\\n1 4 0 6 3 7 5 6 5 2 0 7 3 7User ID with Discrepancy Borrower Credit ID\\n1 5 0 9 1 3 4 2 0 3 2 5 5 0\\n2 8 0 6 7 9 5 1 4 6 2 3 0 7User ID with Discrepancy Deposit ID\\n1 6 3 6 2 6 5 7 2 7 9 8\\n2 7 0 1 0 0 5 6 2 7 1 9User ID with Discrepancy Withdrawal Request ID',\n",
       "  \"account --- this might happen if the user wishes to move the \\npayment to another account that is under the same \\nhousehold, or they mis-typed their user ID. For example, let’s \\nsay the user tries to pay down a balance but is signed onto \\ntheir partner's account. Wrong account receives the money. \\nThey call us, we move the money to another account. We \\nmight need a bit more information on this. The example in \\nPreston’s spreadsheet can be traced from achdetail to \\ndeposit to platform_deposits to the rollforward table. We need \\nto know where the issue is with this one.This isn’t really an issue. With these cases, \\nEng makes it look like the from user has \\nnever received the deposit. Then the \\nACHDetail is matched to the correct user \\nbecause a new deposit is created at the \\ntime of matching the ACHDetail. Thus, \\nthere should be no variance.\\nJesus seems to be a one-off case that \\ndoesn’t even exist in our table today. I’m \\nassuming Preston looked at the table the\",\n",
       "  '319\\nIt appears 4 “refunded” transactions are missing from the Perpay Deserve data. Unclear exactly what a “refunded” transaction is - a guess: \\ndeclined transaction. These transactions occurred in quick succession of each other, and is only one user, so I’d guess it was maybe too \\nmany webhooks / APIs for the system to handle. \\nImpact // Examples: \\nTransaction IDs (less the ones created by Deserve employees)\\n6bf38ad0-4ff7-529e-a694-280bda97c1f1\\nf7657edd-b4f4-570f-9666-ac26b1426cf0\\n8663ecec-f90a-544e-97a6-8a3502d89d2e\\ncc32c5db-df7e-542d-9428-717417b31aa1\\n \\nPerpay Deserve Data Issue # 4: Transaction Latency\\nThere is latency between when a transaction is reported settled and when it is marked as settled in our database. Choosing 2 hours as \\nour baseline latency:\\n \\nThere are 4,584 (out of 114,769, ~4%) transactions with this latency. Of transactions with latency, the average latency is 83 hours. Of all \\ntransactions, the average latency is 3 hours. \\n \\n22limit 100;\\n1with transaction_1 as (',\n",
       "  \"250\\nAccount Review Data Send\\nEvery 20th of the month, we send Experian a CSV containing all requisite PII for all borrowers on our platform with a SSN. This CSV is \\nuploaded to their SFTP server and then we receive the current months Vantage score for these borrowers. This initiative is a part of \\nExperian's Quest program.\\nProcedure\\n1. Begin the procedure on the 20th of every month\\na. DE has a reoccurring “meeting” set for this date (with the link to this page in the description)\\n2. Hop onto the PRODUCTION EC2 instance and exec into the worker \\na. \\n3. cd into the dags directory and run the following:\\na. \\nb. This file will only create a csv file located here: \\n/dags/analytics/credit_reporting/lib/untracked_files/experian_vantage_pull_borrowers.csv\\nc. Only include the -s (send slack notification) and -p (send the slack notification to the prod channel) if this is not a test\\nd. I send a follow message in teh perpay_plus_reporting slack channel that I an the reason the message fired\",\n",
       "  '73            \\'{\"enabled\":\\' || enabled ||\\n74               \\', \"started_card_appliaction\":\\' || started_card_appliaction ||\\n75               \\', \"submitted_card_application\":\\' || submitted_card_application||\\n76               \\', \"approved\":\\' || approved ||\\n77               \\', \"activated\":\\' || activated ||\\n78               \\', \"provisioned\":\\' || provisioned ||\\n79               \\', \"shipped\":\\' || \\'null\\' || -- TBD\\n80               \\'}\\' as card_funnel,\\n81            \\'{\"count\":\\' || coalesce(j.total_transactions_count, 0) ||\\n82               \\', \"amount\":\\' || coalesce(j.total_transactions_amount, 0) ||\\n83               \\'}\\' as lifetime_transactions,\\n84            \\'{\"card_status\":\"\\' || coalesce(q.status, \\'\\') || \\'\"\\' ||\\n85               \\', \"card_status_reason\":\"\\' || coalesce(q.reason, \\'\\') || \\'\"\\' ||\\n86               \\', \"card_funnel\":\\' || card_funnel ||\\n87               \\', \"credit_limit\": \\' || coalesce(f.credit_limit, 0) ||\\n88               \\', \"minimum_payment\": \\' || coalesce(o.minimum_payment, 0) ||',\n",
       "  \"535\\nOther Department Access Controls\\nRegarding data governance, we underscore one of its fundamental pillars: the limitation of permissions. Exercising control over who \\naccesses what data is paramount to safeguarding our assets and ensuring regulatory compliance. Granting permissions should not be a \\ncasual act but a deliberate and cautious decision. Each permission bestowed carries inherent risks, from data breaches to misuse, which \\ncan have consequences for our organization's integrity and trustworthiness.\\nTherefore, it’s important to adopt a principle of least privilege – granting individuals only the permissions essential for the performance of \\ntheir duties. This approach minimizes the attack surface, mitigates insider threats, and fosters accountability across the board.\\nFurthermore, permissions should be viewed as dynamic entities, subject to regular review and adjustment. As roles evolve and\",\n",
       "  \"update for these totals.The code to update the variance sheet lies in a class called ManualPOMatching, within the manual_po_matching_class.py file (PATH: \\ndags/analytics/accounts_payable_revamp/lib/po_matching/manual_po_matching_class.py)\\nIf the rows closer to the bottom don’t turn color upon the ready_for_qb change but are supposed to, you may need to modify the \\nconditional formatting to account for the number of rows in the sheet. It’s currently set up to go to row 1008.\\nThe only case the old records will not be cleared from the sheet: if there are no new records to insert into the sheet and records \\ncurrently on the sheet are marked as ‘Yes' or ‘No’. However, the tables are still being updated, so the ‘Yes’s will still go to the \\npo_matches table and the 'No's are still removed from the po_variances table. The sheet will clear on whichever following run has \\nnew data to insert into it.\",\n",
       "  '132\\nHere’s how we’ll proceed for this specific case, which will be similar for most other cases (an address in a name field):\\ncopy the address (POMPANO BEACH FL 33069)\\nlook into perpay_accounting_datamart_ext.docparser_invoices_master where vendor name is that address\\ngrab its PO (invoice number)\\ncreate temp_int.docparser_invoices_master where the po != the po we collected\\ncheck that the internal table exists before the next step\\nneed to do this on prod admin\\nalso save the contents of perpay_accounting_datamart_ext.docparser_invoices_master to your users schema for safety!\\nthen go to airflow → utils_spectrum DAG to create the ext table from the temp_int table\\ncopy the parameters to create the table\\ndon’t forget to change the table name (experian_borrowers) to the table you want (docparser_invoices_master)!!!!\\nand don’t forget to change the schema name (to perpay_accounting_datamart_ext)\\ntrigger the dag with the config you choose',\n",
       "  '#6. DE calculated card account balance goes down as soon as a \\ncard payment hits COMPLETED, even beyond May 08. Because \\nDeserve decrements card account balance once its hit PENDING \\nin the newer paradigm (May & beyond), we don’t need to \\ndecrement on COMPLETED anymore.We put in a fix that prevents the \\ncalculated card account balance \\nfrom going down as soon as a \\ncard payment hits COMPLETED \\nbeyond may 08. This came from \\nour new understanding that \\nDeserve decrements card \\naccount balance once its hit \\nPENDING in the newer \\nparadigm (May & beyond) \\ninstead of COMPLETED.\\n8 BLOCKED Abby/Andre\\nwProcessing payment timing (link): Card account balance should \\ndecrement the amount of a PROCESSING payment on the day it \\nhits the PROCESSING state, as of May 8, 2023. We’re seeing a \\nlag in the time it takes the card account balance to decrement for \\nsome cases.Conor raised a ticket about this \\nand we have yet to hear back. \\nHe believes the SFTP files are \\ngenerated by Deserve whereas',\n",
       "  '29    null as borrowers_val ,\\n30    null as app_installs ,\\n31    null as app_installs_val ,\\n32    null as checkout_completed ,\\n33    null as checkout_completed_val\\n34from perpay_general_datamart_int .marketing_daily_fact a',\n",
       "  'accounts_payable_int.ecomm_po_data are finished.\\nii) accounts_payable_int.variance_sheet_output_history can be created once \\naccounts_payable_int.variance_sheet_output is finished.\\n3:\\naccounts_payable_int.po_matches and accounts_payable_int.po_variances may be run concurrently, once everything \\nmentioned in 2 completes (and once accounts_payable_int.qb_bills completes).\\n4:\\nDump variances into sheet & create accounts_payable_int.existing_bills_for_qb_upload and \\naccounts_payable_int.new_bills_for_qb_upload. Send contents of these new tables to quickbooks.\\n5:\\ni) accounts_payable_int.po_matches is inserted into accounts_payable_int.qb_history and \\naccounts_payable_int.po_matches and temp_marketplace_int.po_matches are dropped and it’s ddl is recreated.\\nii) Tests run once the tables they’re testing have finished updating. \\nDiagram\\nHere’s a diagram which offers a visual perspective. Tables are in yellow with their creation indicated by dotted lines.',\n",
       "  '110\\nSQL Queries\\nQuery readability is super important! It helps DEs understand code faster. \\nThese are some query conventions everyone should practice when planning to add code to the codebase. \\nReadability\\nReadability is SUPER important to us! If your PR does not follow these conventions, we will not merge it!\\n(1) Commenting conventions\\nEach table in our schema should have a general comment at the top of its creation!\\nAdditionally, each block of code should include a comment above it! \\nThe example above from platform_merged_users.py shows these well (lines 4-6 and each line above the CTE creations). \\n(2) SQL Capitalization\\nall SQL commands should be written in uppercase letters (i.e SELECT, FROM, WHERE, ORDER BY, GROUP BY, JOIN, MIN, MAX, SUM, FIRST, LAST, DESC, ASC, ON)\\nThis shouldn’t be too much of a burden, as it can be done once you’re done writing all your code:\\nIf you use VSCode:\\ninstall Upper Case SQL from the extensions panel',\n",
       "  '35\\nUsing the Unittest library for Data Quality Tests\\nFramework\\n1. Add a task (or tasks) which runs all unit tests as the first step in a DAG. This is a fail fast approach that guarantees that if a test fails, the \\ndata model will not run.\\n2. Separate queries which gather the base data, or inputs, to the logic or calculations. In the logical queries, template the names of the \\nbase table(s) so that we can point to mock data in the following step.\\n3. Mock the input data in a unit test and run the logical queries against a table containing this mock data. Assert that the results match the \\nexpectations.\\nStep 1: Adding the test task\\nThe test task can be added to a DAG with a helper function\\nand should run before anything else as shown below.\\nStep 2: Designing the queries\\nSuppose we had a real estate company and wanted to build a report that counted the number of restaurants within a 5 miles radius for each',\n",
       "  '493\\ntable is missing certain subsets of merged users (which are \\ncausing the variances that Preston observed). \\n4 COMPL… @Derya \\nMeral Date Mismatches Between ACH Files and Rollforward \\nTable: These seem to be due to the way we create the \\ntimestamps of deposits. Deposit timestamps do not reflect \\nthe timestamp of when the ACH deposit was actually \\nreceived, but the timestamp of when the deposit entry was \\ncreated. For accounting purposes this timestamp does not \\nmake sense as it can be multiple days after the money \\nactually makes it into our hands. So we will simply take the \\noriginal creation timestamp from the achdetail table. \\n5 COMPL… ErnestPayments Moved To Different Accounts After Being \\nPosted: There is a process that engineering is responsible \\nfor that allows for payments to be transferred to another \\naccount --- this might happen if the user wishes to move the \\npayment to another account that is under the same \\nhousehold, or they mis-typed their user ID. For example, let’s',\n",
       "  '123\\nCommon Errors\\nThis folder is to serve as the basis of knowledge to correct errors we see all the time. It includes known error messages, solutions, and \\nsome suggestions to keep these errors from occurring again! \\nThe inclusion of ( E r r o r ) in each page title under this folder is for search-bar optimization purposes. When you search a topic, it allows for an \\neasier distinction between these error handling pages and the generic documentation via title. Please include it when you add a new page to \\nthis folder.',\n",
       "  'reported with monthly values, which will include data from the last completed month.\\nData General Specifications:\\n1. The Perpay portfolio will be regarded as a “revolving” portfolio. This portfolio type is similar to a credit card, in that customers who have \\nborrowed with us and have a spending limit but are not actively using the platform are being reported as using 0% of their credit limit; \\ntherefore, even inactive borrowers are benefiting week over week. Experian defines this portfolio type as:\\n2. Once a borrower is reported to Experian, they must be continuously reported week over week; we cannot stop sending their data.\\n3. Every week, borrower data is sent as of end of the last week. One of the fields (payment history profile) includes borrower history for the \\npast 24 months. So, any single record for a borrower sent to Experian will include data over the past 24 months.\\nMetro 2 Formatting and Compliance Detail:',\n",
       "  'meeting notes keep a brief running log of what the current outstanding asks are:\\nThat said, when it’s time for the rotation to change over it’s probably worth taking five minutes to catch the next person up on what the \\ncurrent progress is.\\nProbably doesn’t need to be said, but the credit reporting code requires a lot of precision, so when making the updates requested or when \\nupdating the code for any reason, be sure to test thoroughly!\\nOperations Adhoc Questions\\nThe ops team generally handles credit disputes that come in. One of our customers can dispute how their trade line is being reported with \\nPerpay, and ops will resolve the dispute and enter the data we want to report for the customer into E-Oscar. Occasionally ops will reach out \\nto clarify how we came to a certain data point.',\n",
       "  '166\\nProd Errors 2023-07-10 Onwards\\nThis page will allow us to keep track of the types of errors that fire and the frequency that they do for each week starting on 2023-07-10.\\nThis allows us to see how often an error or warning is sent each week and, at a glance, if it is firing across multiple weeks. Weeks start on a \\nMonday and end on a Sunday to align with our prod error fielding schedule. The name of the team members fielding the errors are also \\nunder each week title so that others may go back and ask the fielders questions if needed!\\nWeek of   -  \\nDerya & Ernest\\nWeek of   -  \\nAbby & Andrew\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nDerya & Hongkai\\nWeek of   -  \\nAbby & Andrew\\n \\nWeek of   -  \\nDerya & Gabe',\n",
       "  'error_message varchar The error message from Domo \\nabout the error\\nerror_status varchar Either \\'concurrency_issue\\' , \\n\\'import_error\\' or \\n\\'unknown_error\\'\\nerror_phase varchar A string containing a comma-\\nseparated list of the phases of the \\nexecution that occurred prior to \\nthe error\\neg. \"INIT,QUEUE,IMPORT\" \\nindicates the execution occurred \\nduring the import phase\\nThe full list of possible phases are:\\nINIT\\nQUEUE\\nIMPORT\\nDATASTORE\\nINDEX\\nFINAL\\n(note: not all phases may be \\npresent in a particular execution)\\nVariable Name: Data Type: Description:',\n",
       "  '311\\nreformatted\\nRemove processing payment fields → removed\\nRemove pending purchase fields  → removed\\nBreak fees into types similar to above → broke fees up, updated the dependencies again and update documentation\\n \\nCard user data model ✅  PR is up\\nPR 2023-01-31: Update to the Card User DM- logic & format changes\\nConfirm all documentation – looks off  → looking into this, it looks like documentation was unfinished and preceded changes to \\nthe code. Outlined information to date here Card User Data Model (updated) based on the tables so that it doesn’t get \\nconfused with the current page in the DA space.\\nCard user fact \\nGood → reformatted\\nCard user detail\\nGood → reformatted\\nalso made a logic update\\nstaff ind value used to be collected in a cte, from the metric layer, atomic_int.card_key_relations(which gets it \\nfrom public.user). Instead, I changed it to come from card_int.card_user_fact',\n",
       "  \"in the card you were referring to. However, it's worth noting that the core table is not yet complete, so I wouldn't recommend relying on it \\nsolely for refund calculations at this point. I want to emphasize that both domo cards you're examining seem accurate but represent different \\nperspectives/contexts.If you prefer the commerce loan table to reflect the entire refund amount, we can certainly explore making that \\nadjustment. But I think it's important to consider that doing so would alter the table's context. On a row-by-row basis, you wouldn't be able to \\ndirectly infer that the entire amount went toward the loan balance.\\nIf we decide to pursue this approach to have the refunds column reflect the total refund amount, my suggestion would be adding a new \\nfield, perhaps amount_towards_account_balance, to capture any funds directed toward the account balance rather than the loan balance.\",\n",
       "  \"22where  b.payment_id is not null and\\n23       perpay_status = 'RETURNED' and\\n24       deserve_payment_status = 'COMPLETED'\\n25       --a.created < current_date and a.created > '2022-11-03' --and c.card_payment_id is not null\\n26order by a.created desc\\n27limit 100;\",\n",
       "  '468\\ninterest_charge\\ninterest_charge_adjustment\\ncash_backSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all INTEREST_CHARGE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\ninterest_charge field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nINTEREST_CHARGE.\\nSource Tables:',\n",
       "  \"116\\nLogic layout\\nBroader logic conventions\\n(1) Don’t compare Boolean values to True or False using the equivalence operator\\nIn python, an if will be entered if the condition is true. This is inferred; there is no need to check for the boolean value. \\n(2) Use the fact that empty sequences are false in if statements\\nThere’s no need to check the length of sequences in Python.\\nReferences\\nHow to Write Beautiful Python Code With PEP 8 – Real Python \\n \\n53                df = pd.read_csv (io.StringIO (response_data .content.decode('utf-8')))\\n54            else:\\n55                df = pd.DataFrame ()\\n56            if df.empty:\\n57                # If the CSV didn't return anything, log the error, raise an error.\\n58                logging .info('[Impact API] No data for actions. Please investigate!' )\\n59                raise Exception (f'[ImpactApi] Issue with response! No data returned for actions: {response_data .text}. URL: {url}')\\n60\\n61            # Rename columns\",\n",
       "  'Additionally, feel free to reach out with any questions.\\n \\nHave a great rest of your week, and I look forward to speaking with you soon!\\n \\n<YOUR NAME>\\nHi <THEIR NAME>,\\n \\nGreat! No need to prepare anything, just a phone call to get to know you and see if we can be a good fit for your career goals. \\n \\nI look forward to the chat,\\n<YOUR NAME>',\n",
       "  '353\\nPages Events\\nThis page explains the difference between segment_storefront_schema.pages and segment_web_schema.pages.\\nThe Difference Between the Pages Table in Each Schema\\nsegment_storefront_schema.pages tracks \"page loaded\" events for sites corresponding to the storefront, \\nwhile segment_web_schema.pages tracks this event for the website only.\\nFor example:\\nAn event in segment_storefront_schema.pages would come from a storefront page (in terms of the URL), similar to\\nhttps://shop.perpay.com/featured/gifts-for-him-2022?p=3&modal=tireWidget\\nor\\nhttps://shop.perpay.com/search?query=Gifts%20for%20couples\\nThe url corresponds to some marketing campaign on the storefront or a search query on the storefront.\\n \\nIn contrast, a URL in the segment_web_schema.pages table would be look like\\nhttps://app.perpay.com/sign_up\\nwhere they person is signing up to be a Perpay customer, but the site isn\\'t directly on the storefront.',\n",
       "  '73\\nAppendix B: Product Data\\nNote: stored in perpay_marketing_datamart.iterable_catalog\\n256 ],\\n257            \"carted_products\": [],\\n258            \"purchased_products\": [44444, 77789],\\n259            \"purchased_products_detail\": [{ // All products that went into repayment.\\n260 \"product_id\": 44444,\\n261 \"purchased_dt\": \"2018-10-16\"\\n262 },\\n263 {\\n264 \"product_id\": 77789,\\n265 \"purchased_dt\": \"2019-11-29\"\\n266 }\\n267 ]\\n268 \"application_started_products\": [553325],\\n269 \"awaiting_payment_products\": [51555],\\n270 \"repayment_products\": [541111],\\n271 \"verification_products\": [987111]\\n272        },\\n273        \"account_management\": {\\n274                    \"days_past_due\": 25,\\n275                    \"pathway_modifier\": null,\\n276                    \"workable_phone\": 1,\\n277                    \"workable_text\": 1,\\n278                    \"workable_email\": 1,\\n279                    \"strategy_email\": \"Standard\",\\n280                    \"strategy_email_override\": null,',\n",
       "  'For json columns, we check for:\\nThe types of each key to match the expected types (see more about Json testing below)\\nAll keys are required\\nNo extraneous keys exist\\nTesting JSON values:\\nTesting json values relies on logic from the jsonschema library. The validation criteria needs to be outlined in json-schema format, and then \\npassed into the expectation as an argument. Please see the jsonschema documentation for more information about the formatting of the \\nvalidation criteria.\\nFor example (from their website):\\n1batch.expect_column_distinct_values_to_be_in_set(\\n2        column=\"phone_verified\",\\n3        value_set=[\"true\", \"false\"],\\n4        result_format=\"SUMMARY\"\\n5    )\\n6\\n1batch.expect_column_mean_to_be_between(\\n2        column=\"balance_outstanding\",\\n3        min_value=0.0,\\n4        max_value=190,\\n5        result_format=\"SUMMARY\"\\n6    )\\n7',\n",
       "  'portfolio. Values available: C = Line of \\nCredit, I = Installment, M = Mortgage, O \\n= Open, R = RevolvingR (revolving)  \\nAccount Type account_type Specific code for account classification. \\nSee exhibit 1 for definitions.07 - charge account.  \\nDate Opened date_opened Date the account was originally opened. \\nThis cannot change for the account, and \\nmust be filled.borrower_fact.created borrower_fact.cr\\neated\\nCredit Limit credit_limit For revolving accounts: Assigned credit \\nlimit. For closed accounts, continue to \\nreport the last assigned credit limit.Spending limit as of end of last \\nweek.borrower_hist_d\\naily_status.spen\\nding_limit\\nHighest Credit or \\nOriginal Loan \\nAmounthighest_credit_o\\nr_originalFor revolving accounts: Highest amount \\nof credit utilized by the consumer.The maximum outstanding \\nbalance on any given day of the \\nborrower to date. \\nTerms Duration terms_duration Contains the duration of the credit \\nextended. For revolving accounts, \\nconstant of REV  \\nTerms',\n",
       "  'TicToc Ads\\nWe get insights data like cost etc per campaign vie thier API and Fivetran\\nIntegrations for reverse ETL\\nWe wrote the code to send the data to these platforms\\nAlgolia\\nThe tool that engineering uses to power search in the storefront. We send data as the index, IE our product catalog\\nPerpay Platform\\nWe have created an automated borrower credits job that sends these data to the core Perpay Platform\\nFacebook audiences\\nWe manage audiences for targeted marketing in Facebook\\nGoogle audiences\\nWe manage audiences for targeted marketing in Google\\nMarketing Attribution Platforms\\nIterable\\nSend data to Iterable using Census\\nFor emails and texting marketing\\nSegment\\nAuthenticated events and third party data (“Authenticated amplitude”)\\nAmplitude\\nUnauthenticated events (user level)',\n",
       "  \"36            and b.status = 'canceled'\\n37        ) as e on e.borrower_id = a.borrower_id\\n38    left join (\\n39            select b.*\\n40            from {{pub_schema}}.feature_history as b\\n41            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\\n42            left join {{pub_schema}}.feature as c on c.id = d.feature_id\\n43            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n44            and b.status = 'opted_in'\\n45        ) as f on f.borrower_id = a.borrower_id\\n46    left join (\\n47            select b.*\\n48            from {{pub_schema}}.feature_history as b\\n49            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\\n50            left join {{pub_schema}}.feature as c on c.id = d.feature_id\\n51            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n52            and b.status = 'pending'\\n53        ) as g on g.borrower_id = a.borrower_id\\n54    group by a.borrower_id;\",\n",
       "  '289\\n+ commerce_loan_payments_for_awaiting_payment_existing\\n+ commerce_loan_payments_for_canceled\\n+ commerce_loan_pre_payments\\n= starting_balance_calc\\n \\nstarting_balance\\n– payments\\n– refunds\\n+ returned_payments\\n= ending_balance\\n \\nstarting_balance_calc\\n+ direct_deposit_deposits\\n+ credit_card_deposits\\n+ debit_card_deposits\\n+ check_deposits\\n+ commerce_cash_refunds\\n+ card_credit_balance_refunds\\n+ card_payment_returns\\n+ commerce_loan_payment_returns\\n+ collections_newly_pending_deposits\\n– withdrawals_paid\\n– commerce_loan_pre_payments\\n– commerce_loan_payments\\n– subscription_payments\\n– card_payments\\n= ending_balance_calc\\nNuances\\nAdditional Resources\\nWhile the DE team was building the tables, we created a few pages within Confluence to enhance our investigation efforts. \\nProcess objective overview: Summary page. Explains the goal of the project.\\nVariance pages: Contain an overview of each variance that was under investigation. As our exploration progressed, we consistently',\n",
       "  '109\\nCoding standards\\nThe more our codebase grows, the messier the code looks and the more difficult it becomes to understand. Coding standards are the best \\npractices and rules to create quality, readable, performant code and coding style. Some are encouraged and others are defined and \\nenforced as rules.\\nWhen writing code, it’s important to have these standards in mind:\\nproper indentation\\nimportant for readability for coworkers\\nmeaningful/specific names\\nimportant for readability for coworkers\\nfrequent and thorough comments\\nalthough we do not want the code to be crowded with comments (i.e. one per line), we want the code to have enough to allow \\nreaders to gain context quickly\\nlow repetition\\ntry to avoid code that’s repeated\\nhelpful to use variables, constants, functions, classes, data structures\\nthis reduces the need to change things in multiple places\\ncontext considerations',\n",
       "  \"82       original_charge_off_amount,\\n83       date_last_payment,\\n84       scheduled_monthly_payment_amount,\\n85       special_comment\\n86from perpay_risk_datamart.equifax_raw_data as a\\n87left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n88left join (\\n89    select * from public.forgiveness_period\\n90    where type != 'disaster_relief'\\n91    and cast(created as date) >= '2020-12-01'\\n92    ) as c on c.borrower_id = a.consumer_account_number\\n93where c.borrower_id is not null\\n94and enter_date = max_enter_date;\",\n",
       "  'shape and flow of the merge network made sense, the lead user was not the same as the last merged to account.\\nThe following are examples of cases (1), (2), and (4):\\nA cluster with cyclic merges.',\n",
       "  '145\\n(Warning) Multiple Primary Jobs\\nIn the airflow_prod_alerts channel:\\nWarning explanation\\nEach job associated with a borrower has a status. A job’s status can be TERMINATED , ARCHIVED , or PRIMARY .\\nIf you see this warning in Slack, it means that there is at least one borrower with multiple job records where more than one has a status of \\nPRIMARY. \\nThis is a problem because each borrower should only have a single PRIMARY  job!\\nInvestigation\\nTo investigate the warning, run:\\nThis will tell you which borrowers have more than 1 PRIMARY job status in our metric layer table, atomic_int.platform_job. If nothing is \\nreturned, the issue has been self-resolved! If there are borrowers returned, check their status once again in the public.job table like so:\\nYou should see more than one with a PRIMARY status.\\nSolution\\nIf it hasn’t self resolved (post-investigation), we let engineering handle this one (because the root of the prob comes from the public.job \\ntable)!',\n",
       "  '324\\nIt appears 4 “refunded” transactions are missing from the Perpay Deserve data. Unclear exactly what a “refunded” transaction is - a guess: \\ndeclined transaction. These transactions occurred in quick succession of each other, and is only one user, so I’d guess it was maybe too \\nmany webhooks / APIs for the system to handle.\\nImpact // Examples:\\nTransaction IDs (less the ones created by Deserve employees)\\n6bf38ad0-4ff7-529e-a694-280bda97c1f1\\nf7657edd-b4f4-570f-9666-ac26b1426cf0\\n8663ecec-f90a-544e-97a6-8a3502d89d2e\\ncc32c5db-df7e-542d-9428-717417b31aa1\\n \\nPerpay Deserve Data Issue # 4: Transaction Latency\\nThere is latency between when a transaction is reported settled and when it is marked as settled in our database. Choosing 2 hours as \\nour baseline latency:\\n \\nThere are 4,584 (out of 114,769, ~4%) transactions with this latency. Of transactions with latency, the average latency is 83 hours. Of all \\ntransactions, the average latency is 3 hours.\\n \\n22limit 100;\\n1with transaction_1 as (',\n",
       "  '(4) Calls upload_bills which calls upload_bills from dags/analytics/accounts_payable/lib/docparser_api.py.\\n1: Calls get_bills_to_pay which gets the data from temp_int.bills_to_pay created in \\ndags/analytics/accounts_payable/queries/bills_to_pay.py and returns it as a df. These are the invoices that have not already been \\npaid and are also not in the variance sheet yet. \\n2: Calls format_bills_to_batches from the Quickbooks API (dags/analytics/accounts_payable/lib/quickbooks_api/quickbooks_api.py) \\nto format those invoices that haven’t been paid to work with the API\\n3: Upload them to Quickbooks via add_bill_batches_to_qb from \\ndags/analytics/accounts_payable/lib/quickbooks_api/quickbooks_api.py',\n",
       "  'Post-upload\\nOnce the line items are sent to QB, the accounts_payable_int.po_matches table contents are inserted into the \\naccounts_payable_int.qb_history table with a created_ts for insertion time. Then, the accounts_payable_int.po_matches table is \\ndropped and it’s DDL is recreated, so the new information that comes from accounts_payable_int.po_source_combo and the information \\npulled in from accounts_payable_int.variance_sheet_output_history can still be inserted in on following runs.\\nThis qb_history table should be used as a reference for what has already made it into QB and when. There is a test that can be ran with \\nthe {\"test_bills_in_hist_table\": \"true\"} config to check that the qb_history table data is accurate (more info below in testing \\nsection). When the DAG runs normally, this test is not conducted.\\n4: Automated Testing',\n",
       "  'tion > 1000 select * from \\npublic.event \\nwhere id = \\n252576188;Engr \\ncanne\\nl\\n  2023-08-\\n06 A card borrower \\nhas  a negative \\nbalance yet has \\nnon-zero days \\npast dueWe have a test that \\nfails for this. I want \\nthe card dm to \\nupdate test_for_nega\\ntive_valueshttps://github.co\\nm/Perpay/perpa\\ny-\\nairflow/commit/\\n4f3aa91deb1c5\\n8551b936ff0d8\\ndc46f85aeb233\\n9 Will \\nreach \\nout \\ntomorr\\now\\n1    sele\\n2    orde',\n",
       "  '9\\nUsage and Contribution\\nDatahub (production location, staging location) serves as the central data catalog for not only the data teams but for Perpay as a whole. This \\nresource provides not only table and column-level information for data held within Perpay’s data warehouse, but also the lineage of the \\nupstream assets used to create the table/column and higher-level conceptual information. The platform is also connected to the data \\nwarehouse and orchestrator to ensure that what is being presented in Datahub is currently representative of the data environment.\\nAlthough schemas, tables, lineage, and other metadata can be automatically pulled in from the machinery that builds our data assets and \\nservices, the contextual information needs to be inputted by us measly humans (and/or AI-assisted). Therefore, this document outlines the \\nstandard operating procedure and style considerations for any DRAAFT team member to contribute to Datahub in order to maintain',\n",
       "  'in the console for the task and the instance it was clear the latency and errors were being driven by the high memory usage. The immediate \\nsolution was to increase the size of the DMS instance to where it had double the memory which has completely removed the issue (though \\ncosting and extra $15 a day). However, longer term, we need to not only re-balance the tables over all of the tasks but also explore the \\ntiming parameter for how long the batch tables are created on the instance. As of writing I am unsure how how drastic of an effect this could \\nhave but its worth exploring.',\n",
       "  \"** if there was a config passed in, all that would change is the timeframe which we collect parser data (i.e. get data parsed in \\nthe past month instead of only after yesterday) and the parsers which we collect (i.e. get data parsed from a single parser \\ninstead of all)\\n(3) Calls process_invoice_data which calls update_variance_sheet from dags/analytics/accounts_payable/lib/docparser_api.py. \\n1: This executes a few queries:\\n- The query to get PO information- the query in dags/analytics/accounts_payable/queries/po_invoice_overview.py\\n- logs [InvoiceMatchingV2] Executing query to get PO information...'\\n- writes it to ext schema as perpay_accounting_datamart_ext.po_invoice_overview\\n- The query to get variances- the query in dags/analytics/accounts_payable/queries/po_invoice_variance_master.py\\n- logs '[InvoiceMatchingV2] Executing query to get variances and put in po_invoice_variance master \\ntable...'\\n- writes it to ext schema as perpay_accounting_datamart_ext.po_invoice_variance_master\",\n",
       "  '411\\ni. If the from_email is in the graph already, but not the to_email, then update the node to the to_email and update the \\nold_emails list in the node properties with the from_email.\\nii. If both the to and from email exist in the graph, then send out a Slack alert. TODO: We should make a note of these cases or \\nactually implement the solution.\\nThe above process will generate a multi-directional graph of many small merge clusters. These clusters will be in the same graph but will not \\nbe interconnected. This is referred to as a weakly connected component in graph theory lingo and, would you believe it, NetworkX has a \\nmethod to separate them apart. So that is what we do next.\\nNow we are going to construct a list of lists that includes, for each subgraph (or weakly connected component), all emails in the cluster, all \\npotential lead emails, any emails involved in cyclic merges, the old emails associated with the cluster, and the last merged to email. In order',\n",
       "  \"Redshift\\n1-- Redshift\\n2-- Note: Concurrency scaling is when AWS has to pull in additional resources when\\n3--   the cluster is under heavy load (near 100% CPU)\\n4select * from (\\n5select\\n6concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\\n7cast(case\\n8 when usagetype = 'Node:ra3.4xlarge'  then 'Compute'\\n9 when usagetype = 'RMS:ra3.4xlarge'  then 'Storage'\\n10 when usagetype = 'CS:ra3.4xlarge'  then 'Conc. Scaling'\\n11 when usagetype = 'USE1-DataScanned'  then 'Spectrum Scan'\\n12 else ''\\n13 end as varchar(100))as usage_type ,\\n14 totalcost:: float\\n15from users_herr .aws_bill_total\\n16where productcode = 'AmazonRedshift'  and\\n17   linkedaccountname is not null and \\n18   linkedaccountname = 'Production Account'\\n19   --linkedaccountname = 'Staging Account'\\n20   and totalcost > 0\\n21   and invoiceid is not null\\n22)\\n23pivot (sum(totalcost ) for usage_type in ('Compute' , 'Storage' , 'Conc. Scaling' , 'Spectrum Scan' ))\",\n",
       "  '142',\n",
       "  \"509\\nResponse: for loan 4860890, I see a Payment on 6/15 4228798 of $10.81 with PaymentDetail 8696475 of $10.81 on 6/15 towards the \\nloan. It looks like you have $0 in payments towards that loan on 6/15?\\nFollow-up: oo weird, will check it out – hm I see that too, there must be something downstream on our side that's causing us to not pull it in- \\nthanks for catching this! \\nResponse: Okay, I'll hold off on digging further into that particular loan for now.\\nFollow-up: Alright sweet! so now that we know for sure that borrower credits are rolled up in the payments tables, can we circle back to \\nthese 64 variances? Knowing what we do now, it seems like the original understanding of the payments column was correct. So for loan \\n4860890 a payments value of $0 should be expected because the payment of $10.81 was already included in borrower credits. However, \\nthe loan balance on this day is decremented by $21.62 not $10.81. Can we double-check here that this borrower credit did not decrement\",\n",
       "  'Definition:\\nThe sum of all REFUND transactions associated with a given card account on a given src_dt. The transactions are associated with the \\ndate in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The refund \\nfield uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is REFUND.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:',\n",
       "  'transaction report. However, the change in account balance is seen on 2023-06-04 Deserve SFTP account balance report. Additionally, \\nthe customer’s monthly account statement (also prepared by Deserve) reports that the transaction was settled on 2023-06-04.\\ndeserve_account_balance_218.csv \\ndeserve_transaction_1050071.csv\\n June 2023 Statement v2.pdf \\nResponse: It looks like transaction occurred on 6/03, the balance was updated with with the transaction on 6/04 (which is indicated on \\nthe statement (post date = balance updated) and in the account balance SFTP reporting) but the transaction actually settled on 6/05.So \\nit seems that the balance may be updated prior to settlement. Are we seeing this consistently?\\nFollow-up Q: I did not realize that the post date and settlement date are defined differently. I will try to look into whether all account \\nbalance changes occur on the post date. In most cases, the account balance will change on the date of settlement, however it could be',\n",
       "  '547\\nFor the Datasync tasks, I had to add a few more policies. I won’t copy-paste all of them here, but the policy names are the following:\\nana-prod-datasync-cloudwatch-inline-access-policy\\nana-prod-datasync-inline-policy\\nana-prod-datasync-spectrum-s3-kms-policy\\nana-prod-spectrum-s3-datasync-policy\\nAccess to the Prod Airflow Bucket\\nThis one was a set of permissions that the metric layer required for handling Google Sheets. I imagine as you transition our codebase to the \\nnew staging DAG, you will come across other permissions that will be necessary. Keep expanding the policies here and keep Terraforming \\nthem.\\nThe policy is called staging-access-to-prod-staging-bucket-inline-policy, probably not the best name since it doesn’t make it clear \\nthat this is about the Airflow bucket, but here is the policy:\\n6            \"Action\" : [\\n7                \"glue:*\" ,\\n8                \"s3:GetBucketLocation\" ,\\n9                \"s3:ListBucket\" ,\\n10                \"s3:ListAllMyBuckets\" ,',\n",
       "  'query = f\"\"\"select\\n                     *\\n                 from {self.test_schema}.card_balance\\n                 where id is null or\\n                     card_account_id is null or\\n                     user_id is null or\\n                     borrower_id is null or\\n                     created_ts is null or\\n                     current_balance is null or\\n                     rank is null or\\n                     current_ind is null;\"\"\"\\n     data = self.spectrum.query_to_df(query)\\n     error_msg = f\\'FAIL: Null values present!!! {data.notnull().any()}\\'\\n     self.assertTrue(data.empty,error_msg)',\n",
       "  'leave a space between the # and the start of the comment message\\nIn line comments\\nIf a single line within a block is not straight-forward, use an inline comment to explain it. Any code that is complex enough where it would require an explanation to a co-worker in person should be \\ncommented. If the comment is too large, use the block comment convention.\\n The pound symbol should be used in this case: # This variable does this\\nleave a space between the # and the start of the comment message\\nExample using the above conventions:\\n1\"\"\"\\n2OVERVIEW: \\n3This class gets all attribution data from Impact\\'s Marketing API.\\n4It populates the perpay_marketing_datamart_ext.impact_actions table.\\n5\"\"\"\\n6class ImpactOrganizer (object):\\n7    # Constructor: sets all variables\\n8    def __init__ (self, config, customized_period =False):\\n9        self .config = config\\n10        self .customized_period = customized_period\\n11        self .spectrum = RedshiftSpectrum (config)',\n",
       "  'already).\\n1-- Copy the table\\n2create table users_herr .payment_riskdata as select * from public.payment_riskdata ;\\n3\\n4-- Drop the actual table\\n5drop table public.payment_riskdata ;\\n6\\n7-- Create the new table schema with the updated keys, using the ddl of the previous\\n8CREATE TABLE public.payment_riskdata (\\n9  id integer,\\n10  created timestamp  without time zone,\\n11  modified timestamp  without time zone,\\n12  source character  varying(150),\\n13  raw_data character  varying(32768),\\n14  loan_id integer,\\n15  user_id integer,\\n16  raw_xml character  varying(32768),\\n17  primary key(id) -- Notice the primary key is in the create table.\\n18)\\n19sortkey(created);\\n20\\n21-- Make the primary key column unique\\n22ALTER TABLE public.payment_riskdata ADD UNIQUE (id);\\n23\\n24-- Add the data back into the new table schema\\n25insert into public.payment_riskdata select * from users_herr .payment_riskdata ;\\n26\\n27-- Delete the copy\\n28drop table users_herr .payment_riskdata ;',\n",
       "  'Timeboards vs Screenboards\\nWhen integrations are installed, DataDog will automatically generate corresponding dashboards. The integration must be configured before \\nits data is populated on its board. New dashboards may be created as well. When a new one is made, you have the option to select a \\ntimeboard layout or a screenboard layout, where timeborads represent time across the entire dashboard (either fixed or real-time) and \\nscreenboards are free-form images, graphs, and logs.  \\nMonitors\\nMonitors keep the team informed by alerting slack channels when critical changes are occurring (based on conditions set based on monitor \\ntype).\\nAlerting is sent to the #airflow_prod_infrastructure_alerts channel.\\nMonitors  \\nTypes of Monitors\\nHost Every Datadog Agent collection reports a \\nheartbeat status check. A host monitor can \\nnotify on the status of this heartbeat \\nacross one or more hosts, which will give \\nyou a good indication whether your hosts \\nare responsive.EC2 is our only host',\n",
       "  '154\\n(Issue) WLM Configuration for Parameter Group\\nBackground: The Parameter Group\\nOur Redshift Spectrum cluster, ana-stage-redshift, has a parameter group attached to it named ana-stage-redshift-parameter-group. This \\nparameter group defines the database parameters and query queues. It is made up of parameters and WLM (workload management).\\nParameters\\nauto_analyze\\nauto_mv\\ndatestyle\\nenable_case_sensitive_identifier\\nenable_user_activity_logging\\nextra_float_digits\\nmax_concurrency_scaling_clusters\\nmax_cursor_result_set_size\\nquery_group\\nrequire_ssl\\nsearch_path\\nstatement_timeout\\nuse_fips_ssl\\nWLM (Workload Management)\\nWLM is used to define the number of query queues available and how queries are routed to those queues for processing through properties \\nsuch as memory allocation across queues and the number of queries that can run concurrently in a queue.\\nIssue\\nAs with all other infrastructure, parameter groups are configured in terraform.',\n",
       "  \"212\\nInvoice Matching V2 DAG Run\\nThis job runs every day at 1AM EST (6AM UTC). It is a fully automated process, so no manual work is typically required for it to run \\nsuccessfully (unlike V1). However, it's good to kick off the DAG once a month to reparse invoices that may have been previously missed. \\nMore instructions on that process can be found on this page. \\nIf the DAG throws an error, refer to this page.\\nWhat’s going on when we run this job?\\n \\nThis job resides in dags/analytics/accounts_payable/invoice_matching_dag.py. Be careful not to confuse this with the other \\ninvoice_matching_dag.py file which is responsible for the V1 process.\\nLooking into this file, we see the get_invoice_data function is called from dags/analytics/accounts_payable/lib/invoice_matching.py with \\ndifferent arguments based on the if statements. Each if statement checks for the config that’s passed in or not. If the config is passed in, this\",\n",
       "  '486\\nReasons for Variances\\nThere are 5 causes that Preston was able to pinpoint:\\n1. Return to Payroll and reversals are not identified\\nOne important thing we learned in the meeting regards the way non-payroll, non-ACH deposits are handled. Sometimes users will \\nsend us money through PayPal, Uber, or Stripe, but we do not accept these payment forms. So we will receive the deposit and then \\nreturn it to the payroll provider (without any interaction with the consumer). These payments are not captured in the rollforward table, \\nmeaning even though they appear in the achdetail table, they do not trickle into the deposit table. \\nOn the other hand, we do use Stripe for additional payments users might make, but that is our own Stripe account that users send \\nmoney to, so it is separate from the issue mentioned here.\\nDetails on how Return to Payroll works: Return to Payroll \\nTechnical details on how Engineering performs the reversal process: ACH Reversals and Rejections',\n",
       "  '41       b.minimum_payment sftp_minimum_payment,\\n42       a.statement_balance as internal_statement_balance,\\n43       b.statement_balance sftp_statement_balance,\\n44       a.due_amount as internal_due_amount,\\n45       a.remaining_minimum_payment_due as internal_remaining_minimum_payment_due\\n46from card_int.depreciated_card_borrower_hist_daily_status as a\\n47left join card_int.card_borrower_hist_daily_status as b on a.borrower_id = b.borrower_id and a.src_dt = b.src_dt\\n48where a.borrower_id = 414845',\n",
       "  'touched\\nhas_cards_ind integer A flag indicating whether the \\ndataset has cards built on it in \\nDomoVariable Name: Data Type: Description:\\npage_id integer ID of the page in Domo\\npage_name character varying Name of the page in Domo\\nparent_page_id integer ID of the parent page in Domo\\nparent_page_title character varying Name of the parent page in Domo\\nlast_touched_ts timestamp Time at which the page was last \\nviewed, shared, or updated.Variable Name: Data Type: Description:\\nobservation_dt date The date of the credit usage \\nobservationVariable Name: Data Type: Description:',\n",
       "  \"208\\n(2) check if already_parsed is true or false\\nif true- do nothing. Proceed to condition statement 3.\\nif false (default)- we enter the block\\n1: Logs '[InvoiceMatching] Fetching docparser and customer partner outputs...'\\n2: Creates a Redshift table for each vendor in the Non Parsed Partners Google Sheet. Saves these to the temp_int \\nschema with the table name invoicing_unparsed_<vendor> (i.e. temp_int.invoicing_unparsed_rymax). \\n- these three vendors are in our current sheet, so one table is created for each:\\n- BestBuy\\n- Rymax\\n- FragranceNet\\nAlso creates a Redshift table for the VendorKeyName sheet. This is saved to temp_int.invoicing_vendor_key_matching.\\n(if you want to look into the function that creates these tables, it’s all_invoicing_sheets_to_redshift in \\ndags/utils/lib/google_sheet_integration.py. It loops through the Non Parsed Partners sheet and calls the sheet_to_redshift\",\n",
       "  'correct justification. Generally, this meant numeric values would be right justified and filled with zeros on the left (formatted with zfill) and \\nstring values would be left-justified with spaces at the end (formatted with ljust). Please stick to this approach if you need to edit or add \\nanything.\\nThe other important thing was to validate the generated NACHA string at the end. This step can always be expanded, but it has held up well \\nso far and we have heard no complaints yet.\\nThe code also implements Slack alerts for cases where we can’t find a user match or when emails map to multiple accounting/routing \\nnumbers. You can find examples of these error messages in the #data_and_accounting channel on Slack.\\nAnother interesting detail is related to the warning at the beginning. NACHA files sent to banks from a single source on a given day require \\nfile IDs starting from A going down the alphabet. Old school, but fine. This meant that we needed to track the number of NACHA files we',\n",
       "  'file IDs starting from A going down the alphabet. Old school, but fine. This meant that we needed to track the number of NACHA files we \\ncreate everyday and label our NACHA files accordingly. To do this, I used the information available on the Airflow backend database to count \\nthe number of NACHA files we produced prior to the run. When you read through the DAG file, this is logic embedded as part of the wrapper \\naround NACHAFile().main() call.\\nThere are two configurations you can feed to the DAG: clear_sheet, which can be set to false during testing in staging, and file_id to \\noverride the file ID value, though I don’t recommend this approach, it should not be used if the problem can be fixed with the solution in the \\nwarning box at the top of this page.\\nScheduling\\nThe most quirky part of this job was its schedule. The reason for this is you cannot CRON your way out of the 10am, 1:30pm, 5pm situation.',\n",
       "  'error_message varchar The error message from Domo \\nabout the error\\nerror_status varchar Either \\'concurrency_issue\\' , \\n\\'import_error\\' or \\n\\'unknown_error\\'\\nerror_phase varchar A string containing a comma-\\nseparated list of the phases of the \\nexecution that occurred prior to \\nthe error\\neg. \"INIT,QUEUE,IMPORT\" \\nindicates the execution occurred \\nduring the import phase\\nThe full list of possible phases are:\\nINIT\\nQUEUE\\nIMPORT\\nDATASTORE\\nINDEX\\nFINAL\\n(note: not all phases may be \\npresent in a particular execution)\\nVariable Name: Data Type: Description:',\n",
       "  \"8    a.type_category,\\n9    a.transaction_amount as sftp_amount,\\n10    b.amount as core_amount,\\n11    a.account_id,\\n12    b.deserve_id,\\n13    a.cleared_at,\\n14    a.dt,\\n15    b.created\\n16from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report as a\\n17left join public.card_transaction as b on a.transaction_id = b.deserve_id\\n18where b.deserve_id is null\\n19--- if you want to look only at transactions after the 'fix' was deployed ---\\n20--and b.created >= '2022-10-17'\\n21order by dt desc\",\n",
       "  'open if engineering gave it an active or suspended status\\nclosed if engineering marked it anything other than active or suspended as a status (either charged_off, terminated, closed)\\nA card account is tied to a single borrower. \\nA card account may have multiple cards. However, the cards must be of a single type, and only one card under the account may be \\nACTIVE at a time.\\nborrower: a Perpay user who has been approved for a credit card. \\nA borrower may have multiple card accounts, with multiple cards under each account.\\nuser:\\ntransaction: a transaction between a borrower and merchant via the card\\nA transaction may be PENDING, CLEARED, or SETTLED.\\nTransactions have not been run through a process yet are considered PENDING transactions.\\nTransactions that make it through the clearing process are considered CLEARED transactions.\\nTransactions that make it through the settlement process are considered SETTLED  transactions.\\nThe term transaction and purchase are used synonymously.',\n",
       "  'There is a column history_type in waffle_flag_history that either contains the value ~ or -. We have logic in our current \\nPinwheel data model that checks this column and ignores the historical record if it contains -, does anyone remember what this was \\nmeant to represent?\\nDoes anyone know of anywhere else where there is historical pinwheel eligibility data besides waffle_flag, waffle_flag_history, \\npinwheel_company_legacy, and pinwheel_payroll_provider_legcay?',\n",
       "  \"181\\nOld Testing Paradigm (Prior to 2023)\\nThe paradigm\\nOriginally, Great expectations was developed to fill our testing needs, but due to the company being very early on in their lifecycle, the \\nproduct was not as flexible, fast, or easy to use as we hoped. Though there is an open source component, this did and would continue to \\nrequire that we support yet another 3rd party with another data/code abstraction. The largest negative, however, was that it was \\ndifficult/impossible to create validations with a high level of specificity.\\nKnowing this, we turned to using the built in unit test library. Each validation is now a SQL query that hits the DB and then returns either a \\nnumber or the rows that do not meet the test (moving forward we put limits on the failed rows so it doesn't stall out). Though this \\nmethodology is not as shiny and snazzy, it works, its fast, and is hyper configurable. It also uses all of our currently built tooling and\",\n",
       "  '21        for year in range(start_dt .year, end_dt.year+1):\\n22            \"\"\"\\n23            The Impact Reports endpoint\\'s ...\\n24            - From investigation, it appears that the API takes the datetime that is passed and adds 5 to it to adjust to UTC. This can cause the\\n25              time range to expand over a year time period which casuses spurious results. However, these still seems to be some ambiguity around\\n26              how the API is adjusting dates, but the current implementation does capture the requisite data.\\n27            - end date utilizes the requested date. This is in UTC, so we set the hh:mm:ss back 6 hours (back 1 extra hour to be safe):\\n28              passing in 12-31T17:23:59Z will give us results up until 12-31 at 23:23:59 ET\\n29            \"\"\"\\n30            start_date = str(year) + \\'-01-02\\'  + \\'T00:00:00Z\\'\\n31            if datetime .strptime (str(year)+ \\'-12-31\\' , \\'%Y-%m-%d\\' ) < end_dt:\\n32                end_date = str(year) + \\'-12-31\\'  + \\'T17:23:59Z\\'',\n",
       "  '122\\nTroubleshooting\\nThis page was created to keep track of the problems we have encountered and their resolutions.\\nWe have documented common errors, common warnings, and any infrastructure issues.',\n",
       "  '476\\nCard Report Variances\\nTracking\\nThe following DOMO cards reflect the variance that occurs in perpay_accounting_datamart_ext.card_daily_rollforward\\nCredit Card Variance Tracking by Day\\nCredit Card Cumulative Variances over Time\\nOverview\\nCOMPLETED variance issues are marked in green. \\nBLOCKED variance issues are marked in red and imply DE is waiting on a response from another team or an action from another team to \\nmove forward.\\nIN PROGRESS variance issues are marked yellow, and this means DE is actively investigating the variance or coding the solution.\\nIN DE REVIEW labeled variances are ones with a PR up for a fix. This means the update is ready, and it’s waiting to be reviewed before the \\nchanges are deployed.\\nNOT STARTED variance issues are in gray; these have been identified/DE may have a hunch as to what’s going on but has not picked up the \\nissue for investigation yet.\\n1 COMPLET… Andrew Credit balance refund (link): A customer pays towards their card',\n",
       "  '460',\n",
       "  \"that the variance is likely due to issues related to returned payments. Conversely, if the flag is set to 0, the variance can be attributed to \\nother factors. \\nTo facilitate the differentiation of variances, we've also introduced a variance_with_workarounds field. In instances involving partial \\nreturns, this field will be 0 while the potential_partial_return_ind field is set to 1, indicating that the typical variance can be attributed to \\npartial returns.\\nPayments Records over Multiple Days\\nSome payments are initiated at 23:59:59.9 on one day, and the payment details from public.payment_details are generated just \\nmilliseconds later on the following day.  This causes variances in our table because ending_balance_calc reflects the payment a day later \\nthan ending_balance does on Engineering's side. While this situation is relatively rare (there are only two cases in the current table), it's \\ncrucial to keep it in mind for future instances.\",\n",
       "  '94    invoice_freight   varchar(4000),\\n95    invoice_po_number varchar(4000),\\n96    ecomm_po_number   varchar(4000),\\n97    invoice_sku       varchar(4000),\\n98    ecomm_sku         varchar(4000),\\n99    invoice_date      varchar(4000),\\n100    invoice_due_date  varchar(4000),\\n101    tracking_number   varchar(4000),\\n102    created_ts        timestamp with time zone encode az64,\\n103    tax               varchar(4000)\\n104);',\n",
       "  \"277\\n3random_sample as (\\n4    SELECT distinct\\n5        a .borrower_id ,\\n6        a .deserve_card_account_id\\n7    FROM\\n8        atomic_int .card_key_relations a\\n9        INNER JOIN card_int .card_borrower_history_daily_fact b\\n10            ON a.borrower_id = b.borrower_id\\n11    WHERE\\n12        -- Getting accounts that were active in Quarter\\n13        b .src_dt between [Quarter start dt] and [Quarter end dt]\\n14        AND deserve_card_account_id is not null\\n15    ORDER BY random()\\n16    LIMIT 100\\n17),\\n18-- Collect the total of all completed card payments in Quarter for those borrowers\\n19card_payments as (\\n20    SELECT\\n21        a .borrower_id ,\\n22        b .account_id ,\\n23        SUM(b.payment_amount ) AS payments\\n24    FROM\\n25        random_sample a\\n26            inner join deserve_data_int .deserve_payments_eod_report b on a.deserve_card_account_id = b.account_id\\n27    WHERE\\n28        payment_status in ('COMPLETED' )\\n29        AND sftp_dt between [Quarter start dt] and [Quarter end dt]\",\n",
       "  '419\\nDeserve Clarity Underwriting SFTP Ingestion\\n \\nDeserve Clarity Underwriting Ingestion W alkthrough\\nDeserve Clarity Underwriting Ingestion Walkthrough\\n\\x00\\x00 min \\x00\\x00 min \\x00 views \\x00 views\\x00',\n",
       "  '25\\nDE Group Objectives\\nThese are our group’s objectives for the upcoming year. They are also broken up by quarter.',\n",
       "  '60\\nOption 2: Once the initial test file is approved for continuous reporting with Experian, upon making sure the data is clean, rollout reporting of \\nother tiers.\\nWhich option is chosen depends on risk aversion.\\n--\\nReporting Period:\\nData will be reported on Tuesday, a midweek day to have time surrounding the reporting date to look into any outliers if necessary. The \\nreporting period is weekly, which is the most frequent pay-cycle Perpay supports. Any references to data from the “previous reporting period” \\nwill then be data from the second-to-last completed week (week-end on Saturday). The displacement of a week for reporting is also to give \\ntime for any outliers to be investigated. For example, data reported on Tuesday, Jan 21 2020 will be from Jan 5-11. Certain fields must be \\nreported with monthly values, which will include data from the last completed month.\\nData General Specifications:',\n",
       "  '533\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  _int\\nacc\\nount\\ns_p\\naya\\nble_\\nint\\nman\\nual_\\nimp\\norts\\n_sc\\nhem\\na_in\\nt\\nexp\\neria\\nn_d\\nata_\\nint\\nfivet\\nran_\\nitera\\nble\\nperp\\nay_\\ngen\\neral\\n_dat\\nama\\nrt_in\\nt\\nmon\\nthly\\n_per\\nform\\nanc\\ne_fil\\nes_i\\nnt\\ncore\\n_lap\\nsed\\n_sig\\nnup\\n_cre\\ndits\\n_int\\nacc\\nount\\n_rec\\nonci\\nliati\\non_i\\nnt\\ndepr\\necia\\nted_\\ncard\\n_int\\ntem\\np_d\\nev_i\\nnt\\nshar\\ned_r\\neso\\nurce',\n",
       "  'move the money to another account. \\nThis results in neither the original, nor the second email showing up in the rollforward table.\\nAt other times, this problem can be resolved by RTP.23 \"ach\" public.auto_pay_payment\\n46 \"direct_deposit\" public.achdetailsource_rsn_cd \\n(i.e.content_type_id)source Source Table from Public Schema',\n",
       "  \"the day, any borrower_credits, refunds, or returned_payments that may have affected the balance, and finally ending_balance to reflect \\nwhat the final balance was at the end of the day. We track these on a daily level so accounting can make sure all balances tie out with the \\ncustomer funds that we're observing/tracking. If it helps, I can build out this sheet here (some of the fields are empty because I started with \\nthe card_daily_rollforward table and have switched over to commerce_loan_daily_rollforward this week); we have an overview \\npage here for further context as well\\nResponse: Okay, so if I'm understanding correctly, your main intent here is to tie the accounting out on a per-day level?\\nFollow-up: yes, you got it. \\nThis is also the day the borrower_credits happened\\nSince we're looking on a daily level, the total amount of borrower credits for borrower_id 2479348 was $24.45 on 2023-07-11, which is the\",\n",
       "  \"164\\n10/24/2\\n3https://g\\nithub.co\\nm/Perp\\nay/perp\\nay-\\nairflow/\\ncommit/\\n741ca0\\n8f11ce8\\n8a6a7e\\n421495\\n18ab54\\nc0fc8a8\\n78 10/22/23 Withheld \\nunique_key \\n'8548232_2651\\n582_response' \\nfrom \\nreferral_fact.there's 4 instances \\nof the \\n8548232_2651582_\\nresponse \\nunique_key i the \\nreferral_attribute \\ntable. This is from a \\nduplicate in the \\npublic.referrals\\n_referralrespons\\ne table. Because I \\nknow about this \\nissue and it’s the \\nweekend, I will \\nreach out to \\nengineering \\nmonday. But I don’t \\nwant the error to \\nkeep firing and I \\nwant the dag to \\ncomplete tomorrow \\nmorning, so I’m \\ntaking out currently \\nerroneous \\nborrowers.  https://github.co\\nm/Perpay/perpa\\ny-\\nairflow/commit/\\n5e07f4de413fe\\n2390d9baa123\\n6cff12efed9285\\n5 select * \\nfrom \\npublic.ref\\nerrals_ref\\nerralrespo\\nnse where \\nreferral_i\\nd = \\n8512486 \\nand id = \\n2651582; \\n  11/19/202\\n3Held out a card \\npayment id for \\nthe card select \\n* from \\ncard_int.paym\\nent_fact \\nwhere \\npayment_id = \\n368930;I thinkkkk that two \\nstatuses occurred at\",\n",
       "  'correct justification. Generally, this meant numeric values would be right justified and filled with zeros on the left (formatted with zfill) and \\nstring values would be left-justified with spaces at the end (formatted with ljust). Please stick to this approach if you need to edit or add \\nanything.\\nThe other important thing was to validate the generated NACHA string at the end. This step can always be expanded, but it has held up well \\nso far and we have heard no complaints yet.\\nThe code also implements Slack alerts for cases where we can’t find a user match or when emails map to multiple accounting/routing \\nnumbers. You can find examples of these error messages in the #data_and_accounting channel on Slack.\\nAnother interesting detail is related to the warning at the beginning. NACHA files sent to banks from a single source on a given day require \\nfile IDs starting from A going down the alphabet. Old school, but fine. This meant that we needed to track the number of NACHA files we',\n",
       "  'who fail a test (yet still notifying the failure) from reports will also expedite our processes\\nPlatforms\\nGE was a great idea in theory and has utility to subject data model tables to a series of standard validation tests. However, due to the highly \\nspecific API, inflexibility, slow nature, and dubious support, we will be moving forward with adopting the “unittest” methodology over time. \\nThis method is extremely flexible and would enable us to create highly specific tests extremely rapidly.\\nEven though we will be using the “unittest” library, the tests are not “unittests,” they are data validation tests. We are just taking advantage of \\nthe library’s features.\\nThe down side is that a general understanding of python is required to implement the tests. However, since we are moving in the direction of \\nonly allowing DE to have access to the code base, this is not an issue. Additionally, the tests need to be written with motivation to reduce I/O',\n",
       "  '73            \\'{\"enabled\":\\' || enabled ||\\n74               \\', \"started_card_appliaction\":\\' || started_card_appliaction ||\\n75               \\', \"submitted_card_application\":\\' || submitted_card_application||\\n76               \\', \"approved\":\\' || approved ||\\n77               \\', \"activated\":\\' || activated ||\\n78               \\', \"provisioned\":\\' || provisioned ||\\n79               \\', \"shipped\":\\' || \\'null\\' || -- TBD\\n80               \\'}\\' as card_funnel,\\n81            \\'{\"count\":\\' || coalesce(j.total_transactions_count, 0) ||\\n82               \\', \"amount\":\\' || coalesce(j.total_transactions_amount, 0) ||\\n83               \\'}\\' as lifetime_transactions,\\n84            \\'{\"card_status\":\"\\' || coalesce(q.status, \\'\\') || \\'\"\\' ||\\n85               \\', \"card_status_reason\":\"\\' || coalesce(q.reason, \\'\\') || \\'\"\\' ||\\n86               \\', \"card_funnel\":\\' || card_funnel ||\\n87               \\', \"credit_limit\": \\' || coalesce(f.credit_limit, 0) ||\\n88               \\', \"minimum_payment\": \\' || coalesce(o.minimum_payment, 0) ||',\n",
       "  '460',\n",
       "  \"21            when position('bi_weekly' IN b.pay_cycle_json) > 0 then 'bi_weekly'\\n22            when position('weekly' IN b.pay_cycle_json) > 0 then 'weekly'\\n23            end AS pay_cycle,\\n24        CASE when d.everyone = 'true' then 1\\n25            when e.everyone = 'true' then 1\\n26            when c.verified = 'false' then 1\\n27            else 0 end AS pinwheel_eligible_ind,\\n28        CASE\\n29            -- !!!!!!!!!!!!!!!! Here's where the enabled ts is used\\n30            when (fe.first_card_enabled_ts <= mbf.created or mbf.created is null) then 0\\n31            else 1 end AS marketplace_borrower_at_enabled_ind,\\n32        a.staff_ind\\n33    FROM\\n34        {{card_schema}}.card_user_fact a\\n35        LEFT JOIN {{report_schema}}.borrower_fact mbf on mbf.borrower_id = a.borrower_id\\n36        LEFT JOIN {{atomic_schema}}.platform_job b ON a.borrower_id = b.borrower_id AND b.current_primary_ind = \\n37        LEFT JOIN {{pub_schema}}.company c ON c.id = b.company_id\",\n",
       "  'AWS Data Migration Service (DMS)95\\n           The Source Layer96\\n                Handling Upstream Schema Changes - Considerations104\\n      Coding standards109\\n           SQL Queries110\\n           Python114\\n           Formatting SQL with SQLFluff117',\n",
       "  'refund_card_balanace - withdrawals_completed + deposit_ach_completed + deposit_check_completed + \\ndeposit_collections_completed + deposit_credit_card_completed + deposit_direct_deposit_completed - \\ndeposit_direct_deposit_returned - payment_credit_card_initiated + payment_credit_card_returned + \\npayment_marketplace_canceled - payment_marketplace_completed + payment_marketplace_returned - \\npayment_perpay_plus_completed for the merge cluster.\\nEnding Balance: Calculated as the day’s starting balance added to the merge cluster’s total movement for the given day.',\n",
       "  \"subtractedputting in pr1 - trying to get the table to a \\nbetter base state\\n6 COMPLETED abs commerce_borrower_credits_for_awaiting_payment_\\nnew: don't subtract \\ncommerce_borrower_credits_for_awaiting_payment_newputting in pr1 - trying to get the table to a \\nbetter base state\\n7 COMPLETED abs card_payment_returns: should subtract \\ncard_payment_returns from the balanceputting in pr1 - trying to get the table to a \\nbetter base state\\n8 COMPLETED abs commerce_loan_payments_for_canceled: add \\ncanceled loan paymentsputting in pr1 - trying to get the table to a \\nbetter base state\\n9 COMPLETED \\nremoved \\nfrom eqtn\\nIN PROGRE… \\ncomplete \\nunderstand\\ningabs/andr\\newcommerce_cash_refunds: don't include \\ncommerce_cash_refunds- when should these be taken \\ninto account?putting in pr1 - trying to get the table to a \\nbetter base state\\nfollow-up questions put in channel & \\nwaiting for response\\n10 COMPLETED abs/erne\\nst withdrawals_canceled: should be added to balance committed straight to master bc such a\",\n",
       "  'went to 0. Otherwise, null \\n(open account). \\nDate of Last \\nPayment\\n date_last_pay\\nmentMost recent date a payment was \\nreceived, full or partial.If the account is charged off \\nand the borrower most \\nrecently paid True Accord \\n(after paying us), then the last \\npayment date to TA. \\nOtherwise, the maximum not-\\nnull last_pmt_dt for the \\nborrower.borrower_hist_\\ndaily_status.las\\nt_pmt_dt\\nInterest Type \\nIndicator\\n interest_type_i\\nndicatorF = Fixed, V = Variable/Adjustable \\ninterest.Blank  \\nReserved\\n  reserved2 Blank fill   \\nSurname\\n  surname Last name, cleaned for generation \\ncodes and suffixes. user_attribute.l\\nast_name\\nFirst Name\\n  first_name First name, cleaned for titles and \\ngeneration codes. user_attribute.fi\\nrst_name\\nMiddle Name\\n  middle_name Middle name Blank',\n",
       "  'history tables we built before introducing django-simple-history – these will not follow the same format/specifications.\\n \\nHistorical tables will include columns from the original table that we are interested in tracking, as well as some other default columns:\\nhistory_id: primary key for the historical table (not to be confused with id, which is the primary key from the table being tracked)\\nhistory_date: timestamp for when the creation, update, or deletion occurred\\nhistory_type: specifies if a record was created, updated, or deleted. Created will be indicated with +, updated will be indicated with ~, \\nand deleted will be indicated with -\\nhistory_change_reason and history_user_id are also default fields, but are not currently being used.\\n \\nUpon introducing a new history table, it is filled with records indicating the current state of the base table.\\nList of existing historical tables and the fields that are being tracked on them:\\nwaffle_switch_history:\\nname\\nactive\\nwaffle_flag_history:',\n",
       "  'them or update them incorrectly, causing the parser to capture incorrect information. This leads to the underpayment of bills and \\nwork on DE’s end to fix broken DAGs. Our invoice matching v2 DAG also checks that Docparser pulls in very specific fields, which \\ndoes not need to be the case for every vendor. Thus, the removal of this will be a lot less touchy. I will obtain an updated list of all \\nrequired fields from Preston, which we can check for downstream in the code before pushing the data to QB. \\nSimilar to CSVs, we can utilize Zapier for all attachments into S3, including the PDFs.\\nAmazon Textract uses ML to extract data from documents and forms. One of its specialties is analyzing expense documents like \\nInvoices (check out this invoice extraction example using their dummy invoice- you can also upload your own to see how it’ll parse). \\nI want to verify all our current invoices look good through their test parser, but if that’s the case and it captures everything we need, I',\n",
       "  '132\\nHere’s how we’ll proceed for this specific case, which will be similar for most other cases (an address in a name field):\\ncopy the address (POMPANO BEACH FL 33069)\\nlook into perpay_accounting_datamart_ext.docparser_invoices_master where vendor name is that address\\ngrab its PO (invoice number)\\ncreate temp_int.docparser_invoices_master where the po != the po we collected\\ncheck that the internal table exists before the next step\\nneed to do this on prod admin\\nalso save the contents of perpay_accounting_datamart_ext.docparser_invoices_master to your users schema for safety!\\nthen go to airflow → utils_spectrum DAG to create the ext table from the temp_int table\\ncopy the parameters to create the table\\ndon’t forget to change the table name (experian_borrowers) to the table you want (docparser_invoices_master)!!!!\\nand don’t forget to change the schema name (to perpay_accounting_datamart_ext)\\ntrigger the dag with the config you choose',\n",
       "  '-> exits if there is no data in the sheet (empty sheet = there’s nothing to do!)\\n4: Logs \"[InvoiceMatching] Adding ready_for_qb=Y from sheet into QB table, ready_for_qb=N to pending \\nsheet...\"\\n5: Takes the data from last run’s external table (perpay_accounting_datamart_ext.invoicing_new_quickbooks_entries), \\ncopies it to a temp table (temp_int.invoicing_new_quickbooks_entries). Then appends new data (only those where \\nready_for_qb was \\'Y\\') (from temp_int.invoicing_level_match_reviewed, the table we just pulled from the sheet) to the \\ntemp table. Then saves the final result back to the external table again, \\nperpay_accounting_datamart_ext.invoicing_new_quickbooks_entries(which overrides the initial table). \\nRemember- nothing up to this point has been deleted in the sheets\\n6: Append the NOs to the pending df, then write it back to the Pending sheet in the Invoice Matching Google sheet:',\n",
       "  '259\\nPerpay Plus Feature Enrollment Cancellation Requests\\nBackup resource: JD\\nThere will be Trello cards created by Ops of users requesting to cancel Perpay Plus. Most of these will be users who opted in to Perpay Plus \\non accident; we want to keep the option to opt back into Perpay Plus open if the user has not been already reported. There are two courses \\nof action:\\n1. If a user is already reported, we must cancel the enrollment.\\n2. If a user has not been reported, we can delete the enrollment as if it never happened.\\nEngineering will ask data engineering to check if the user is reported. The action here is to run the query below. If there is data returned, \\nadvise action 1) (enrollment cancellation). If not, advise action 2) (deletion of enrollment).\\n \\n1/*** SEE ALL DATA BY EMAIL ***/\\n2select a.*, b.email\\n3from perpay_risk_datamart_ext .equifax_raw_data as a\\n4left join perpay_general_datamart_int .user_attribute as b on b.borrower_id = a.consumer_account_number',\n",
       "  'Completed Direct Deposits: A completed direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of completed_accounting. The amount of the row is applied as a deposit to the associated \\nborrower_id on the date of the created_ts timestamp. The completed_accounting status comes before the completed timestamp \\nthat is used in engineering tie-out. The completed_accounting timestamp more closely represents when Perpay actually receives the \\nmoney, whereas the completed timestamp more closely represents when Perpay can attribute the deposit to a user.\\nReturned Direct Deposits: A rejected direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of returned_accounting. The amount of the row is deducted from the core balance of the associated \\nborrower_id on the date of the created_ts timestamp.\\nPayment Tracking',\n",
       "  '142',\n",
       "  '580\\nRoadmap for Completion\\nThis page serves as a living document for completing the full roll out of the automated AP process. The sections describe the questions and \\nprocedures that still need to be fleshed out.\\nThe overall plan is to push Almo all the way through to act as a POC for the entire (non-csv) process. \\nIn Progress\\nCommerce has been informed that we need to know what level of variance is acceptable between out books and the vendors - we await \\ntheir reply\\nSylvia has reached out to Alec to determine what file format is easiest for him so that we can instruct the vendors to be more uniform. It \\nis assumed that CSV would be easier for us in this process due to the more standerd format compared to a PDF\\nNext Steps\\nNeed to incorporate pagination into how the API response is being handed from Docparser so that we can be sure all data would be \\nmaking it though to Redshift',\n",
       "  '240\\nHere’s an example PR for reference.\\nRegardless of CSV or PDF……\\nupdate the constants.py file within the AP Revamp folder to include the name of the vendor as seen in QB. This is used in the \\naccounts_payable_int.qb_bills query to gather all vendor names from QB.\\n6.2: Testing new code\\nTest on stagginggg!!!\\nIf accounting performed steps 1-5, do a quick check to make sure the zaps and quickbooks vendor are set up correctly (mistakes \\nhappen, just double check so the process runs smoothly), and you are ready to test! Refer to the section above for staging tips.\\nMake sure that you add the new vendor name to Sandbox QB and check that the new invoice made it into the po_matches or \\npo_variances table (either into Sandbox QB or into the Manual PO Matching sheet).\\nRefer to this link for instructions on how to access Sandbox QB.\\n7: Update the vendors sheet\\nUpdate the Vendor Invoice Types and Terms Google Sheet in the AP Folder in the DE Drive.\\n8: Loop back to the accounting page',\n",
       "  'recommendations.\\nhttps://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/Databricks \\nDelta LakeAbility to SQL query the Data Lake\\nHighly performantCannot be a source connected to Domo, so would need \\nseparate data warehouse\\nCannot be a source for DMS (or other popular CDC frameworks \\nfor that matter), so creating a staging environment and putting \\ndata into the above mentioned data warehouse would require an \\nextra intermediate storage\\nCannot be a destination for DMS, would require using another \\nCDC framework\\nCannot be a destination for Segment\\nSnowflake Highly performant\\nCan function as both data warehouse and \\ndata lakeCannot be a source for DMS (or other popular CDC frameworks \\nfor that matter), so creating a staging environment and putting \\ndata into the above mentioned data warehouse would require an \\nextra intermediate storage\\nCannot be a destination for DMS, would require using another \\nCDC framework\\nAWS S3 /',\n",
       "  '-> exits if there is no data in the sheet (empty sheet = there’s nothing to do!)\\n4: Logs \"[InvoiceMatching] Adding ready_for_qb=Y from sheet into QB table, ready_for_qb=N to pending \\nsheet...\"\\n5: Takes the data from last run’s external table (perpay_accounting_datamart_ext.invoicing_new_quickbooks_entries), \\ncopies it to a temp table (temp_int.invoicing_new_quickbooks_entries). Then appends new data (only those where \\nready_for_qb was \\'Y\\') (from temp_int.invoicing_level_match_reviewed, the table we just pulled from the sheet) to the \\ntemp table. Then saves the final result back to the external table again, \\nperpay_accounting_datamart_ext.invoicing_new_quickbooks_entries(which overrides the initial table). \\nRemember- nothing up to this point has been deleted in the sheets\\n6: Append the NOs to the pending df, then write it back to the Pending sheet in the Invoice Matching Google sheet:',\n",
       "  'than the drop and create process previously described. This approach is much more efficient for compute resources and allows the data to \\nbe on a synchronized level, eliminating timing issues.\\nThis approach is implemented in the current metric layer in the account_reconciliation_int schema and provides a synchronized table that is \\nnot prone to timing issues between layers. However, the materialized view has its shortcomings in the context of our architecture. In \\ncases where the public layer has a change that disrupts its overall numerical consistency, engineers have to delete and reinitialize the',\n",
       "  \"440\\nSo, the file card_activation_events will need to be updated for cards in the wild ❌ . \\nCard Key Relations\\nThis table will be affected. We get the enabled borrowers in a CTE and then use it to set our population. This affects the population of \\ndownstream data models as well such as Card Borrower Fact (which in turn affects all of Card Borrower DM). Here is a shortened version of \\nthe code with the affected areas marked with comments: \\n6    -- !!!!!!!!!!!!!! Anything here gets an enabled status\\n7    'enabled' as event \\n8  from {{pub_schema}}.feature_enrollment a\\n9  left join {{metric_schema}}.card_key_relations b \\n10    on a.borrower_id = b.borrower_id\\n11  where a.feature_id = 67 and a.status = 'enabled'\\n12),\\n13...\\n14activation_events as (\\n15  -- !!!!!!!!!!!!!! Unions with other events here\\n16  select user_id, created_ts, event from card_enabled\\n17    UNION\\n18  select user_id, created_ts, event from deserve_events\\n19    UNION\",\n",
       "  'visits across the two tables have a discrepancy of a few minutes)\\nUpdate: We looked into this issue more closely -- most of the overlapping URLs between Web/Storefront correspond to the paths / and \\n/dashboard (which are known to be shared). If we disregard those, then there are only ~60 overlapping rows, so we think the impact on \\nthe device usage data should be relatively limited. \\n6/card\\n7/orders\\n8/dashboard/\\n9/referrals/broseman4',\n",
       "  '(38,2)Credit card payments that have hit an \\ninitiated status, resulting in the amount \\nbeing deducted from the user’s core \\naccount balancePayment \\nTracking\\npayment_credit_card_returned Numeric\\n(38,2)Credit card payments that have hit a \\nreturned status, resulting in the \\namount being added back to the \\nuser’s core account balancePayment \\nTracking\\npayment_marketplace_completed Numeric\\n(38,2)Marketplace payments that have been \\ncompleted, resulting in the amount \\nbeing deducted from the user’s core \\naccount balancePayment \\nTracking\\npayment_marketplace_returned Numeric\\n(38,2)Marketplace payments that have been \\nreturned, resulting in the amount being \\nadded back to the user’s core account \\nbalancePayment \\nTracking\\npayment_marketplace_canceled Numeric\\n(38,2)Marketplace payments on loans that \\nhave hit a canceled status on this day, \\nresulting in the payments made up to \\nthis point being added back to the \\nuser’s core account balancePayment \\nTracking\\npayment_perpay_plus_completed Numeric',\n",
       "  \"57\\nTechnical Process\\n \\nTables and Sheets Involved\\nGoogle Sheets\\n \\nRedShift Tables\\nQuickBooks Bills Sheet1 Bulk loaded data every \\nfew weeksManual Manually added to \\nevery few weeks\\nQuickBooks Bills NewEntries Proposed new entries to \\nbe copied into \\nQuickBooksAutomatic Appended to each \\nrun\\nNon Parsed Partners […All Partners…] Raw invoice data from \\nall partners that don’t \\nsend PDF attachments.Manually Manually added to, \\nas often as possible\\nNon Parsed Partners VendorKeyName Matching across vendor \\nnames that appear in \\ninvoices.Manual Manually added to \\nwhen needed\\nInvoice Matching DoInvoiceMatching Proposed invoices to be \\nreviewed before adding \\nto QuickBooksAutomatic Each run\\nInvoice Matching Pending Invoices with \\nready_for_qb-'N' still \\nbeing reviewedAutomatic Each run\\nParser Output Sheet1 Docparser populated this \\nwhen it parses invoice \\nattachments.Automatic Docparser is \\nconstantly adding to \\nit.Workbook Name Sheet Name Description Manually or Auto \\npopulated?When is it\",\n",
       "  '104\\nHandling Upstream Schema Changes - Considerations\\nGoal: We need a lightweight solution to process upstream changes to raw data from the public schema, ensuring numerical \\ncuration/standardization. This solution should seamlessly propagate changes to downstream layers in response to any unexpected \\nchanges in the public schema, eliminating the need for manual intervention.\\nUnexpected Changes and Accountability\\nCan there be better systems in place to better anticipate changes from our sources before we run into errors?\\nInternal departments (engineering) - what kind of changes do they make? \\nExternal companies (deserve, sources through fivetran, etc) - if they are predictable, what changes do we know they make? If \\nunpredictable, can we have a system in place to ask them to document changes they make as they make them?\\nIf we can expect it via it being communicated, we could adjust our tests or DMS task. The idea is to get ahead of changes wherever we can,',\n",
       "  'runs. \\nExtensions\\nWhile we have a very minimal Travis file, it is quite easy to build on top of by adding test files or scripting. Travis has many capabilities that \\nwe could choose to play around with as well, such as using Docker within it or notifying via Slack. We can add further Airflow testing, such \\nas DAG unit testing, and automate it. We could also choose to only test modified files from the current PR, which could decrease build time. \\nWith flake8, we can add custom rules within our code if we’d like to make our syntax errors Airflow specific. This would require adding a',\n",
       "  \"User’s statement: 3 transactions, total: $117.68\\n1select count(*) as cnt, transaction_id\\n2from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report\\n3group by 2 having cnt > 1;\\n1select *\\n2from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report\\n3where transaction_id = '0c5df619-62c7-584e-978a-a229cb0bbd4d';\\n4\\n5select *\\n6from perpay_accounting_datamart_ext.deserve_cleared_transactions_eod_report\\n7where transaction_id = '0c5df619-62c7-584e-978a-a229cb0bbd4d';\\n8\\n9select *\\n10from public.card_transaction_snapshot\\n11where card_transaction_id = 126915;\\n1select  *\\n2from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report\\n3where transaction_id = '2eb0528e-e022-566e-a25b-a36ebb5382e4';\\n4\\n1select *\\n2from public.card_transaction\\n3where id = 20371;\\n4\",\n",
       "  '26\\nwip - 2023\\ndocument contains a draft of how DE will progress through 2023 by topic\\nKTLO Week\\nOne week (maybe 2) allocated for no outside work, only maintenance and updates\\nJira project tracking\\neach discipline has its own board - need the flexibility for specificity\\nswimlane for assignment so manager knows who is working on what\\nonly fully detailed cards are added\\npossible swimlanes/labels denoting long/short term projects\\nProject prioritization\\n2 week work periods\\nMonday week 1 is set the tasks for the 2 weeks\\nMonday week 2 is for updates\\nFriday week 2 discuss what was completed / postmortems if necessary or actionable\\nClear tasks with goals - cannot have “just make progress” \\nEx: acct rec is said to be completed by end of feb - this is really difficult for prioritization and need to break down into descrete sub \\ntasks\\nEach project should be evaluate against its return and how probably/risky it is to be de-prioritized\\nEvery work period should include at least 1 infa task/project',\n",
       "  'companies and payroll providers moving on and off the pinwheel eligibility list. Our assumption is that these tables represent an effort to \\nstore historical Pinwheel data prior to the introduction of waffle_flag and waffle_flag_history. It is unknown where this effort was ever \\ncompleted, or given up on without ever collecting clean data. \\nThe pinwheel_company_legacy and pinwheel_payroll_provider_legacy tables contain records from as early as Jul 2020 and as late \\nas Jun 2021. We began directing customers to Pinwheel in Jul 2020 and the earliest records from waffle_flag and \\nwaffle_flag_history are from Jun 2021. So it is possible that the two archived tables could contain the complete history from the time we \\nbegan using Pinwheel to the time that the data began to be handled by the waffle flag tables. However, it is also likely that this is not the \\ncase as both pinwheel_company_legacy and pinwheel_payroll_provider_legacy contain records indicating that companies and',\n",
       "  'based testing) \\nThe current organization allows for data mart tables (ex user_attribute) to be available to anyone who has access to the particular datamart. \\nThese users are outside of the data engineering group and therefore can mistakenly use incorrect data which could force unwanted \\noutcomes. The updated format would pull our testing layer back and therefore anything that would be added to the datamarts would also be \\nabsolutely validated against our testing suite. Additionally, as with the metric layer testing, could be logged and removed so the entire \\nprocess continue and the issues could be triaged asynchronouslyRobot test cases are easily \\nreadable\\nrich built in libraries make it \\neasy to useinadequate in parallel \\ntesting (no built in feature) It is suggested to use a different \\noption if you want to develop a \\ncomplex automation framework \\nbecause other options involve \\nPython code\\nPytest open source, easy to learn\\ntesting framework in Python\\nwrite test suites in a',\n",
       "  '352\\nSegment',\n",
       "  \"Depending on which column the error was caught on (clicks, impressions, reach), that’s the table you look into.\\nfor instance, if it’s impressions:\\n35         left join perpay_marketing_datamart_ext .iterable_daily_insights as b on b.campaign_id = a.campaign_id a\\n36where a.platform = 'Iterable' ;\\n37\\n38select\\n39    a.join_key ,\\n40    a.src_dt,\\n41    e.join_key ,\\n42    e.src_dt,\\n43    e.campaign_id ,\\n44    e.adset_id ,\\n45    e.ad_id\\n46    count(*) as cnt\\n47from perpay_general_datamart_int .marketing_daily_fact a\\n48         inner join temp_dev_int .marketing_daily_source_metrics_iterable_2 e on a.platform = e.platform and a.jo\\n49group by 1,2,3,4,5,6\\n50having cnt > 1;\\n1select *\\n2from perpay_marketing_datamart_ext .iterable_daily_insights\\n3where ad_id = <the ad id > and adset_id = <the adset id > and action_date = <the action date>\\n1--- table 1\\n2select *\\n3from perpay_marketing_datamart_ext .iterable_daily_insights\\n4where ad_id = 7430126 and adset_id = 5496568 and action_date = '2022-12-05'\",\n",
       "  \"437\\nAdjusting for Cards in the Wild\\nWe are going to be moving to cards in the wild soon. This means we won't be able to rely on enablements to set our populations. This page \\nprofiles what this impact will be.\\n \\nFiles\\nCredit Reporting\\nCard Reports\\nMarketplace Reports\\nExperian Pull Data\\nMetric Layer Files\\nCard Activation Events\\nCard Key Relations\\nCard RDD Config\\nPlatform Payments\\nCard Data Models\\nCard User Fact\\nCard User Detail\\nCard Borrower History Daily Fact\\nCard Borrower Account Detail\\nCard Borrower Performance\\nMarketplace Data Models\\nBorrower Attribute\\nLoan Risk Factors Hist\\nPlus Borrower Attribute\\nPlus Borrower Fact\\nPlus Borrower Status History\\nPlus Funnel Fact\\nUser Attribute\\nDaily Summary Plus\\nRisk Data Model\\nRisk Appetite Platform User Fact\\nIterable Send\\nThe Iterable Customer Data Table / Generate Data Task\\nAccount Rec\\nAccount Balance\\nMonthly V antage Scores\\nMonthly V antage Scores\\nNew Test Monthly V antage Scores\\nTest Monthly V antage Scores\\nOthers\\nFS Daily Perpay Plus Revenue\",\n",
       "  '354\\nThe Effect Of Overlapping Events Across Tables on Marketing (Iterable) Data\\nThis topic was brought up in this PR that deals with device usage. Upon investigation, we learned that the overlap of events is extremely \\nsmall, and will not significantly impact marketing metrics. For further context, this was the conversation had about the topic:\\nQuestion:\\nWe\\'re seeing some URLs appear across the Segment tables for both the web and the storefront for the same users around the same time. \\nOur understanding is that the Web & Storefront Segment tables are meant to cover separate sets of URLs -- Engineering has confirmed \\nthis, although they say there may be cases where one of the URLs is invalid and shows up as \"Page not found\" in the UI. For \\nexample, shop.perpay.com/orders is a valid Storefront URL, whereas app.perpay.com/orders is not, but the /orders path appears in',\n",
       "  \"511\\nEng Follow-Up Question: How many cases of this are there?\\nDE Response: 28 doubled_returned_payments.csv \\nResponse: Most of those are manual errors. I couldn’t figure out the issue with 6 of them, but we will clean them all.\\nVariance 4: Uncleared Balance Histories\\nQuestion: hey! sending two examples where the payments, borrower credits, refunds, & returned payments all are being tracked as 0, but \\nyou guys have that the ending balance is lower than the starting balance - can someone assist in digging into what's missing here or why \\nthat is? thanks!\\nvariance_4.csv \\nResponse: I know this email, this is one of our most VIP users \\n , will take a look!\\nFollow-Up R: This was a manual error- It’s fixed now.\\nFollow-up Q: Looks like a similar issue is happening for ~1,700 other borrowers in our table. Maybe not exactly the same case as \\nwindaperez1@yahoo.com, but here are 10 more examples where the ending balance changes and we're not tracking a reason as to why\\nvaraince_4_continued.csv\",\n",
       "  'Only for the first prod deployment: When deploying to prod for the first time, we will need to prepare ALL secrets. This won’t be \\nnecessary for smaller future redeployment for updates and the like. In those cases, pay close attention to whether there are one or \\ntwo secrets that might need to be updated. Currently, the necessary secrets are the following:\\nDATAHUB_SYSTEM_CLIENT_ID for the datahub-actions container (de-datahub-system-client-secrets)\\nDATAHUB_SYSTEM_CLIENT_SECRET for the datahub-actions container (de-datahub-system-client-secrets)\\nDATAHUB_SECRET for the datahub-frontend-react container (de-datahub-play-secret)\\nThe ARN references to these in the container definitions will need to be updated since AWS Secrets Manager adds a short random \\nID to the end of the ARN labels. No good way to automate this one unfortunately.\\nThe following steps are necessary for the first prod deployment and can be skipped for later deployment when these parameters \\nhave already been set.',\n",
       "  'credit reporting where we care about a user’s activity status.\\nFinally, we combine the healthy healthy clusters and healthy cyclic clusters for our final flattened merged user table \\n(atomic_int.platform_flattened_merged_users) after excluding cases with missing user IDs, and produce three more tables to assist \\nengineering in debugging: a table with all unhealthy merge clusters, a table with all events for the unhealthy merge clusters, and a table with \\nall merge clusters with missing user IDs.\\nData Models\\natomic_int.platform_flattened_merged_users\\nshared_resources_int.unhealthy_merge_clusterscluster_id BIGINT Unique identifier for the cluster.\\nmerged_from_email VARCHAR Any email that exists in the cluster \\nincluding the lead node to ensure \\neasy joins.\\nmerged_to_email VARCHAR The lead node of the cluster, \\nshould be same value for any \\ngiven cluster_id.\\nmerged_from_user_id VARCHAR (should be INTEGER)user_id corresponding to the \\nmerged_from_email.',\n",
       "  \"a.  Example: let’s say we want to look at DMS costs on the Production Account in the month of November, 2022. \\nb. Filter on LinkedAccountName = ‘Production Account’ and ProductCode = ‘AWSDatabaseMigrationSvc’. Sum the TotalCost field \\nto get the total cost for this service. \\n \\nOther notes:\\nThese bills spreadsheets have been manually loaded into the users_frank schema (from 01/2022 - 11/2022) for more programatic \\nanalysis. A further, more automated iteration of this could include programmatically loading them into the accounting schema, rather than \\nmanually doing this. \\nRelevant queries run to produce the data for the November MRNA:\\n1with remove_totals as (\\n2select *\\n3from users_frank.aws_bills_11\\n4where invoiceid is not null)\\n5select\\n6  productname,    \\n7  sum(totalcost) as product_cost\\n8from remove_totals\\n9where (linkedaccountname = 'Production Account' or linkedaccountname = 'Staging Account')\",\n",
       "  \"4: Click add permissions > attach policies\\nattach the following policies:\\nAWSLambdaExecute (provides access to S3 bucket)\\nAmazonTextractFullAccess (gives Textract full access. It's a better idea to create a custom policy and then attach that policy within this \\nIAM role, so probably do that instead)\\n5: Add the s3 trigger- click add trigger, and configure it to S3 with the correct bucket\\nHere, event trigger will be all object create events\\n6: Add a description (optional) → Configuration > General configuration > Edit\\nexample for unzip-invoice-lambda-function: Retrieves .zip files from the accounts payable bucket under zipped_pdfs/ and unzips \\nthem as .pdfs under unzipped_pdfs/.\\nexample for parse-invoice-lambda-function: Sends .pdfs from accounts payable bucket folder unzipped_pdfs/ to AWS Textract. \\nParses the returned JSON and creates .csvs in parsed_outputs/.\\n \\nNext, we'll want to update the source code of the lambda function to analyze the information from the API. For this,\",\n",
       "  \"Step 8: There is lots of room for improvement here. \\n1create external schema segment_logs\\n2from data catalog\\n3database 'segment-logs'\\n4iam_role 'arn:aws:iam::606618693306:role/ana-stage-redshift-role'; -- insert prod role for production environment\",\n",
       "  '168\\nTesting & Validation\\nThis page contains the outline of DE’s long term goals of data testing and validation. The current methodology got us very far but with our \\nincreased volume, scope, and scrutiny, we need a faster and more flexible methodology in place. \\nTR;DL: We are moving to utilizing the python unittest library for our data validation which should scale for the duration of our time using \\nRedshift, as long as tests are written appropriately. As we are rebuilding and refactoring, a more nuanced version of tests will be added to \\nkeep every single test failure from always causing an entire DAG to fail when that may not be necessary and a simple notification would \\nsuffice. Lastly, in addition to adding notifications and adhering to our policy of stale > incorrect, incorporating the ability to remove cohorts \\nwho fail a test (yet still notifying the failure) from reports will also expedite our processes\\nPlatforms',\n",
       "  \"example, shop.perpay.com/orders is a valid Storefront URL, whereas app.perpay.com/orders is not, but the /orders path appears in \\nboth the Storefront and Web tables for the same user. This can happen if the user manually types in the URL app.perpay.com/orders in \\ntheir browser, sees that the URL is invalid, and subsequently switches to shop.perpay.com/orders .\\nAnother instance where this might happen is when the user tries to click the back button in their browser to transition from a storefront page \\nto a web page (or vice versa) -- Segment has some issues resolving the proper page when the user clicks the back button, so that's why a \\nweb URL might show up in the storefront table or vice versa.\\nWhen we compute users' device usage metrics, we're still including the visit to the Invalid /orders page (and tracking which device they \\nvisited the invalid page on). We're wondering if this is still OK for Marketing?\\nAnswer:\",\n",
       "  'Tracking\\ncharge_off_settlement Numeric(\\n38,2)Sum of transactions of type \\nCHARGE_OFF_SETTLEMENT for the card \\naccount on the observation dateTransaction \\nTracking\\ncourtesy_credit Numeric(\\n38,2)Sum of transactions of type \\nCOURTESY_CREDIT for the card account \\non the observation dateTransaction \\nTracking\\nprincipal_debit_adjustment Numeric(\\n38,2)Sum of transactions of type \\nPRINCIPAL_DEBIT_ADJUSTMENT for the \\ncard account on the observation dateTransaction \\nTracking\\nmerchant_cash_back Numeric(Mastercard rewards for using the card with Rewards',\n",
       "  '39\\nAdditionally we should see cost benefit as resources would only be dedicated to Airflow as needed. The current setup requires an \\nEc2 that can handle max load at all times. Notice that this resource is being underutilized (i.e. over payed for)\\nWe should start using AWS Secrets for management of secrets and configurations. Benefits are\\nBetter security (described in article linked at top of page)\\nEasier configuration via terraform integration (that is we can set secret values with terraform and automatically update Airflow \\nvariables with infrastructure changes deployed by terraform)\\nRemoves error prone process of manually updating a 1pass file\\nI believe this is the default if we use AWS Managed Airflow\\nWorkflow Problems\\nNo functional testing! That is there are (very few) tests that check the logic of queries and business logic.\\nAnalysts are required to leverage docker for contributing to the datamart\\nLarge lift for testing can cause testing to be circumvented',\n",
       "  'When rounding decimals, don’t callROUND. Instead, cast the decimal column so that it has a NUMERIC datatype (and specify the relevant no. of decimal places in the NUMERIC type).\\nWe’ve previously had issues with ROUND not behaving well with SELECT DISTINCT queries – decimals which visually appear the same are actually under the hood different values, since ROUND only \\nchanges how the decimal is displayed but doesn’t actually change the underlying data representation. This issue was fixed by using CAST(... AS NUMERIC) instead, so we recommend using this \\napproach moving forward.\\n \\nThe order of execution for a query\\nThe earlier you can filter data out, the better! \\n1-- NOT PREFERRED ❌\\n2SELECT\\n3  abracadabra .id,\\n4  abracadabra .date,\\n5  bananas .flavor,\\n6  bananas .color\\n7FROM \\n8  abracadabra\\n9  LEFT JOIN bananas ON abracadabra .primary_key = bananas .foreign_key\\n10\\n11-- PREFERRED✅\\n12SELECT\\n13  a.id,\\n14  a.date,\\n15  b.flavor,\\n16  b.color\\n17FROM \\n18  abracadabra a',\n",
       "  '315\\nclearing: a series of checks which are ran to make sure the transaction passes inspection.\\nThis happens before settlement.\\nsettlement: the process of moving funds from the cardholder’s account to the merchant’s account.\\nThis happens after clearing.',\n",
       "  '544',\n",
       "  'snapshot was taken\\nentity_type string “user” or “group”\\nentity_id integer user_id or group_id\\nread_ind integer 1 if the entity has a read \\npermission, 0 otherwise\\nwrite_ind integer 1 if the entity has a write \\npermission, 0 otherwise\\ndelete_ind integer 1 if the entity has a delete \\npermission, 0 otherwise\\nshare_ind integer 1 if the entity has a share \\npermission, 0 otherwise\\nadmin_ind integer 1 if the entity has admin \\npermissions, 0 otherwiseVariable Name: Data Type: Description:',\n",
       "  '136\\nSolution\\nClear and rerun the CardPaymentStatusStatus task!\\nFollow up\\nCheck that the Test_CardPaymentStatus task that had initially failed now runs to completion.',\n",
       "  \"89\\nReal-time Alerting\\nWith Datadog, we can set up alerts based on custom thresholds and conditions. When critical metrics hit our predefined limits, Datadog will \\nSlack us in the #airflow-prod-infrastructure-alerts channel. This ensures proactive monitoring and quick response to potential \\nissues.\\nEnhanced Troubleshooting\\nDatadog offers pre-built dashboards and the ability to create dashboards from scratch to enable efficient troubleshooting. By visualizing the \\nmetrics, logs, and traces together, we can quickly identify the root cause of performance issues or errors in our AWS environment.\\nCost Optimization\\nDatadog's integration with AWS provides visibility into resource utilization, enabling us to optimize costs effectively. By analyzing metrics like \\nCPU usage, network traffic, and storage utilization, we can identify underutilized resources, right-size instances, and optimize our AWS \\nspend.\\n4. Costs\",\n",
       "  '2select count(*) from accounts_payable_int.po_matches;\\n1drop table if exists accounts_payable_int.po_matches;\\n2create table accounts_payable_int.po_matches as select * from users_rehmet.po_matches;\\nif you don’t rerun the Quickbooks Upload Task and instead opt to rerun the entire DAG, reset the \\ntemp_marketplace_int.po_matches table. This happens in the Update_QB_History_Table task, downstream of \\nQuickbooks_Upload, so if you were to skip that task before restarting the entire process, it’ll result in the same error as before.\\n[PROD] ERROR has occured!\\nDAG: accounts_payable_revamp\\nTask: Quickbooks_Upload\\nError: [QuickbooksApi/get_vendor_id] Vendor that was requested, Milor Group, does not exist in Quickbooks. Please add the vendor to \\nQuickbooks and restart the job...',\n",
       "  \"407\\nKnown Deserve Incidents\\nNovember 2022 - Returned Payments on Balance\\nIn the 2022-11 to 2022-12 timeframe, there was a known issue in which a payment was completed, the balance was properly decremented, \\nbut no balance adjustment occurs upon the payment being reversed. The end result is the user got credit for a payment that was actually \\nreversed. A business decision was made not to adjust the customer balances and instead remediate through our monthly invoice process \\nwith Deserve.\\nVariances caused by this incident have a incident_variance value of November 2022 - Returned Payments on Balance. \\nOctober 2022 - Double Counted Payments\\nOn 2022-10-27, several payments were counted twice towards a customer's balance. The erroneous payments were not reversed, and \\nDeserve instead took the total amount off of our monthly invoice.\\nVariances caused by this incident have a incident_variance value of December 2022 - Double Counted Payments.\",\n",
       "  'voicing alerts/warnings when this occurs). A example is that if in the payments data model a payment comes through that is negative (clearly \\nan error), this payment could be removed from being incorporated into the final report, with the addition of a notification that a particular \\npayment was held out by a test so it can be triaged. With this methodology, the reports can still be updated on schedule with validated data, \\nand the problem-cohort can be rectified without disturbing SLAs. This is not a universal solution because some SLAs require all information',\n",
       "  'refund amount. Subsequently, the account balance is decremented when the loan is \"paid off\" by the refund, which is what failed here.I\\'m \\nnot aware of any field that directly indicates that the refund did not pay down the loan. I\\'ve had to cobble that together from looking at the \\nloan principal balance history and the fact that the current loan principal balance does not equal the following notional equation loan \\nprincipal balance = loan amount - payments - refund paydown + returned payments\\nFrom my reading of the code, when the refund verification checks fail, it looks like we:\\n1. Send arik an email\\n2. Write to our std_out logs, which are ephemeral and are not saved in the database\\nSo there\\'s no sort of log storage or event table data that you can query. The best check I can think of is: looking at the \\ncore_accountbalancehistory table, at the timestamp when the refund was created, was the account balance negative?\\nuncounted_refunds.csv refund_details.csv',\n",
       "  '310\\nTotal credit limit, etc etc should all reflect only the active, i.e. non-closed accounts. This table will likely be useful for credit \\nreporting. In other words, I don’t care if they opened 10 cards at 1,000 lines if only one is still open. Their total credit limit is \\nonly what is currently open → done. changed dependencies to match the field name changes. checked dependencies for \\ntotal_credit_limit now that definition has been updated to only active accounts…. this sparked the creation of Card Gloss\\nary … keep noting definitions on that page as we run into them!\\nPlease audit borrower 2429389 – total credit limit and total available credit don’t make sense.\\nWe should break down what is ‘active’ vs. what is ‘open’. Are they the same thing? → talked with JD: a card account can either \\nbe open or closed. a card can either be active or not active\\nreformatted\\nCard borrower performance\\nConfluence documentation is not correct → fixed\\nTrim to match card_account metrics: → done',\n",
       "  '529\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  cen\\nsus\\ncen\\nsus\\n_so\\nurce\\n_int\\nprod\\nuct_\\nwor\\nksp\\nace\\nfivet\\nran_\\nquic\\nkbo\\noks\\ncred\\nit_re\\nporti\\nng_i\\nnt\\nrefer\\nenc\\ne_ar\\nchiv\\ne_in\\nt\\ntem\\np_in\\nt\\ntem\\np_pl\\natfor\\nm_i\\nnt\\ntem\\np_m\\narke\\ntpla\\nce_i\\nnt\\ntem\\np_c\\nard_\\nint\\ndom\\no_in\\nt\\ncore\\n_lap\\nsed\\n_sig\\nnup\\n_cre\\ndits\\n_int\\neph\\neme\\nral_\\next\\nperp\\nay_\\nml_\\next',\n",
       "  'portfolio. Values available: C = Line of \\nCredit, I = Installment, M = Mortgage, O \\n= Open, R = RevolvingR (revolving)  \\nAccount Type account_type Specific code for account classification. \\nSee exhibit 1 for definitions.07 - charge account.  \\nDate Opened date_opened Date the account was originally opened. \\nThis cannot change for the account, and \\nmust be filled.borrower_fact.created borrower_fact.cr\\neated\\nCredit Limit credit_limit For revolving accounts: Assigned credit \\nlimit. For closed accounts, continue to \\nreport the last assigned credit limit.Spending limit as of end of last \\nweek.borrower_hist_d\\naily_status.spen\\nding_limit\\nHighest Credit or \\nOriginal Loan \\nAmounthighest_credit_o\\nr_originalFor revolving accounts: Highest amount \\nof credit utilized by the consumer.The maximum outstanding \\nbalance on any given day of the \\nborrower to date. \\nTerms Duration terms_duration Contains the duration of the credit \\nextended. For revolving accounts, \\nconstant of REV  \\nTerms',\n",
       "  'align between the two, ~23k of them were within a day of the dates of change in the Pinwheel eligibility metric layer for the company that the \\nuser was from.\\nThe primary reason for this is that the Pinwheel eligibility metric layer often only takes into account the date of changes in a company’s \\nPinwheel eligibility and not the specific time. Having a timestamp would paint a more accurate picture of the history of company’s Pinwheel \\neligibility, however for the majority of data sources that are used to form the historical record, this piece of information is not available. A \\ntimestamp data type is used and includes the data where it is available, but just assigns a time of 00:00:00 where there is only a date \\navailable. The Segment events themselves could also be used to try to approximate a timestamp for certain records in the Pinwheel metric \\nlayer, although there is hesitancy to put any data into a metric layer for which the accuracy cannot be guaranteed.',\n",
       "  '347',\n",
       "  \"- logs '[InvoiceMatchingV2] Executing query to get variances and put in po_invoice_variance master \\ntable...'\\n- writes it to ext schema as perpay_accounting_datamart_ext.po_invoice_variance_master\\n2: Pulls data from Invoice Matching Google sheet (specifically, the Invoice Matching v2 sub sheet) into the \\ntemp_int.invoice_matching_sheet_temp table. \\n- This is where the Invoice Matching v2 sub sheet in the Invoice Matching Google sheet is cleared\\n- If the Invoice Matching sheet had data in it\\n1: Log '[InvoiceMatchingV2] Executing query to update invoice variance master table based on invoice \\nmatching Google Sheet...' \\n2: update the perpay_accounting_datamart_ext.po_invoice_variance_master table that was just created\\n- Join the info from the sheet (from temp_int.invoice_matching_sheet_temp) onto \\nperpay_accounting_datamart_ext.po_invoice_variance_master and save that to temp_int schema as \\ntemp_int.po_invoice_variance_master\",\n",
       "  \"443\\nThe logic for detail will need to change. ❌\\nCard Borrower History Daily Fact\\nSimilar to card user fact and card user detail, we use enrollments to get enabled timestamp for the card, but are not slimming back the \\npopulation. The population is determined by card accounts through deserve_accounts_eod_report. Here’s where setting the enabled ts \\nhappens.\\n8            min(created) as first_card_enabled_ts\\n9        FROM {{pub_schema}}.feature_enrollment\\n10        WHERE feature_id = 67\\n11        GROUP BY borrower_id\\n12    )\\n13    SELECT\\n14        a.user_id,\\n15        c.id AS company_id,\\n16        c.name AS company_name,\\n17        h.rdd_applied_for,\\n18        CASE\\n19            when position('semi_monthly' IN b.pay_cycle_json) > 0 then 'semi_monthly'\\n20            when position('monthly' IN b.pay_cycle_json) > 0 then 'monthly'\\n21            when position('bi_weekly' IN b.pay_cycle_json) > 0 then 'bi_weekly'\\n22            when position('weekly' IN b.pay_cycle_json) > 0 then 'weekly'\",\n",
       "  '107\\ndlt also uses slack webhooks to indicate updates columns, tables or data types.\\nWhat sources and destinations that we use does dlt support?\\nSources\\nRedshift\\nS3\\nRDS DB (Postgres)\\nGoogle Ads\\nFacebook Ads\\nTakeaway: Not enough to try and replace DMS, but we could leverage its capabilities with handling schema evolution in the source layer, \\nsince it would be redshift/s3 to redshift.\\nIncremental Loading\\ndlt offers incremental loading, which can decrease expenses compared a full load on every run. This medium page has a section on how it \\nworks in dlt.\\n1. Append\\nWe can use this for immutable or stateless events (data that doesn’t change), such as taxi rides — For example, every day \\nthere are new rides, and we could load the new ones only instead of the entire history.\\nWe could also use this to load different versions of stateful data, for example for creating a “slowly changing dimension” table for',\n",
       "  'TMENT for the card account on the \\nobservation dateTransaction \\nTracking\\nreturned_payment_fee Numeric(\\n38,2)Sum of transactions of type \\nRETURNED_PAYMENT_FEE for the card \\naccount on the observation dateTransaction \\nTracking\\ninterest_charge Numeric(\\n38,2)Sum of transactions of type \\nINTEREST_CHARGE for the card account \\non the observation dateTransaction \\nTracking\\ninterest_charge_adjustment Numeric(\\n38,2)Sum of transactions of type \\nINTEREST_CHARGE_ADJUSTMENT for the \\ncard account on the observation dateTransaction \\nTracking\\ncash_back Numeric(\\n38,2)Sum of transactions of type \\nBANK_CASHBACK for the card account on \\nthe observation dateTransaction \\nTracking\\ncash_back_adjustment Numeric(\\n38,2)Sum of transactions of type \\nBANK_CASHBACK_ADJUSTMENT for the card \\naccount on the observation dateTransaction \\nTracking\\ncharge_off_settlement Numeric(\\n38,2)Sum of transactions of type \\nCHARGE_OFF_SETTLEMENT for the card \\naccount on the observation dateTransaction \\nTracking\\ncourtesy_credit Numeric(',\n",
       "  '“transaction”. \\nConor said he would keep DE in the loop about the outcome of this conversation.\\nIn card borrower hist daily, we use logic to consider this transaction a fee, not a transaction. \\nThere has since been a product update to apply the user’s first payment (that causes the card to be provisioned) to their card \\noutstanding balance, so users in the future will have paid their account opening fee with their first direct deposit payment.\\nUser activated their card on 07/21\\nThere is nothing in the SFTP files to indicate this\\nUser made their first purchase on 07/24\\nYou can see the user’s current_balance increase on 07/24.\\nUser continues to make purchases on their card, and you can see their current_balance increase as a result. \\nOn 07/31, the user’s first statement information is generated.\\nIt’s unclear why this generated on this day. The user’s first statement ends on 08/10 EST, so it does not correlate with a statement \\nend date.',\n",
       "  '532\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  andr\\nill\\nseg\\nmen\\nt_pe\\nrpay\\n_cor\\ne_pr\\nodu\\nctio\\nn_in\\nt\\nseg\\nmen\\nt_pe\\nrpay\\n_sto\\nrefr\\nont_\\nweb\\n_pro\\nduct\\nion_\\nint\\nseg\\nmen\\nt_pe\\nrpay\\n_sto\\nrefr\\nont_\\nweb\\n_pro\\nduct\\nion_\\ntrac\\nks\\nseg\\nmen\\nt_pe\\nrpay\\n_we\\nb_pr\\nodu\\nctio\\nn_in\\nt\\nseg\\nmen\\nt_pe\\nrpay\\n_we\\nb_pr\\nodu\\nctio\\nn_tr\\nacks\\nfivet\\nran_\\nbran\\nch\\nfivet\\nran_\\ntikto\\nk_a\\nds\\ncard\\n_int\\nato\\nmic',\n",
       "  \"30      sum(case when f.type in ('perpay_plus', 'perpay_plus_v2') then fp.amount else 0 end) as perpay_plus_revenu\\n31      sum(case when f.type not in ('perpay_plus', 'perpay_plus_v2') then fp.amount else 0 end) as other_feature_\\n32  from {{pub_schema}}.feature_payment fp\\n33  left join {{pub_schema}}.feature_enrollment fe on fp.feature_enrollment_id = fe.id\\n34  left join {{pub_schema}}.feature f on fe.feature_id = f.id\\n35  group by cast(fp.created as date), fp.borrower_id, f.type\\n36  )\\n37  select\\n38      dc1.borrower_id,\\n39      dc1.user_id,\\n40      dc1.account_id,\\n41      dc1.src_dt,\\n42      dc1.deposit_amt,\\n43      dc1.cash_refund,\\n44      dc1.refund_to_loan,\\n45      dc1.returned_payment_amt,\\n46      dc1.payments_to_loan,\\n47      dc1.credits_applied_to_loan,\\n48      dc1.payments_less_credit,\\n49      dc1.awaiting_payment_bal,\\n50      dc1.awaiting_payment_applied,\\n51      dc1.withdrawal_amount,\\n52      coalesce(dc2.perpay_plus_revenue, 0) as perpay_plus_revenue,\",\n",
       "  'direct_deposit and a status of returned_accounting. The amount of the row is deducted from the core balance of the associated \\nborrower_id on the date of the created_ts timestamp.\\nPayment Tracking\\nInitiated Credit Card Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\ncard and a status of initiated.\\nReturned Credit Card Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\ncard and a status of returned.\\nCompleted Marketplace Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\nmarketplace and a status of completed. One difference in how this field is defined by accounting is that marketplace payments cannot',\n",
       "  \"546\\nTo understand the above command, see this StackOverflow link. The 172.17.0.1 IP is Docker's IP which you can find in the output of \\nifconfig. This SSH tunnel links all requests sent to Docker's IP through port 5439 to the production Redshift cluster through the jump box.\\nNotes on Permissioning and Connections\\nHad to manually edit Datasync task via the console to give it permission to write logs (there is a little checkbox at the very bottom of the \\nEdit page). Once I did it for one, it seemed to work for all. Could not find how to set this up via Terraform.\\nThe initial list of permissions I had to think about were the following:\\nStaging access to production Datahub,\\nStaging access to the production Redshift cluster,\\nStaging permissions for production Glue crawlers and Datasync tasks.\\nOther connections I had to add during the metric layer conversion:\\nAccess to the production Airflow bucket for the staging Airflow EC2 instance.\\nLet’s expand on the above.\\nAccess to Prod Datahub\",\n",
       "  \"26    ), \\n27    card_feature_enabled as (\\n28        select\\n29            fe.borrower_id,\\n30            min(fh.created) as first_card_enabled_ts,\\n31            fe.status\\n32        from public.feature_enrollment fe\\n33        inner join public.feature_history fh on fe.id = fh.feature_enrollment_id and\\n34            fh.status = 'enabled'\\n35        where fe.feature_id = 67\\n36        group by \\n37            fe.borrower_id,\\n38            fe.status\\n39    )\\n40    --card_dict_cte as (\\n41        select\\n42            b.id as user_id,\\n43            p.card_account_id,\\n44            case \\n45                when d.status is null then 0 \\n46                when d.status = 'enabled' then 1 \\n47                else 0\\n48            end as enabled_ind,\\n49            case when g.borrower_id is null then 0 else 1 end as approved_ind,\\n50            case when h.borrower_id is null then 0 else 1 end as activated_ind,\\n51            round(f.current_balance/f.credit_limit, 2) as utilization,\",\n",
       "  '205                        \"category_path_5\": NULL,\\n206                        \"category_path_6\": NULL,\\n207                        \"visits\": 17\\n208                    },\\n209                    {\\n210                        \"category_path_deep\": \"fashion-beauty/personal-care\",\\n211                        \"category_path_1\": \"fashion-beauty\",\\n212                        \"category_path_2\": \"personal-care\",\\n213                        \"category_path_3\": NULL,\\n214                        \"category_path_4\": NULL,\\n215                        \"category_path_5\": NULL,\\n216                        \"category_path_6\": NULL,\\n217                        \"visits\": 10\\n218                    }\\n219                ],\\n220                \"365_days\": [\\n221                    {\\n222                        \"category_path_deep\": \"electronics/computers/ipads-tablets\",\\n223                        \"category_path_1\": \"electronics\",\\n224                        \"category_path_2\": \"electronics/computers\",',\n",
       "  \"480\\npayment of $100 on the 18th which is why the total decrease in account balance was $850 total). Do you know why Deserve \\ndecremented that card account balance by $750 the day after we'd expect? Let me know if any other data would be useful for you.\\naccount_rollforward_847f677-1d00-4fb4-84e2-23a5fd612da5.csv \\npayments_d847f677-1d00-4fb4-84e2-23a5fd612da5.csv \\nResponse: Nothing jumps out. I believe the SFTP files are generated by Deserve whereas the processor (Corecard) actually maintains \\nthe ledger, so it’s possible there’s a delay between the two that caused this. I’ll raise to them in a ticket and lyk of their response.\\nFollow-up: Bit in the weeds but in these situations it looks like Deserve is reporting the payment to corecard immediately, but there is a \\nslight delay in corecard updating the balance (delay of circa 2 min). Apparently the slight delay is causing the EOD reporting that we\",\n",
       "  'stakeholders for a review. They will be able to provide feedback stemming from a more comprehensive business perspective. \\na. for other repositories like terraform or terraform-modules, request JD and Abby as reviewers; they will not be added by default.\\n6. Once the PR is approved by all reviews, you may squash and merge your PR!!\\n \\n \\n \\n \\nFYI, we have a rule that we don’t merge pull requests on Fridays.\\nThe only exception is when we’re resolving a quick prod error (hotfix) or major error that cannot wait for the weekend. We do this \\nbecause we don’t want any unforeseen errors to occur over the weekend from changes in the code. Anything non-urgent can be \\nmerged on Monday, leaving us the whole week to troubleshoot if something goes awry!',\n",
       "  \"the loan balance on this day is decremented by $21.62 not $10.81. Can we double-check here that this borrower credit did not decrement \\nthe loan balance twice the amount it should have?\\nResponse: It does look to me like 10.81 was paid twice to that loan but we only have a single PaymentDetail, so not sure. I'll have to keep \\ndigging tomorrow to better understand if this is a system error, the result of a manual process that wasn't followed correctly, or if there's \\nsomething else I'm missing. Can you please post the sheet with all 64 variances so I can spot check some others?\\nFollow-up: going to send you the 52 instead, because those were filtered slightly more specific to this variance, and I think would be more \\nhelpful to focus on. These are the examples pre-our understanding up to a few days ago because all updates since then aren't in \\nproduction/aren't reflected in the table yet... so the payment col here subtracts out borrower credits, so when you read this, imagine\",\n",
       "  '6            \"Action\" : [\\n7                \"glue:*\" ,\\n8                \"s3:GetBucketLocation\" ,\\n9                \"s3:ListBucket\" ,\\n10                \"s3:ListAllMyBuckets\" ,\\n11                \"s3:GetBucketAcl\" ,\\n12                \"ec2:DescribeVpcEndpoints\" ,\\n13                \"ec2:DescribeRouteTables\" ,\\n14                \"ec2:DescribeNetworkInterfaces\" ,\\n15                \"ec2:DescribeSecurityGroups\" ,\\n16                \"ec2:DescribeSubnets\" ,\\n17                \"ec2:DescribeVpcAttribute\" ,\\n18                \"iam:ListRolePolicies\" ,\\n19                \"iam:GetRole\" ,\\n20                \"iam:GetRolePolicy\" ,\\n21                \"cloudwatch:PutMetricData\"\\n22            ],\\n23            \"Resource\" : [\\n24                \"*\"\\n25            ]\\n26        }\\n27    ]\\n28}\\nI would highly recommend going through the above and removing them one at a time to see if they are absolutely necessary for the',\n",
       "  \"359\\n \\nOne of the most frequently triggered Amplitude events is the `Loaded a Page` event. This event is currently not referenced anywhere \\nin our codebase. The reason why this event is triggered is thus:\\nAmplitude currently ingests Segment events in our setup, and in the Segment dashboard, the 'Track All Pages to Amplitude' \\noption is currently enabled (see picture below). As a result, whenever a user views a page, Segment sends a ‘page_viewed: \\n<url>’ event along with a ‘Loaded a page' event to Amplitude. Segment treats these two events the same, so these events are \\ndouble-counted in Amplitude. We can thus get rid of the `Loaded a Page` event. \\nRegarding the mechanics of removing an event from Amplitude, there are three ways to remove an event from Amplitude (“hiding”, \\n“blocking” and “deleting”). It is recommended that we block events that we don’t need. \\nThe ‘T rack All Pages to Amplitude’  option in the Segment dashboard\",\n",
       "  '207\\nLooking into this file, we see the perform_matching function is called from dags/analytics/invoice_matching/lib/invoice_matching.py. The \\nfunction has three parameters:  already_parsed, already_processed, and build_matching_tables. The default arguments are False, \\nTrue and True, respectively. \\n \\nEntering the perform_matching function, we have the following logic:\\n(1) check if already_processed is true or false\\nif true (default)- do nothing. Proceed to condition statement 2.\\nif false- we enter the block \\n1: Logs \\'[InvoiceMatching] Processing data for QB...\\'\\n2: Logs \"[InvoiceMatching] Getting reviewed matches into redshift table...\"\\n3: Pulls data from Invoice Matching Google sheet into temp_int.invoicing_level_match_reviewed\\n-> exits if there is no data in the sheet (empty sheet = there’s nothing to do!)\\n4: Logs \"[InvoiceMatching] Adding ready_for_qb=Y from sheet into QB table, ready_for_qb=N to pending \\nsheet...\"',\n",
       "  '58            \\'{\"dt\":\\' || case when m.created_ts is not null then concat(concat(\\'\"\\', to_char(m.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n59                                              else \\'null\\' end ||\\n60                \\', \"ind\": \\' || case when application_started_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as started_card_appliaction,\\n61            \\'{\"dt\":\\' || case when n.created_ts is not null then concat(concat(\\'\"\\', to_char(n.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n62                                              else \\'null\\' end ||\\n63                \\', \"ind\": \\' || case when submitted_card_application_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as submitted_card_application,\\n64            \\'{\"dt\":\\' || case when g.created_ts is not null then concat(concat(\\'\"\\', to_char(g.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n65                                              else \\'null\\' end ||',\n",
       "  \"guys take a look?\\nFor a similar case:\\nHello! Seeing a card account application (application ID: 6512 , borrower ID: 61097) that has a null card_account_id  in the \\npublic.card_account_application table, but the card account application status is provisioned.I can see that they do have a \\ncard account (ID: 4588 ), and in the past, we see the card_account_id field populated in the card_account_application table \\nwhen the card account is created. This null field is causing some tests to break on our end.Would someone be able to help \\ninvestigate if this was expected behavior or not? If so, we'll update the tests on our end. Thanks!\\nAnd another case:\\nHi! We’re seeing a card_account with id = 6697 but no card tied to the account (nothing in card_card table where card_account_id \\n= 6697). This is breaking some tests on our end. Can someone look into this?\\n22    coalesce (b.id, j.card_id) as card_id ,\\n23    f.id as card_account_application_id ,\",\n",
       "  'dispute_write_off field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type \\nis WRITE_OFF.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe summed dollar amount of payments towards the card that hit a COMPLETED status in the day.\\nBusiness Context:\\nStatuses:\\nCard payments can be in one of the following statuses:\\nPROCESSING\\nCOMPLETED\\nRETURNING\\nRETURNED\\nOnce they’re in a PROCESSING state, the system generated card account balance is decreased by the payment amount. I don’t \\nunderstand why this happens if we decrement for completion as well (doing 2x?). We should verify this is the case, and if so, we would \\nneed to account for this PROCESSING state in the table for accounting purposes (not decrease the card account balance until the \\npayment is in COMPLETED):',\n",
       "  \"Solution\\nIf it hasn’t self resolved (post-investigation), we let engineering handle this one (because the root of the prob comes from the public.job \\ntable)!\\nReach out to Talha and tell him there are multiple primary jobs and he’ll run a query on their end to clean it up \\n \\n[PROD] - SPECTRUM - WARNING: Some borrowers have multiple primary job statuses, please check the Platform Job metric layer \\nlogs\\n1-- primary jobs\\n2select\\n3  b.borrower_id ,\\n4  b.status,\\n5  count(b.status) as cnt\\n6from \\n7  public.borrower a\\n8  left join atomic_int .platform_job b on a.id = b.borrower_id\\n9where \\n10  borrower_id is not null and \\n11  b.status = 'primary'\\n12group by 1,2\\n13having cnt > 1;\\n1select * from public.job where borrower_id = <borrower_id >;\",\n",
       "  'add more functionality to these models so that the company can keep a tight grasp on how our new products are fairing out in the wild.\\nDefinitions and data model structure is defined with stakeholders\\nNew data model logic is tested in staging with appropriate emphasis on readability (Jinja) and resource optimization\\nData validation tests are added for all vital features in the new model\\nThis can utilize great expectations as well as normal pytests \\nLogic is integrated into the production environment\\nDocumentation is added (or updated) for the new data model in data analytics space\\nHow do we prioritize projects?\\nBelow are a list of project and task types. They are listed in order of importance to how they should be addressed. Typically, multiple are \\nbeing tackled concurrently, however.  \\nImmediate, catastrophic issues with the data platform and or infrastructure\\nSlack is the main channel where issues are boiled up\\nLarge product-driven initiatives (ex pinwheel integration)',\n",
       "  'are associated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\ncash_back_adjustment field uses its join condition to only bring in the rows in this card_transactions CTE where \\ntransaction_type is BANK_CASHBACK_ADJUSTMENT.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all REFUND transactions associated with a given card account on a given src_dt. The transactions are associated with the',\n",
       "  '73\\nAppendix B: Product Data\\nNote: stored in perpay_marketing_datamart.iterable_catalog\\n256 ],\\n257            \"carted_products\": [],\\n258            \"purchased_products\": [44444, 77789],\\n259            \"purchased_products_detail\": [{ // All products that went into repayment.\\n260 \"product_id\": 44444,\\n261 \"purchased_dt\": \"2018-10-16\"\\n262 },\\n263 {\\n264 \"product_id\": 77789,\\n265 \"purchased_dt\": \"2019-11-29\"\\n266 }\\n267 ]\\n268 \"application_started_products\": [553325],\\n269 \"awaiting_payment_products\": [51555],\\n270 \"repayment_products\": [541111],\\n271 \"verification_products\": [987111]\\n272        },\\n273        \"account_management\": {\\n274                    \"days_past_due\": 25,\\n275                    \"pathway_modifier\": null,\\n276                    \"workable_phone\": 1,\\n277                    \"workable_text\": 1,\\n278                    \"workable_email\": 1,\\n279                    \"strategy_email\": \"Standard\",\\n280                    \"strategy_email_override\": null,',\n",
       "  'integrations, but can be compared to Perpay’s internal ETLs to import external data from sources like Iterable, Facebook, Google, etc.\\nhttps://fivetran.com/blog/case-study-ritual-retention\\nDBT does not support Redshift Spectrum, and only partially supports Presto. A full assessment of any possible re-platforming in the near \\nfuture should be done before fully deploying DBT.\\nhttps://docs.getdbt.com/docs/supported-databases/\\nSummary:\\nSTATUS: De-prioritized\\nPros:\\nDBT allows analysts to embrace software engineering principles like testing, versioning, and branching all while coding in only SQL. With \\nthat, data engineers can focus on maintaining and growing infrastructure for the analysts. Lastly, DBT is an open source platform that offers \\na UI to do all of the above and integrates with our current platform. It would mainly add to the process of how Perpay analytics rolls out and \\naugments data models. \\nCons:',\n",
       "  '227\\nRe-ingesting data from the Manual PO Matching Sheet\\nBut wait!! Just before the data from accounts_payable_int.po_variances gets put into the sheet, we need to account for information that \\nmay already be in the sheet. To do so, we ingest the data into a table, accounts_payable_int.variance_sheet_output and insert it into \\nan accounts_payable_int.variance_sheet_output_history table with an attached timestamp called created_ts (the ts which the \\nrecord entered the table). Then we decide what to do with it based on its ready_for_qb status. Following the descriptions above,\\nIf ready_for_qb is…\\nmarked as Yes: we insert the record in the accounts_payable_int.po_matches table. We also remove the record from the \\naccounts_payable_int.po_variances table.\\nmarked as No: we remove the record from the accounts_payable_int.po_variances table. We don’t add it anywhere because \\naccounting will manually upload the data. Thus, it is removed from the automated process altogether (will not exist in',\n",
       "  \"The third-party data prepared in part 1 is now ingested into Redshift and matched against the internal purchase order information obtained from Magento.\\nEach row from the third-party data is joined on PO number to product POs from Magento. The cost is compared between the invoiced product cost and the product cost in \\nMagento. The difference between the two values is stored in the variance column.\\nPart 3: Reviewing the Matched Data: \\nThe matched data from part 2 is now used to populate the DoInvoiceMatching Google sheet in the Invoice Matching workbook.\\nRows in which no variances have been found are tagged as ready for quickbooks automatically. Also, rows in which the variance is in our favor (invoice total is less than product \\ncost in Magento) and the variance is less than $3 are tagged as ready for quickbooks automatically.\\nTo approve anything that has a variance but is still ready to be inputted into QuickBooks, change the ready_for_qb column to 'Y'.\",\n",
       "  '66\\nFurther clarification on current implementation:\\nAccount Status:         included in the first line of address.\\nCity city Contains city name for address of \\nprimary consumer. Truncate rightmost \\npositions if city name is greater than 20 \\ncharacters or use standard 13-character \\nU.S. Postal Service city abbreviations.city of user_attribute, or if not \\npresent, magento address. \\nTruncated to 20 characters (field \\nmax). Not truncated to USPS 13-\\ncharacter abbreviation, as there \\nare some invalid cities in need of \\ncleaning. \\nState state  state of user_attribute, or if not \\npresent, state of magento \\naddress. Magento state \\nconverted to state abbreviation \\nusing \\nperpay_general_datamart.mage\\nnto_region_to_abbreviation \\nPostal / Zip \\nCodezip_code Report 5 or 9 digit zip codes. zip of user_attribute, or if not \\npresent, 5 digit zip of magento \\naddress. \\nAddress \\nIndicatoraddress_indicato\\nrIf the address is reported, can be',\n",
       "  '166\\nProd Errors 2023-07-10 Onwards\\nThis page will allow us to keep track of the types of errors that fire and the frequency that they do for each week starting on 2023-07-10.\\nThis allows us to see how often an error or warning is sent each week and, at a glance, if it is firing across multiple weeks. Weeks start on a \\nMonday and end on a Sunday to align with our prod error fielding schedule. The name of the team members fielding the errors are also \\nunder each week title so that others may go back and ask the fielders questions if needed!\\nWeek of   -  \\nDerya & Ernest\\nWeek of   -  \\nAbby & Andrew\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nDerya & Hongkai\\nWeek of   -  \\nAbby & Andrew\\n \\nWeek of   -  \\nDerya & Gabe',\n",
       "  'ssBilling/mailing address for the primary \\nconsumer. Cleaned for special \\ncharacters, malformed strings.user_attribute.street_addr, or if \\nnull, last created \\nanalytics_magento_sales_flat_\\norder_address.streetuser_attribute.s\\ntreet_addr\\nSecond Line of \\nAddress\\n second_line_ad\\ndressContains 2nd line of address such as \\napartment number. This can also be \\nincluded in the first line of address.Blank  \\nCity\\n  city Contains city name for address of \\nprimary consumer. Truncate rightmost \\npositions if city name is greater than \\n20 characters or use standard 13-\\ncharacter U.S. Postal Service city \\nabbreviations.city of user_attribute, or if not \\npresent, magento address. \\nTruncated to 20 characters \\n(field max). Not truncated to \\nUSPS 13-character \\nabbreviation, as there are \\nsome invalid cities in need of \\ncleaning. \\nState\\n  state  state of user_attribute, or if not \\npresent, state of magento \\naddress. Magento state \\nconverted to state abbreviation \\nusing \\nperpay_general_datamart.mag',\n",
       "  'loan balance. Refund amounts prior to the refund being observed are summed and considered to be existing refunds for the loan that are \\nalso deducted from the loan balance.\\nOne added filter applied to the refund tracking is to adjust for engineering’s negative account balance test that refunds entirely to the user’s \\ncore account balance in the event that a user carries a negative account balance at the time of the refund. In these cases, the refund is \\nexcluded from the loan refund tracking.\\nCalculating Daily Balances\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance. \\nOn the first day of observation, there needs to be some sort of starting point with which to calculate from. These initial starting balances are \\nsourced from the payment_loanprincipalbalancehistory table. We first try to set the starting balance to the ending_balance of the',\n",
       "  \"13\\n14-- Default privs for schemas\\n15WITH user_acl AS (\\n16SELECT * FROM svv_default_privileges WHERE grantee_type ='user' AND schema_name LIKE 'users_%'  AND schema_name IS\\n17)\\n18SELECT DISTINCT  CONCAT('ALTER DEFAULT PRIVILEGES IN SCHEMA ' , CONCAT(schema_name , CONCAT(' REVOKE ALL PRIVILEGES\\n19\\n1SELECT \\n2    CONCAT (\\n3      CONCAT ('ALTER TABLE ' , \\n4             CASE WHEN schemaname ='public'  THEN '' ELSE schemaname +'.' end+tablename ), '\\n5OWNER TO jherr;' ) AS query \\n6FROM pg_tables WHERE tableowner = 'sfrank' ;\\n7\\n8SELECT \\n9  CONCAT (\\n10    CONCAT (\\n11      CONCAT (\\n12        CONCAT ('ALTER TABLE ' , schemaname ),'.'), tablename ),' OWNER TO jherr;' ) \\n13FROM pg_tables \\n14WHERE tableowner ='skrasnik'  OR tableowner ='jwilliams'  OR tableowner ='skrasnik' ;\",\n",
       "  'Engineering’s public tables. \\nRollforward Table Logic:\\n \\nSource Tables:\\n \\nDefinition:\\nA unique id associated with each Perpay user.\\nBusiness Context:\\nAn id for anyone who has signed up for the Perpay platform. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables.\\nRollforward Table Logic:\\n \\nSource Tables:',\n",
       "  'associated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. The account opening fee is a one-time $9 fee.\\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\naccount_opening_fee field uses its join condition to only bring in the rows in this card_transactions CTE where \\ntransaction_type is ACCOUNT_OPENING_FEE.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all ACCOUNT_OPENING_FEE_ADJUSTMENT transactions associated with a given card account on a given src_dt. The',\n",
       "  \"20    select\\n21        a.vendor_name,\\n22        a.invoice_number,\\n23        a.invoice_total,\\n24        a.ecomm_total,\\n25        a.variance,\\n26        'Not Addressed'::varchar(13) as ready_for_qb,\\n27        a.chosen_payment_option,\\n28        a.invoice_qty,\\n29        a.ecomm_qty,\\n30        a.invoice_net_price,\\n31        a.ecomm_net_price,\\n32        a.invoice_shipping,\\n33        a.invoice_dropship,\\n34        a.invoice_freight,\\n35        a.invoice_po_number,\\n36        a.ecomm_po_number,\\n37        a.invoice_sku,\\n38        a.ecomm_sku,\\n39        a.invoice_date,\\n40        a.invoice_due_date,\\n41        a.tracking_number,\\n42        current_timestamp as created_ts\\n43    from\\n44        accounts_payable_int.variance_sheet_output_history a\\n45        inner join recent_records b on a.invoice_number = b.invoice_number and a.created_ts = b.max_ts\\n46);\\n1select * from users_rehmet.variance_sheet_output_history_additions;\\n1insert into temp_marketplace_int.variance_sheet_output_history\",\n",
       "  '1create table temp_dev_int .marketing_daily_source_metrics_iterable_2 as\\n2select distinct\\n3    b.campaign_id ,\\n4    b.adset_id ,\\n5    b.ad_id,\\n6    a.join_key ,\\n7    a.src_dt,\\n8    a.platform ,\\n9    b.clicks,\\n10    b.impressions ,\\n11    b.reach,\\n12    b.applications_started ,\\n13    b.applications_submitted ,\\n14    b.applications_approved ,\\n15    b.applications_in_revenue ,\\n16    b.applications_in_revenue_val ,\\n17    b.applications_started_val ,\\n18    b.applications_submitted_val ,\\n19    b.applications_approved_val ,\\n20    null as unique_clicks ,\\n21    null as signups ,\\n22    null as spend,\\n23    null as product_view ,\\n24    null as product_view_val ,\\n25    null as product_add_to_cart ,\\n26    null as product_add_to_cart_val ,\\n27    null as borrowers ,\\n28    null as first_time_borrowers ,\\n29    null as borrowers_val ,\\n30    null as app_installs ,\\n31    null as app_installs_val ,\\n32    null as checkout_completed ,\\n33    null as checkout_completed_val',\n",
       "  '353\\nPages Events\\nThis page explains the difference between segment_storefront_schema.pages and segment_web_schema.pages.\\nThe Difference Between the Pages Table in Each Schema\\nsegment_storefront_schema.pages tracks \"page loaded\" events for sites corresponding to the storefront, \\nwhile segment_web_schema.pages tracks this event for the website only.\\nFor example:\\nAn event in segment_storefront_schema.pages would come from a storefront page (in terms of the URL), similar to\\nhttps://shop.perpay.com/featured/gifts-for-him-2022?p=3&modal=tireWidget\\nor\\nhttps://shop.perpay.com/search?query=Gifts%20for%20couples\\nThe url corresponds to some marketing campaign on the storefront or a search query on the storefront.\\n \\nIn contrast, a URL in the segment_web_schema.pages table would be look like\\nhttps://app.perpay.com/sign_up\\nwhere they person is signing up to be a Perpay customer, but the site isn\\'t directly on the storefront.',\n",
       "  'reconcile.py (data_model_accounting/daily_account_reconciliation_data_model/reconcile.py)\\ncore_account_daily_rollforward.py\\nreconcile.py (data_model_accounting/account_reconciliation/reconcile.py)\\ndebit_cards.py (data_model_accounting/account_reconcilliation/credit_cards.py)\\nach_payments.py\\naccount_balance.py\\ndata_model_accounting_dag.py\\ngenerate_am_workable_file.py (call_lists/lib/queries)\\nborrower_company_form_check.py\\ninbound_phone_test_report.py\\n1-- Only returns the payments in the auto_pay_payment table\\n2SELECT \\n3  a.*\\n4FROM\\n5  perpay_accounting_datamart_ext .deserve_payments_eod_report a\\n6  LEFT JOIN public.card_cardpayment b on a.payment_id = b.deserve_id\\n7  INNER JOIN public.auto_pay_payment c on b.id = c.card_payment_id',\n",
       "  '14\\nCurrent Documentation Additions\\ncard_user_application\\ncard_user_application_summary\\ncard_user_application_temporary\\ncard_user_fact\\ncard_user_current_application_status\\ncard_user_detail',\n",
       "  'or 95. Contains a code that identifies \\nwhether the account was current, past \\ndue, in collections or charged off. \\n0 = 0–29 days past due\\n1 = 30-59 days past due\\n2 = 60-89 days due\\n3 = 90-119 days past due\\n4 = 120-149 days past due\\n5 = 150-179 days past due\\n6 = 180 or more days past due\\nG = Collection\\nL = Charge-offSee further clarification below. \\nThis includes logic for new \\ncharge-off policy. \\nPayment History \\nProfilepayment_history\\n_profileContains up to 24 months of consecutive \\npayment activity. Report one month’s \\npayment history in each byte from the \\nleft to right in most recent to least recent \\norder. The first byte should represent the \\naccount status in the previous reporting \\nperiod. Payment codes are the same as \\nthe payment rating, with the following \\nadditions:\\nB = No payment history available prior to \\nthis time (account not opened)\\nD = No payment history available this \\nmonth.\\nE = Zero balance and current account \\n(only applied to credit cards and lines of \\ncredit)',\n",
       "  '580\\nRoadmap for Completion\\nThis page serves as a living document for completing the full roll out of the automated AP process. The sections describe the questions and \\nprocedures that still need to be fleshed out.\\nThe overall plan is to push Almo all the way through to act as a POC for the entire (non-csv) process. \\nIn Progress\\nCommerce has been informed that we need to know what level of variance is acceptable between out books and the vendors - we await \\ntheir reply\\nSylvia has reached out to Alec to determine what file format is easiest for him so that we can instruct the vendors to be more uniform. It \\nis assumed that CSV would be easier for us in this process due to the more standerd format compared to a PDF\\nNext Steps\\nNeed to incorporate pagination into how the API response is being handed from Docparser so that we can be sure all data would be \\nmaking it though to Redshift',\n",
       "  '7: Logs [InvoiceMatching] Adding ready_for_qb=Y from sheet into NewEntries sheet, formatted...\"\\n8: Append the YESes to the already_in_new_qb_sheet df, then write it back to the NewEntries sheet in the QuickBooks Bills \\nGoogle sheet:\\n- Pulls everything down from the yes table,perpay_accounting_datamart_ext.invoicing_new_quickbooks_entries, \\nand only keep the most recent ones that were entered. Save that to a df called new_qb_df\\n- Grab all data from the QuickBooks Bills Google sheet NewEntries sub sheet and save it to a df called \\nalready_in_new_qb_sheet. \\n- If there was something in that Google sheet (meaning already_in_new_qb_sheet had data), we concatenate the two \\ndataframes together and save it into the all_new_qb_df.\\n- If there wasn’t anything in that Google sheet (meaning already_in_new_qb_sheet didn’t have data), we just save \\nthe data from the yes table (new_qb_df) into the all_new_qb_df.\\n- Then sort the data in all_new_qb_df by Bill No.',\n",
       "  'views. A good example of a source layer implementation is how DBT implements this structure.\\nMaterialized Views\\nWhat are they? Why use MVs? How do they work?\\nWhat: Materialized views are data tables that can query from multiple existing tables and store precomputed queries. MVs store \\nprecomputed queries as physical tables written to the disk, and can update changes to the underlying data from its source \\ntables. They can be both manually updated and automatically updated on a schedule. \\nWhy: They provide benefits in managing database resources such as improving speed and decreasing compute power, as well as a few \\nothers. \\nSpeed: Querying from a MV does not require recomputing the data from source table, due to MV’s ability to precompute the \\nexpensive joins and aggregations.\\nConsistent data: Provides a consistent view of the data at any given time snapshot. By consolidating several complex queries into',\n",
       "  'Host DataDog Host \\n{{host.name}} AlertAlerts when our host \\n(airflow-prod, ana-prod-\\nairflow-ec2) has gone \\ndown  \\nIntegration New Airflow Dag \\nDuration: High on \\n{{host.name}} for \\n{{dag_id}}Alerts when the task \\nduration is long average duration over \\nthe day:\\n190 < warning <= 210 < \\nalertAlerts for 57 dags- 5 per \\npage, sorted in order of \\nmost recently triggered \\nto less recently triggered\\nIntegration New Airflow Dagrun \\nSuccesses: Low on \\n{{host.name}}Alerts when the dagrun \\nsuccesses are lowsum of successes every \\nhour:\\nalert <= 5 < warning <= \\n13 \\nIntegration New Airflow Queued \\nTasks: High on \\n{{host.name}}Alerts when the number \\nor queued tasks are \\nhigh max count every 15 \\nmins:\\n15 < warning <= 20 < \\nalert \\nIntegration New Airflow Running \\nTasks: High on \\n{{host.name}}Alerts when the number \\nor running tasks are \\nhigh max count each 5mins:\\n33 < warning <= 40 < \\nalert Monitor Type Name Goal Conditions Notes',\n",
       "  'commensurately.\\n2. borrower_id = 4251822 / card_account_id = 14777 / card_payment_id = 250680\\ndeserve_payment_250680.csv \\ndeserve_account_balance_14777.csv \\nVariance 10: Missing International Transaction Fees\\nAlready resolved- won’t attach examples',\n",
       "  'schema with the table name invoicing_unparsed_<vendor> (i.e. temp_int.invoicing_unparsed_rymax). \\n- these three vendors are in our current sheet, so one table is created for each:\\n- BestBuy\\n- Rymax\\n- FragranceNet\\nAlso creates a Redshift table for the VendorKeyName sheet. This is saved to temp_int.invoicing_vendor_key_matching.\\n(if you want to look into the function that creates these tables, it’s all_invoicing_sheets_to_redshift in \\ndags/utils/lib/google_sheet_integration.py. It loops through the Non Parsed Partners sheet and calls the sheet_to_redshift \\nfunction to write its contents to the schema. The argument False is set to False so that the Non Parsed Partners google sheet \\ndoesn’t clear. If set to true, the sheets will be cleared; we never want this to happen, so we leave it as false)\\n3: We create empty external tables for the vendors, using the temp_int tables from step 2. These tables follow the same',\n",
       "  '404\\nTransaction Tracking\\nData Sources\\n1. All transactions in the card_transaction metric layer with a status of SETTLED and a current_per_status_ind of 1.\\n2. All transactions in the deserve_daily_posted_transactions_report built from the Deserve SFTP data that either do not exist or are \\nnot settled in the card_transaction metric layer.\\n3. All transactions in the deserve_daily_settled_transactions_report built from the Deserve SFTP data that either that are pending \\nwith a non-null expiry date in the webhook data that were transacted before 2023-02-28. This accounts for a known issue that previously \\nexisted with pre-auth transactions settling and not being reported as settled.\\nData Nuances\\naccount_opening_fee: There is a $9 fee assigned to new accounts that are opened, with the exception of staff card accounts.38,2) certain merchants Tracking\\nrefund Numeric(\\n38,2)Sum of transactions of type REFUND for \\nthe card account on the observation dateTransaction \\nTracking',\n",
       "  \"Adjusting for Cards in the Wild437\\n                Clarity Score Drift from FTBs459\\n                VARIANCES - Accounts Receivables Reconciliation461\\n                     Card Report Definitions462\\n                     Card Report Variances476\\n                     Core Report Context483\\n                     Core Report Definitions489\\n                     Core Report Variances492\\n                     Commerce Report Definitions496\\n                     Commerce Report Variances502\\n                     Tying out Engineering's Account Balance Flow514\\n                Deposit, Payment, and WithdrawalRequest Updates519\\n                Iterable DAG Revamp523\",\n",
       "  'a LEFT JOIN onto the atomic_int.card_balance table here.\\nSource Tables:\\nThe field comes directly from public.borrower\\n \\nDefinition:\\nA unique id associated with each Perpay borrower.\\nBusiness Context:\\nAn id for anyone who has had a loan hit a revenue status in the past. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables. \\nRollforward Table Logic:\\nBorrower id is used as a join key to connect card payments with the rest of the card history here.\\nSource Tables:\\nBorrower id is sourced from our metric layer tables, which source it from the public.borrower table.\\nDefinition:\\nA unique id associated with each Perpay user.\\nBusiness Context:\\nAn id for anyone who has signed up for the Perpay platform. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables.\\nRollforward Table Logic:\\nUser id is used as a join key to connect our metric layers to the public.borrower table here.\\nSource Tables:',\n",
       "  'direct_deposit and a status of returned_accounting. The amount of the row is deducted from the core balance of the associated \\nborrower_id on the date of the created_ts timestamp.\\nPayment Tracking\\nInitiated Credit Card Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\ncard and a status of initiated.\\nReturned Credit Card Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\ncard and a status of returned.\\nCompleted Marketplace Payments: The sum of amount in the platform_payments metric layer for the account with a destination of \\nmarketplace and a status of completed. One difference in how this field is defined by accounting is that marketplace payments cannot',\n",
       "  \"accounting will manually upload the data. Thus, it is removed from the automated process altogether (will not exist in \\naccounts_payable_int.po_matches and will not exist in accounts_payable_int.po_variances). We will never delete records from \\nthe accounts_payable_int.variance_sheet_output_history table, so that table acts as a historical record for every row that has \\nbeen in the Google Sheet with its corresponding timestamp.\\nmarked as Awaiting Decision: we update the status of the record in accounts_payable_int.po_variances to be 'Awaiting Decision'. \\nThat way, it will reenter the sheet, but with the updated status.\\nmarked as Not Addressed: we don’t take action. These records are still in the accounts_payable_int.po_variances table, so will \\nreenter the sheet as is.\\nThe chosen_payment_option operates independent of ready_for_qb. While ready_for_qb dictates where the data will travel in the\",\n",
       "  '29\\n2023 Q1 OKRs\\n Timeline 1/16/2023 - 3/27/2023\\nRelated \\npagesRoadmap:\\nPerpay OKRs | Q1 2023 - RAAF Roadmap Team @JD Herr @Abigail Rehmet \\nComplete a refactoring of our \\naccount reconciliation process\\nOwner: @Abigail Rehmet 1.0 Month 1\\nMonth 2\\nMonth 3\\nCardholders are accurately \\nreported to all three major credit \\nbureaus\\nOwner: @Abigail Rehmet   @JD \\nHerr   \\n     \\n     \\nTarget: Hire 1+ individual with \\n4+ years experience\\nOwner: @JD Herr      \\n     \\n     Objectives Key results Owner Partner \\nwithExpected \\nEoQ key \\nresult \\nscoreCurrent status',\n",
       "  '263\\nDeserve SFTP Ingestion\\nIn an ideal scenario, Deserve will send us a new set of files each day, comprising\\ndeserve_cleared_transactions_eod_report\\ndeserve_daily_settled_transactions_report\\ndeserve_net_settlement_eod_report\\ndeserve_payments_eod_report\\ndeserve_rewards_daily_summary_report\\ndeserve_applications_eod_report\\ndeserve_disputes_eod_report\\ndeserve_accounts_eod_report\\ndeserve_cards_daily_report\\nTypical Cadence\\nCurrently, our Deserve SFTP DAG structure executes once daily at 9:30 AM EST to ingest these new files. This takes about 7 minutes.\\nThere is also an option to manually trigger the DAG with a configuration to initiate a full rerun ({\"full_rerun\": true}). However, this \\nprocess consumes nearly 4 and a half hours. During this time, tables are dropped, and files are gradually appended by sftp_dt. This means \\nthat the full dataset becomes temporarily inaccessible for queries.\\nFluctuations in Deserve Report Data',\n",
       "  '468\\ninterest_charge\\ninterest_charge_adjustment\\ncash_backSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all INTEREST_CHARGE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\ninterest_charge field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nINTEREST_CHARGE.\\nSource Tables:',\n",
       "  '331\\nData Discrepancies v1 (11/21/22)\\nHigh Impact Issues\\nPerpay Core Issues:\\nPerpay Core Issues - Latency:\\nNote: for simplicty, we will consider anything longer than 2 hours “late”1 transaction ID: \\n206089Transactions that exist in \\npublic.card_transaction do not \\nexist in \\npublic.card_transaction_snapsho\\nt36,731 transactions (~19% \\nof transactions)This is fixed! PR has \\nbeen merged to make \\nsure this doesn’t \\nhappen with future \\ntransactions and older \\ntransactions have \\nbeen backfilled to \\nhave snapshots. The \\nfix was deployed on \\n12/29/22, snapshots \\nwere backfilled on \\n12/30/22.\\n2 transaction IDs: \\n5ae53d2c-b816-\\n5c84-92a6-\\n17c367cbe28b\\n120eba31-d983-\\n50bc-81e7-\\n88b3df9383e5\\n3b51342f-72e5-\\n5cd2-9138-\\n7828c9dbc727The total transaction is different in the \\nPerpay Core database (as compared \\nto Deserve Admin & SFTP files).3 transactions This is fixed! These \\nwere deserve issues \\nwhen they used to \\nsend us webhooks \\nbefore updating their \\nown database. I’ve \\nupdated these',\n",
       "  'Most lines can be broken by implied continuation, but if that is not possible, you can use a \\\\ to break the line.\\nIf you have to break a line, use indentation to improve readability so that the reader knows which line spanned over two. Function Use a lowercase word or words. Separate \\nwords by underscores to improve \\nreadability.function, my_function\\nVariable Use a lowercase single letter, word, or \\nwords. Separate words with underscores to \\nimprove readability.x, var, my_variable\\nClass Start each word with a capital letter. Do not \\nseparate words with underscores. This \\nstyle is called camel case or pascal case.Model, MyClass\\nMethod Use a lowercase word or words. Separate \\nwords with underscores to improve \\nreadability.class_method, method\\nConstant Use an uppercase single letter, word, or \\nwords. Separate words with underscores to \\nimprove readability.CONSTANT, MY_CONSTANT, \\nMY_LONG_CONSTANT\\nModule Use a short, lowercase word or words. \\nSeparate words with underscores to',\n",
       "  '\"true\"}. This should be done every so often, say, every ~30ish days.\\nThis turns the accounts_payable_int.qb_history table into a dataframe. This is called the historical_bills_df.\\nIt submits a GET request via the Quickbooks API (quickbooks_api_class.py) on Bills and turns the response into a dataframe. This is \\ncalled the qb_bills_df.\\nThe contents of qb_bills_df are “subtracted“ out of the historical_bills_df using Python joins. The remaining contents of \\nhistorical_bills_df after “subtraction“ has occurred are the bills that exist in historical_bills_df but not in qb_bills_df.\\nWe’re hoping that the number of historical_bills_df that exist at the end is 0, because that means that every bill in qb_history \\nhas been uploaded to Quickbooks at some point. If this isn’t true, this means bills aren’t being uploaded completely/some were \\nmissed in the upload process. This should be double checked by doing a search in QB and manually uploaded if it’s true (you can',\n",
       "  'There was no use of the initiated_at fieldThis Confluence page tracks when the change over to the newer configuration occured:\\nACH Holdout Logic  \\nVariance 8: Processing Payment Timing\\n1: borrower_id = 23817 / card_account_id = 6123 / deserve_card_payment_id = d847f677-1d00-4fb4-84e2-23a5fd612da5\\nQuestion: (for Conor) According to deserve_payments_eod_report, this $750 payment was PROCESSING on 2023-08-17. Based on \\nour prior conversations, considering this happened post-May (in our current Deserve paradigm), the card account balance should have \\ndecremented on the 17th. However when we look at the account rollforward, it decrements on the 18th (along with another pending',\n",
       "  \"436\\nSome additional queries that were required to catch lingering permissions:\\nI also found this page very useful, next time we should start here for revoking privileges. The following two queries were sufficient to resolve \\nremaining problems (objects owned by sfrank which happened to be two functions):\\n1. https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_generate_user_grant_revoke_ddl.sql \\n2. https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_find_dropuser_objs.sql \\n9user_table_privs AS (\\n10SELECT DISTINCT  namespace_name , identity_name , identity_type FROM svv_relation_privileges WHERE identity_type ='u\\n11)\\n12SELECT DISTINCT  CONCAT('REVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA ' , CONCAT(namespace_name , CONCAT(' FROM ' ,\\n13\\n14-- Default privs for schemas\\n15WITH user_acl AS (\\n16SELECT * FROM svv_default_privileges WHERE grantee_type ='user' AND schema_name LIKE 'users_%'  AND schema_name IS\\n17)\",\n",
       "  '133\\n(Error) Company Switch Mid-Run\\nIn the airflow_prod_alerts channel:\\nError explanation\\nUsually this error is an indication that there was a company switch mid run (which occurred from a timing issue). \\nInvestigation\\nUsually look into loan_fact or user_fact first for duplicates; most tables rely on these. \\nIf there is a duplicate, see where it’s data differs across row- e.g. if there was a company switch mid-run the company values will differ \\nacross rows.\\nSolution\\nBecause it was due to timing, it should self resolve if re-ran. Thus, we clear the tasks that rely on public.company in DM full. Be careful when \\nyou clear tasks- you want to clear the correct one (verify the name when you clear it) and clear the actual table build, not just the test that \\nfailed. \\n** something to keep in mind: now that dm full tasks are more independent as of [November 2, 2022], you need to check other data',\n",
       "  '585\\nChoose a schedule for your crawler (i.e., Run on demand), then click next.\\nWe will specify in the dag that the glue crawler must run before the rest of the dag\\nChoose the destination database for your your crawler’s output (the database you created in step 3). Click next.\\nReview the crawler, and click finish\\nOnce the crawler is made, run the crawler by clicking “Run it now?”\\nOnce the crawler is finished, the number under “Tables added” will turn from 0 → 1. To view the table, Navigate to tables and filter on the \\ndatabase you created in step 3. You will see the newly created “table” from the CSV. You can click into it for the details. \\nStep 5: Create a glue job to transform the data from csv to parquet\\nCreate a bucket in s3 for the script to be stored in (example: s3://ana-stage-airflow-s3-1zcit/scripts_storage/)\\nNavigate to AWS Glue service in the AWS console → ETL → Jobs (legacy)\\nClick Add job\\nFill in Job Properties',\n",
       "  'is a variance, meaning one of the following:\\n1. DE missed a field/a business case or miscalculated a field. Either the calculation is missing something or is being performed \\nincorrectly\\n2. The logic for a field that goes into this calculation is buggy on Engineering’s side and we’re looking at the downstream effects.\\n3. The logic for a field that goes into this calculation is buggy on Deserve’s side and we’re looking at the downstream effects.\\nRollforward Table Logic:',\n",
       "  \"in the card you were referring to. However, it's worth noting that the core table is not yet complete, so I wouldn't recommend relying on it \\nsolely for refund calculations at this point. I want to emphasize that both domo cards you're examining seem accurate but represent different \\nperspectives/contexts.If you prefer the commerce loan table to reflect the entire refund amount, we can certainly explore making that \\nadjustment. But I think it's important to consider that doing so would alter the table's context. On a row-by-row basis, you wouldn't be able to \\ndirectly infer that the entire amount went toward the loan balance.\\nIf we decide to pursue this approach to have the refunds column reflect the total refund amount, my suggestion would be adding a new \\nfield, perhaps amount_towards_account_balance, to capture any funds directed toward the account balance rather than the loan balance.\",\n",
       "  \"296\\n2\\n0\\n9\\n7\\n0 M\\ny\\nC\\nre\\ndi\\nt\\nN\\no\\nw\\n8\\n5\\n0\\n_\\nQ\\n1\\n2\\n0\\n2\\n4          3\\n6\\n9      p\\na\\ny       X\\nT\\n_\\nLI\\nN\\nK s:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          o\\np \\nn\\no\\nw, \\np\\na\\ny \\nla\\nte\\nr, \\nb\\nui\\nld \\ncr\\ne\\ndi\\nt                     M\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0                 e              e       e         e               e    G\\nLI\\nS\\nH                      2\\n4-\\n0\\n1-\\n0\\n2\\nT\\n1\\n7:\\n2\\n4:\\n2\\n5-\\n0\\n5:\\n0\\n0  B\\ne\\ngi\\nn\\nni\\nn\\ng\\ns, \\nL\\nL\\nC'\\n]          e             \\n1\\n9\\n2\\n0\\n8\\n1\\n7 P\\nu\\ns\\nh\\nn\\na\\nm\\ni_\\nQ\\n4\\n2\\n0\\n2\\n4                                                  1\\n4\\n3\\n6\\n9      P\\ner\\np\\na\\ny       T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK ht\\ntp\\ns:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          []                                                                                                                                                      $\\n1,\\n0\\n0\\n0 \\nto \\nS\\nh\\no\\np \\nN\\no\\nw, \\nP\\na\\ny \\nL\\nat\\ner                                                                    H\\nT\\nM\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0.\\n0\",\n",
       "  '616\\nAirflow Deployment',\n",
       "  '474\\nvarianceWe take the starting_balance and add any transactions, fees, charges, etc within the day that would increment the card account \\nbalance from an accounting standpoint and subtract any refunds, returns, disputes, adjustments, etc that would decrement the card \\naccount balance from an accounting standpoint.  \\nRight now, we add(+) to card starting balance…\\ntransactions\\naccount_opening_fee\\nmonthly_fee\\nlate_payment_fee\\nshipping_fee\\ninterest_charge\\ncash_back_adjustment\\ncredit_balance_refund\\nreturned_payments\\nWe subtract(-) from card starting balance…\\naccount_opening_fee_adjustment\\nmonthly_fee_adjustment\\nlate_payment_fee_adjustment\\ndebit_card_processing_fee_adjustment\\ninterest_charge_adjustment\\ncash_back\\nrefund\\ndispute_won\\ndispute_lost_withdrawn\\ndispute_provisional_credit\\ndispute_write_off\\npayments\\nSource Tables:\\nstarting_balance comes from atomic_int.card_balance, which pulls current_balance from webhooks first through the',\n",
       "  '438\\nFiles\\nThis is a list of files that calls the feature_enrollment table. If you scroll down, I’ll parse each to decide what needs to change. The green \\ncheck indicates they shouldn’t require any change (in the clear ✅ ) and the red X indicates they will likely (didn’t make the cut ❌ ). \\n \\nab_test_backend_signup_report ✅\\ngenerate_am_workable_file ✅\\nproduct_card_activation ❌\\ndetail (card user dm) ❌\\nfact (card user dm) ❌\\nfact (card borrower history dm) ❌\\naccount_detail (card borrower dm) ❌\\nperformance (card borrower dm) ❌\\ngenerate_data (credit reporting) ✅\\nupdate_borrowers (credit reporting) ✅\\nscript_experian_pull_monthly_vantages ❌\\naccount_balance ✅\\nplus ✅\\nfs_daily_perpay_plus_revenue ✅\\nattribute (borrower dm) ✅\\nrisk_factors_hist (loan dm) ✅\\nattribute (plus borrower dm) ✅\\nfact (plus borrower dm) ✅\\nstatus_history (plus borrower dm) ✅\\nfact (plus funnel dm) ✅\\nattribute (user dm) ✅\\nuser_fact (risk dm) ❌\\nadhoc_card_data_update_query ✅\\ngenerate_data (iterable send) ❌\\ncard_activation_events ❌',\n",
       "  'Airflow scheduler out of memory problems (stack overflow)\\nAirflow worker failed to start due to error OSError (cannot allocate memory)\\nInteresting PR from 2017 that explains how Airflow handles dead gunicorn worker\\nAWS APv2 Infra Articles:\\nThese were mostly used when creating the infrastructure for AP v2 statement ingestion\\nHow to Convert Many CSV files to Parquet using AWS Glue\\nxls to csv converter\\nXLS TO CSV Conversion\\nProcess Excel files in AWS Lambda using Python, Pandas and Layers\\nDeploy Python Lambda functions with .zip file archives\\nAWS Lambda with Pandas and NumPy\\nNo Zipping Needed: Importing Python Packages in AWS Lambda\\nExtract Email Attachment using AWS\\nAutomating ETL jobs on AWS using Glue, Lambda, EventsBridge and Athena\\nHow can I use a Lambda function to automatically start an AWS Glue job when a crawler run completes\\nwrite a python shell job in glue documentation\\nAWS Glue job to import an excel file\\nConvert CSV / JSON files to Apache Parquet using AWS Glue',\n",
       "  '540\\n(Datagatew\\nay Profile)\\nEquifax \\nSFTP (EFX \\nFile \\nGateway)\\nIntercom \\nHMAC\\nDOMO full \\n(access \\nmanaged \\nby Zach M)\\n \\nDOMO \\nread only \\n(access \\nmanaged \\nby Zach M)\\nGitHub: \\nEngineerin\\ng \\ncodebases\\nGitHub: DE \\ncodebases\\nGitHub: DE \\ncodeowner\\nadmin.perp\\nay.com\\nSegment\\n  \\n  \\n  \\nMiro\\nLoom',\n",
       "  'The Unit Tests job runs tests on business logic to ensure nothing breaks during development.\\nAirflow Tests calls a shell script in the same location and executes like Unit Tests above, but parses for the prefix /airflowtest_. The \\ncurrent import error test collects the DAGs from our repo and to check their imports. A specific error is held out from the error \\nmessage: psycopg2.OperationalError: could not translate host name \"postgres\" to address: Name or service not known - this is thought \\nto just be a problem with Travis not accessing our database, so it is suppressed. \\nSyntax Tests calls flake8 directly from the terminal and passes in which specific flake8 tests we’d like to run. It does not run all flake8 \\nlinting since this would be awful. Below is a list of those that are used and are relevant for simple syntactical testing. They can be \\nexpanded upon in the future.\\n # F822: undefined name `name` in `__all__` \\n# F823: local variable name referenced before assignment',\n",
       "  'past 24 months. So, any single record for a borrower sent to Experian will include data over the past 24 months.\\nMetro 2 Formatting and Compliance Detail:\\nMetro 2 is the file format accepted by the major credit bureaus in the US, regulated by CDIA. It consists of a text file, with each row being a \\nrecord. In the fixed length format, each row is a set number of characters, with certain positions being allotted for different fields. Perpay will \\nbe reporting the Base and N1 segments, totaling to 572 characters per record.\\nAfter running the query to generate the data content, the script converts the dataframe into Metro 2 format, and runs compliance tests as \\nnoted throughout the CDIA documentation.\\nThe formatting works as follows:\\nGeneral rules: All numbers should be rounded to a whole number. All dates (besides Time Stamp) should be formatted MMDDYYYY. All \\nstrings should be upper case.',\n",
       "  'the created timestamp. (NOTE: There are instances of user merges where a payment is attributed to a lead user, but the associated \\nborrower credit for the payment is attributed to a different user in the merge cluster. This is handled later on in the aggregation logic.)\\nCompleted Perpay Plus Payments: Completed Perpay Plus payments are sourced from rows in the feature_payment table with a \\nfeature_id that has a type of either perpay_plus or perpay_plus_v2. The amount is considered to be a Perpay Plus payment for the \\nassociated borrower_id on the date of the created timestamp.\\nCalculating Daily Balances\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance. \\nOn the first day of observation, there needs to be some sort of starting point with which to calculate from. These initial starting balances are',\n",
       "  '574\\nCalculating AWS Costs\\nTo calculate Analytics AWS costs, do the following:\\n1. Log into the AWS web console as the root user (i.e., jd@perpay.com)\\n2. Navigate to the S3 → Buckets console. \\n3. Click into the perpay-bills bucket. \\n4. To get the most recent bills, sort by last Last Modified descending. \\n5. To get a month’s bills, click on the 127568170241-aws-billing-csv-year-month.csv file. Download it.\\na. Let’s say, for example’s sake, that we want to calculate costs for the month of November, 2022. This file is 127568170241-aws-\\nbilling-csv-2022-12.csv\\n6. Once downloaded, to calculate AWS costs, you can filter on LinkedAccountName (Analytics accounts are Staging Account, Production \\nAccount, and Segment Account). You can also filter on ProductCode to further break down these costs. \\na.  Example: let’s say we want to look at DMS costs on the Production Account in the month of November, 2022.',\n",
       "  '75\\nAppendix C: Card Data field Sourcescategory_name_4 varchar(94) The category name of the next level sub \\ncategory.\\ncategory_name_5 varchar(94) The category name of the next level sub \\ncategory.\\ncategory_name_6 varchar(94) The category name of the next level sub \\ncategory.\\ncategory_path_1 varchar(250) The category url path of the category.\\ncategory_path_2 varchar(250) The category url path of the next level sub \\ncategory.\\ncategory_path_3 varchar(250) The category url path of the next level sub \\ncategory.\\ncategory_path_4 varchar(250) The category url path of the next level sub \\ncategory.\\ncategory_path_5 varchar(250) The category url path of the next level sub \\ncategory.\\ncategory_path_6 varchar(250) The category url path of the next level sub \\ncategory.\\ncategory_id_path_1 varchar(43) The category ID path of the category.\\ncategory_id_path_2 varchar(43) The category ID path of the next level sub \\ncategory.\\ncategory_id_path_3 varchar(43) The category ID path of the next level sub',\n",
       "  'Data Understanding\\nProblem\\nIndependent of locks occurring on any tables, the DAG takes a long time to run. One can expect running the entire DAG from start to finish \\nto take ~2h45m. \\nUnfortunately, this figure often becomes much larger due to locks that occur on some of the tables needed by the tasks. When a lock \\noccurs, the task will have to restart, which further delays the DAG.\\nRun from June 16, 2022',\n",
       "  \"ensures that stakeholders know that when they are looking at a data element, the parent assets have also been documented and are ready \\nfor use.\\nDocumentation Review\\nA beneficial feature of Datahub is the ability to enable and disable data assets' visibility in the UI through search. This allows the Data \\nEngineering team to pull in new data sources without making them searchable for viewing by Datahub users, while still being able to access \\nthe page with the exact URL. Consequently, all new documentation will be created and reviewed in a state that is not accessible to outside \\nstakeholders. Once the new documentation has been reviewed and approved, the new data assets will be enabled for viewing through \\nsearch, and a release message will be created and distributed via the Datahub front page “posts.” Note, documentation review will also \\ninvolve non-DE team members so that SMEs can validate that the documentation is accurate and feature complete.\\nDatahub Tooling\",\n",
       "  'lo\\nw\\nd\\nem\\no\\nbi\\nle\\nrp\\nh\\no\\nn\\nep\\nr\\no\\nm\\nola\\nn\\ng\\nu\\nali\\nm\\nit\\ne\\ndli\\nm\\nit\\ne\\ndr\\ne\\nst\\nri\\nctr\\ne\\nst\\nri\\ncts\\ne\\na\\nst\\no\\np\\ns\\neld\\ne\\nal\\nidd\\ne\\nal\\nn\\nad\\ne\\nal\\nd\\ned\\ne\\nal\\nst\\nad\\ne\\nal\\ntyd\\ne\\nal\\ns\\ncd\\ne\\nal\\np\\nrd\\ne\\nal\\nc\\nadi\\ns\\nc\\no\\nud\\ns\\nc\\no\\nu',\n",
       "  '110\\nSQL Queries\\nQuery readability is super important! It helps DEs understand code faster. \\nThese are some query conventions everyone should practice when planning to add code to the codebase. \\nReadability\\nReadability is SUPER important to us! If your PR does not follow these conventions, we will not merge it!\\n(1) Commenting conventions\\nEach table in our schema should have a general comment at the top of its creation!\\nAdditionally, each block of code should include a comment above it! \\nThe example above from platform_merged_users.py shows these well (lines 4-6 and each line above the CTE creations). \\n(2) SQL Capitalization\\nall SQL commands should be written in uppercase letters (i.e SELECT, FROM, WHERE, ORDER BY, GROUP BY, JOIN, MIN, MAX, SUM, FIRST, LAST, DESC, ASC, ON)\\nThis shouldn’t be too much of a burden, as it can be done once you’re done writing all your code:\\nIf you use VSCode:\\ninstall Upper Case SQL from the extensions panel',\n",
       "  '608\\nEncyclopedia',\n",
       "  '38\\nPlatform and Infrastructure Performance Notes\\nThis page keeps track of overall observations of performance issues of the analytics data platform.\\nThis article highlights many of the issues and improvements that are possible on our platform\\nPlatform Problems\\nCannot build the platform images on Prod or Staging\\nApache Airflow constantly runs into situations where too many dags/tasks are running\\nThis can cause the scheduler to fail which stops all analytics processes\\nBringing the scheduler back up can be precarious with making sure not all dags start back up simultaneously\\nThe queries in the accounting DM dag are very inefficient and every day between 11 AM and 12 PM, they cause mass locks on Redshift\\nThis impacts other dags that are running and is capable of causing cascading effects\\nIt happens every day so the errors are not noticed or addressed\\nThe user dm being pulled into DOMO is extremely wide and takes over an hour and a half to load load into DOMO',\n",
       "  '186\\nDomo Data Governance\\nOverview:\\nThis document outlines the process for gathering governance data from Domo and reporting it to the Perpay team. \\nPurpose:\\nDomo provides internal governance data that we can analyze to produce suggestions on inactive resources within Domo. This information \\nwill allow for informed removal of unused cards, datasets, columns, etc. within the Domo instance.\\nAbout the data:\\nWithin daily Governance DAG:\\n1. Internal governance data is generated by Domo’s “Governance Datasets” and  “DomoStats” Third Party APIs once a day\\n2. This data is pulled into redshift from Domo once a day, and then formatted to be useful. \\n3. Key Definitions:\\na. Page View: counted each time a page renders\\nb. Card View: counted each time a user clicks into the Details view for a card\\nAd-Hoc Domo Governance DAG:\\n1. Internal governance data is generated by Domo’s “Governance Datasets”',\n",
       "  '445\\nThe performance file is ❌ .\\nMarketplace Data Models\\nIn general, none of these should be affected by cards in the wild because, well, this is the marketplace. Regardless, I’ve scoped them out to \\ncheck.\\nBorrower Attribute\\nEnrollments are used to determine if the borrower is enrolled in pp+. This happens in a few spots. \\n12        a.borrower_id,\\n13        a.card_account_id,\\n14        cast(a.created_ts as date) as first_card_provisioned_dt,\\n15        CASE\\n16            -- !!!!!!!!!!!!!!!!!!! This is where the ts is used as a comparison\\n17            WHEN (fe.first_card_enabled_ts <= gbf.created OR gbf.created is null) THEN 0 \\n18            ELSE 1 \\n19            END AS marketplace_borrower_at_enabled_ind,\\n20...\\n21    FROM\\n22        {{card_schema}}.borrower_fact a\\n23        LEFT JOIN {{report_schema}}.borrower_fact gbf ON gbf.borrower_id = a.borrower_id\\n24        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status b ON a.borrower_id = b.borrower_id AND cast',\n",
       "  \"51            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n52            and b.status = 'pending'\\n53        ) as g on g.borrower_id = a.borrower_id\\n54    group by a.borrower_id;\\n1create table {{temp_schema}}.plus_funnel_fact_1 as\\n2    select coalesce(b.user_id, e.id) as user_id,\\n3           min(a.server_received_time) as first_engagement_ts,\\n4           d.first_plus_eligible_dt,\\n5           f.status\\n6    from {{marketing_schema}}.amplitude as a\\n7    left join {{marketing_schema}}.amplitude_user_mapping as b on b.amplitude_id = a.amplitude_id\",\n",
       "  'only serves as a destination\\nCannot be a destination in DMS, would need to use a separate \\nCDC service\\nPanoply as a \\ndata warehouseSits on top of S3 as a way to query the \\ndata lakeThis would not be a data lake solution, but an accessibility \\nsolution to an S3 (spectrum) data lakePlatform Pros Cons',\n",
       "  '(38,2)The present account balance. This \\nvalue only populates on the \\nobservation row for the current dateCalculating \\nDaily \\nBalances\\nvariance Numeric\\n(38,2)The difference between the ending \\nbalance and the current balanceAccount \\nLevel \\nVariances',\n",
       "  'Date of last payment\\nPerpay Plus Enrollments: \\nThere will be Trello cards created by Ops of users requesting to cancel Perpay Plus. Most of these will be users who opted in to Perpay Plus \\non accident; we want to keep the option to opt back into Perpay Plus open if the user has not been already reported. There are two courses \\nof action:\\n1) If a user is already reported, we must cancel the enrollment.\\n2) If a user has not been reported, we can delete the enrollment as if it never happened.\\nEngineering will ask analytics (Sarah) to check if the user is reported. The action here is to run the query below. If there is data returned, \\nadvise action 1) (enrollment cancellation). If not, advise action 2) (deletion of enrollment).\\nField Values:\\n1/*** SEE ALL DATA BY EMAIL ***/\\n2select a.*, b.email\\n3from perpay_risk_datamart.equifax_raw_data as a\\n4left join perpay_general_datamart.user_attribute as b on b.borrower_id = a.consumer_account_number',\n",
       "  '272\\nSecurity Groups\\nAirflow Connection\\nOnce the service is up and running, find its URL for the GMS container which will look something like: http://X.X.X.X:8080. We will then go to \\nAirflow’s UI, select Connections from the Admin tab, and create a new connection like so:\\nUpdate the Server Endpoint with the URL, and you should create an access token from the Datahub UI with an admin account, click the little \\ncopy button (otherwise you’ll have formatting issues) and paste into the Password box. Test the connection, if it works, all is well. \\nRedshift Permissions For Datahub\\nUse the following code snippet to create the datahub user in Redshift and be sure to save the <PASSWORD> in 1Pass. These allow \\nDatahub the access necessary to ingest metadata from Redshift.\\nNext, we need to give Datahub permission to do column-profiling for which it needs SELECT permissions in Redshift. \\n1. Start by running the following query against the Redshift database of interest:',\n",
       "  '187\\n \\nDomo Governance Cards (domo_gov_cards_full):\\nDomo Governance Datasets & Cards (domo_gov_datasets_cards):\\nDomo Governance Pages and Cards (domo_gov_pages_full):\\n Domo Governance Credit Usage (domo_gov_credit_usage_hist):redshift_table_name character varying Name of the table in Redshift\\ndomo_data_set character varying Name of the table in Domo, if one \\nexists\\ncard_title character varying Name of the card in Domo\\ndomo_card_id integer ID of the card in Domo\\npage_name character varying Name of the page on which the \\ncard falls in Domo\\nlast_touched_ts timestamp Time at which the card was last \\nviewed, updated, or sharedVariable Name: Data Type: Description:\\ndataset_id integer ID of the dataset in Domo\\nname character varying Name of the dataset in Domo\\ndataset_last_touched_ts timestamp Time at which the dataset was last \\ntouched\\nhas_cards_ind integer A flag indicating whether the \\ndataset has cards built on it in \\nDomoVariable Name: Data Type: Description:',\n",
       "  'have hit a canceled status on this day, \\nresulting in the payments made up to \\nthis point being added back to the \\nuser’s core account balancePayment \\nTracking\\npayment_perpay_plus_completed Numeric\\n(38,2)Feature payments that have been \\ncompleted, resulting in the amount \\nbeing deducted from the user’s core \\naccount balancePayment \\nTracking\\nending_balance Numeric\\n(38,2)The balance resulting from all balance \\nmovement being applied to the day’s \\nstarting balanceCalculating \\nDaily \\nBalances',\n",
       "  'change in credit score that is statistically significant in the test is 3.8%, which can only be achieved with a 50/50 split of those reported and \\nnot reported to Experian. Therefore, at any point in time during the test, 50% of the T4 Stage II population will be reported to Experian.\\nThe list of borrowers who are sent to Experian is stored in perpay_risk_datamart.experian_borrowers where the \\nsend_to_experian_flag = 1. Accounts that are merged that no longer have loans associated to them will be deleted from Experian.\\n--\\nBorrower Rollout (DEPRECATED):\\nSee note in above section about Perpay Plus.\\n--\\nEventually, we want to send all borrower data to Experian. There are two possible rollout plans:\\nOption 1: Continue to only report T4 Stage II until the test is proven out. This would entail:\\nProving that reporting T4 borrowers to Experian does improve credit score or have positive effects for Perpay.',\n",
       "  'and data models it contains. Adding descriptive information to any schema accessed by stakeholders will give them a clearer understanding \\nof the commonality and representation of all data assets stored within. Below are the required pieces of information that should be added to \\nthe description field. If there are relevant glossary terms or subject matter experts/owners, please add them via the selection boxes.\\n1. Product Focus\\na. Card, marketplace, platform, perpay plus, etc\\n2. Data Sources\\na. What is/are the source(s) of the data that populate the tables within the schema? Is the data coming from Core, Deserve SFTP, \\nsegment, a combination?\\n3. Use Case\\na. What is the purpose of the data models/tables in the schema? Are they used for credit reporting, domo reporting, general analytics, \\ntracking lapsed signup assignments?\\n4. Description\\na. A medium length high level description of nature of the schema that highlights any additional context/caveats of the data within that',\n",
       "  \"9    from public.card_account_snapshot\\n10)\\n11select * from balance_1 where card_account_id = 5078;\\n12\\n13\\n14-- Check the second CTE: this is usually where the issue is\\n15with\\n16id_xwalk as (\\n17    select\\n18        borrower_id ,\\n19        user_id ,\\n20        card_account_id\\n21    from atomic_int .card_key_relations\\n22    where card_account_id is not null\\n23    group by borrower_id , user_id , card_account_id\\n24)\\n25select * from id_xwalk where card_account_id = 5078;\\n1with\\n2    staff as (\\n3        select\\n4            id as user_id\\n5        from public.user g\\n6        where g.email in ('jarrett.julieanna@gmail.com' , 'derek+deservealpha@perpay.com' , 'horanconor@aol.com' , \\n7    ), enabled as (\\n8    select\\n9        distinct  borrower_id\\n10    from public.feature_enrollment\\n11    where feature_id = 67 and status = 'enabled'\\n12), arik_hard_code as (\\n13    select 5283013 as borrower_id , 1 as card_account_id , 1 as card_id\\n14)\\n15\\n16select\\n17    distinct\\n18    d.id as borrower_id ,\",\n",
       "  'from using API calls as well.\\nDAG Not Running \\nOften something in data model full will break just because of how many tasks exist in the DAG. When this happens the DAG will fail and \\nIterable will not be updated again until the next time that the data model full DAG runs. When a single GE test fails, we do not necessarily \\nwant the entire DAG to fail. Implementing logic to allow the GE tests to warn without failing would allow the Iterable data to be updated much \\nmore frequently. \\nDomo Tasks\\nNot every task in data model full contributes to the data that eventually is transported to Iterable. Some of the tasks are only needed for the \\ncreation of the Domo reports that data model full is also responsible for. These tasks only exist in the data model full because they have \\ndependencies on other tasks within the DAG. This set of tasks does not need to run before the data is ready to be picked up by Census and',\n",
       "  '197\\nThere are 2 types of 3rd-party integrations we tackle; we input data from a third part or we send data to a 3rd party (or we alloy them to \\naccess it). Thought he data flow is in opposite directions for each of those tasks, the requirements for a completed project are largely \\nidentical.\\nCorrect permission’s are set up where any 3rd party has access to our data\\nTests and error handling around API integrations or the creation of the exposed data\\nLogic tested and validated in a staging environment to ensure integration does not contain any breakages\\nStake holders are briefed of integration before the solution is deployed and documentation of the integration is added to confluence\\nData Models\\nThe data models that are produced and maintained is the heart of the DE group. As new initiatives are launched, it is regularly necessary to \\nadd more functionality to these models so that the company can keep a tight grasp on how our new products are fairing out in the wild.',\n",
       "  'ID). Below are the recommended elements to include in Glossary entries while the normal additional owner and tags values can be selected \\nas well.\\n1. Description (required)\\na. A short to medium form description of the term and how it pertains to Perpay. What are its rules? When does it change? What affects \\nit?\\n2. Project Links\\na. Please add links to any relevant projects that provides greater detail on the term. And example is the underwriting risk score where a \\nlink to the data science project documentation would be natural to add so that the user can have more background on the concept if',\n",
       "  '351\\nab_test_backend_signup_report.py\\nexpected_company_pay_dates.ipynb (check with DS)\\n& more\\nHighlighted files:\\n1. I think it’s best to start with the two metric layer files. These feed into most of our data models so it’s starting as close to the source as \\npossible.\\na. card_ach_payment_tracking.py: I actually don’t think anything needs to change here but someone should double check me on that. \\nFor this, the auto pay payment information shouldn’t have changed. It will still populate when we receive the payment. It’ll be \\nimportant to note how the card_payment_created and card_payment_modified dates are used downstream, however, since those \\ncome from the public.card_cardpayment which is filled earlier than before.\\ni. one thing I didn’t realize is there is also a public.payment_achtransaction table. I’m assuming this is populated at the same \\ntime as the public.auto_pay_payment: edit: The ach transaction table represents the money we get from jpmorgan (for the',\n",
       "  \"36        LEFT JOIN {{atomic_schema}}.platform_job b ON a.borrower_id = b.borrower_id AND b.current_primary_ind = \\n37        LEFT JOIN {{pub_schema}}.company c ON c.id = b.company_id\\n38        LEFT JOIN pinwheel_company_max_date_cte f ON f.company_id = c.id\\n39        LEFT JOIN pinwheel_company_cte d ON c.id = d.company_id AND f.max_history_date = d.history_date\\n40        LEFT JOIN pinwheel_provider_max_date_cte g ON c.payroll_provider_id = g.payroll_provider_id\\n41        LEFT JOIN pinwheel_payroll_cte e ON c.payroll_provider_id = e.payroll_provider_id AND g.max_history_date\\n42        LEFT JOIN rdd_applied h on a.user_id = h.user_id\\n43        -- !!!!!!!!!!!!!!!! Left join- not cutting our population size\\n44        LEFT JOIN feature_enabled fe on a.borrower_id = fe.borrower_id\\n45);       \\n1...    \\n2    SELECT\\n3        cast(cdt.borrower_id AS varchar(16)) || '_' || cast(di.src_dt AS varchar(16)) AS join_key,\\n4        cdt.borrower_id,\\n5        cdt.card_account_id,\",\n",
       "  '356\\nProjects\\nThis is a page for large-project documentation. \\nSome have been completed, some are still WIP (work in progress), and others have yet to be done.\\nAll projects should be put in the Jira space.',\n",
       "  'Business Context:\\nEngineering is able to initiate customer transaction activity (the flow of funds in and out of one’s card account), but the actual \\nincrementing or decrementing of the card account balance occurs on Deserve’s side. Once Engineering initiates a payment, withdrawal, \\nreturn- whatever it may be- Deserve changes the card account balance and sends Engineering webhooks with that updated balance. \\nEngineering then updates their tables to reflect the change. \\nRollforward Table Logic:\\nBecause this field reflects balance at the start of the day, when we pull current_balance from the source tables below, we do so on the \\nday prior, which reflects the card account balance at end of the day prior/the beginning of the current day.\\nSource Tables:\\nThis comes from atomic_int.card_balance, which pulls current_balance from webhooks first through the \\npublic.card_account_snapshot table. If not there, it uses current_balance from the deserve_accounts_eod_report SFTP. \\nDefinition:',\n",
       "  'staging and prod would be good to go over as well (some things are not straightforward, such as the spectrum s3 bucket is created in \\nthe reshift directory, not the bucket directory).\\n7. DE platform architecture plans for the future! (Medallion)\\na. This really is an overview of how we are moving to using the metric layer tables as the “core” tables in building data models, isolating \\nthe production environment, and updating our testing structure to be uniform and adjustable with respect to error criticality.\\n8. How we currently handle errors - when we need to step in\\na. Overview of the kinds of errors we have to deal with every day. When to reach out to engineering, when to step in and fix something, \\nhow we (currently) approach divvying up taking errors, when its necessary to step in after hours, etc. It would also be good to walk \\nthrough some of the confluence documentation that has been put together (thank you Abby!) that goes over our common trouble \\nshooting.\\nOnboarding',\n",
       "  \"587\\nLambda function to convert excel --> csv file types\\nFor the Accounts Payable project, we sometimes receive invoices in excel format. Unfortunately, AWS is unable to crawl these files directly - \\nwe will need to convert them CSV files first. We can do this very simply using the Pandas package in Python.\\nIn an ideal world, we would do this in an event-driven manner; i.e., whenever we receive a new excel file from a vendor, the python code \\nthat transforms this to a CSV is automatically run. A lambda function in AWS that is triggered by an S3 bucket event is perfect for this! \\nTo make an S3 event triggered lambda function, there is extremely good documentation by AWS found here.\\n \\nUnfortunately, most python packages are not automatically available for a lambda function to use. You use a deployment package to deploy \\nyour function code to Lambda. The deployment package acts as the source bundle to run your function's code and dependencies (if\",\n",
       "  '625\\nTerraform\\nVideo topic ideas:\\nwhy do we have terraform?\\nwhat is a terraform plan? what is terraform drift?\\ngeneral overview of writing infrastructure-as-code in terraform (making resources, attaching them to each other, etc.)',\n",
       "  'only serves as a destination\\nCannot be a destination in DMS, would need to use a separate \\nCDC service\\nPanoply as a \\ndata warehouseSits on top of S3 as a way to query the \\ndata lakeThis would not be a data lake solution, but an accessibility \\nsolution to an S3 (spectrum) data lakePlatform Pros Cons',\n",
       "  'The files in which the events were referenced\\nThe no. of references to each event in each file / across the entire repo\\n(See README for documentation on how to use the script)\\namplitude_event_url_resolution.py - searches through the list of Amplitude events that have Segment counterparts, and figures out \\nwhether their affiliated tables contain URL/UTM information. The script produces an Excel file with multiple sheets that respectively \\ncontain Amplitude events that should be kept, removed and manually checked. \\nThe two scripts each come with a command-line progress bar to show search progress:\\nRemoving Amplitude Events\\nAfter running amplitude_event_reduction.py to find all references to Amplitude events in our codebase, we then ran \\namplitude_event_url_resolution.py to figure out which events are affiliated with tables that contain URL/UTM information. \\nManual checks were then performed to determine which events we can keep',\n",
       "  \"8    left join perpay_general_datamart_int.user_fact b on a.borrower_id = b.borrower_id\\n9    left join perpay_general_datamart_int.user_attribute c on b.user_id = c.user_id\\n10    full outer join (\\n11            select distinct a.borrower_id\\n12            from public.feature_enrollment as a\\n13            left join public.feature as b on b.id = a.feature_id\\n14            where b.type = 'credit_reporting'\\n15        ) as d on d.borrower_id = a.borrower_id\\n16    where (c.blacklisted_ind in (0) or c.blacklisted_ind is null)\\n17    and (a.borrower_status not in ('ChargedOff') or a.borrower_status is null);\\n18...\\n1...\\n2card_enabled as (\\n3  select \\n4    b.user_id,\\n5    a.created as created_ts,\",\n",
       "  '390\\nDE Notes on Initial Core Discrepancies\\nPayment Detail Amounts Do Not Sum to Payment Amount\\nCases where the sum of all payment details associated with the payment does not total to the actual payment amount.\\nMissing Micro Deposit on Balance\\nCases where a micro deposit was made by the user, but no corresponding payment was made with the micro deposit and the deposit does \\nnot reflect on the user’s core balance.\\nMissing Payment Detail for Out of Stock Credit\\nCases where an out of stock credit was redeemed by a user, but there is no corresponding payment detail row for the use of the borrower \\ncredit.1 0 0 4 8 6 7 4 1 6 9 4 6 2\\n2 0 1 0 1 6 0 3 5 6 5 3 9 9\\n2 5 2 7 7 5 6 4 1 2 1 6 5 5\\n4 8 2 3 8 0 0 4 6 5 2 6 9 3User ID with Discrepancy Payment ID\\n1 8 3 1 3 5 0 1 4 6 8 3 1 5\\n2 6 1 7 1 5 3 1 4 7 6 1 8 6\\n8 0 0 0 2 0 8 4 1 0 7 9 5 7 \\n8 3 3 4 5 7 6 4 3 6 0 2 9 6 User ID with Discrepancy Deposit ID\\n8 0 5 7 3 0 8 1 2 8 6 9 2 8 3\\n8 0 7 9 9 5 4 1 2 7 0 7 3 6 2\\n8 1 1 9 7 6 6 1 2 9 4 1 6 3 9',\n",
       "  \"6    raise Exception (f'[ImpactApi] Issue with Action Report - no data returned for yesterday!' )\\n7\\n8  # Write df to the impact_actions table\\n9  self.spectrum .write_df_to_internal (resulting_df , self.spectrum .get_schema_by_env (constants .TEMP_SCHEMA ), 'impact_actions' , varchar_length =65000)\\n10  self.spectrum .create_external_prod_table_as (\\n11      table_name ='impact_actions' ,\\n12      write_schema =self.spectrum .get_schema_by_env (constants .PERPAY_MARKETING_DM_SCHEMA ))\",\n",
       "  'to the associated account_id. \\nPending Collections Deposits: Pending collections deposits are sourced from rows in the deposit table with a content_type_id of \\n146 and a status of valid. The amount is considered to be a pending collections deposit on the date of the created timestamp, and it \\nis attributed to the associated account_id.\\nCompleted Credit Card Deposits: Completed credit card deposits are sourced from rows in the deposit table with a content_type_id \\nof 22 and a status of valid. The amount is considered to be a completed credit card deposit on the date of the created timestamp, \\nand it is attributed to the associated account_id.\\nCompleted Direct Deposits: Completed direct deposits are sourced from rows in the deposit table with a content_type_id of 46. The \\namount is considered to be a completed direct deposit on the date of the created timestamp, and it is attributed to the associated \\naccount_id.',\n",
       "  '123\\nCommon Errors\\nThis folder is to serve as the basis of knowledge to correct errors we see all the time. It includes known error messages, solutions, and \\nsome suggestions to keep these errors from occurring again! \\nThe inclusion of ( E r r o r ) in each page title under this folder is for search-bar optimization purposes. When you search a topic, it allows for an \\neasier distinction between these error handling pages and the generic documentation via title. Please include it when you add a new page to \\nthis folder.',\n",
       "  'return- whatever it may be- Deserve changes the card account balance and sends Engineering webhooks with that updated balance. \\nEngineering then updates their tables to reflect the change. \\nRollforward Table Logic:\\nBecause this field reflects balance at the end of the day, when we pull current_balance from the source tables below, we do so on the \\ncurrent day, which also reflects the card account balance at end of the day.\\nSource Tables:\\nThis comes from atomic_int.card_balance, which pulls current_balance from webhooks first through the \\npublic.card_account_snapshot table. If not there, it uses current_balance from the deserve_accounts_eod_report SFTP. \\nDefinition:\\nWhat DE believes the ending balance should be after taking into consideration the flow of all relevant customer funds within the day.\\nBusiness Context:\\nIn Engineering terms, a customer has a card account balance if they have funds available to withdraw from their card account.',\n",
       "  '173\\n(code for reference)\\n \\nReferring to the code above, we can analyze how easy_render.py works and what it did well and what it fell short of. Lets first take a look \\nat what it does well and the logic we can migrate:\\n1. Class Design: Continued use of a class (CombinedRenderer) to encapsulate rendering functionality, promoting modularity and \\nreusability.\\n2. Environment-Specific Logic: Retained logic for handling different schemas based on environment variables, maintaining adaptability to \\ndifferent deployment settings.\\n3. Local Configuration: Continued dynamic import of local configurations after paths are set, ensuring context-specific configurations.\\n \\nThese are all things I wanted to add to my new implementation in combined_render.py, however there are also some things that are \\nmissing from the code that needed to be added:\\n1. Lack of Error Handling: Does not handle potential errors, such as missing files, invalid paths, or rendering failures, which could lead to',\n",
       "  \"update it (more info here- it’s scenario 1)\\n3: Creates a table for each remaining parser in the temp_int schema with the name of docparser_<vendor name in \\nlowercase>_<todays date with underscores> (for instance, temp_int.docparser_almo_2023_01_12). \\n4: Logs '[DocparserApi] Writing formatted dataframe to Redshift table {table_name}...'In short, this job does 4 things: \\n1. It parses the data in Docparser (typically all vendors from yesterday onwards, but can be specified differently) and \\nsaves it to the perpay_accounting_datamart_ext.docparser_invoices_master table.\\n2. It clears the data in the Invoice Matching Google sheet (Invoice Matching v2 sub sheet specifically)\\n3. It re-uploads to the the Invoice Matching v2 sub sheet with only the variance data that had ready_for_qb = 'N'\\n4. Upload the invoices that have not already been paid and are also not in the variance sheet into Quickbooks\\nSpecifics about the job can be found below:\",\n",
       "  '8and outputs the result.\\n9Usage:\\n10  python3 -m dags.scripts.combined_render <file_path> [--params \\'{\"key\": \"value\"}\\']\\n11Examples:\\n12  For an SQL file:\\n13    python3 -m dags.scripts.combined_render dags/bi_reporting/commerce/storefront_tracking_clicks.py\\n14        **** OR *****\\n15    python3 -m dags.scripts.combined_render dags/bi_reporting/commerce/storefront_tracking_clicks.sql --para\\n16\\n17  For a Python file:\\n18    python3 -m dags.scripts.combined_render dags/bi_reporting/commerce/storefront_tracking_clicks.py\\n19\"\"\"\\n20\\n21\\n22import sys\\n23import argparse\\n24import importlib .util',\n",
       "  '223                        \"category_path_1\": \"electronics\",\\n224                        \"category_path_2\": \"electronics/computers\",\\n225                        \"category_path_3\": \"electronics/computers/ipads-tablets\",\\n226                        \"category_path_4\": NULL,\\n227                        \"category_path_5\": NULL,\\n228                        \"category_path_6\": NULL,\\n229                        \"visits\": 17\\n230                    },\\n231                    {\\n232                        \"category_path_deep\": \"fashion-beauty/personal-care\",\\n233                        \"category_path_1\": \"fashion-beauty\",\\n234                        \"category_path_2\": \"personal-care\",\\n235                        \"category_path_3\": NULL,\\n236                        \"category_path_4\": NULL,\\n237                        \"category_path_5\": NULL,\\n238                        \"category_path_6\": NULL,\\n239                        \"visits\": 10\\n240                    }\\n241                ]\\n242            },',\n",
       "  \"31        INNER JOIN random_sample b\\n32            ON a.borrower_id = b.borrower_id\\n33    WHERE\\n34        a .status IN ('completed' )\\n35        AND a.destination_rsn_cd = 0\\n36        AND convert_timezone ('UTC','EST',created_ts )::date like '2024-03-%'\\n37    GROUP BY 1, 2\\n38),\\n39-- Capture all rewards that were redeemed for the borrowers in March\\n40rewards_redeemed_unsummed as (\\n41    SELECT\\n42        b .borrower_id ,\\n43        d .email,\\n44        a .account_id ,\\n45        a .sftp_dt,\\n46        a .rewards_redeemed_usd\\n47    FROM\\n48        deserve_data_int .deserve_rewards_daily_summary_report a\\n49        INNER JOIN random_sample b on a.account_id = b.deserve_card_account_id\\n50        LEFT JOIN public.borrower c on b.borrower_id = c.id\\n51        LEFT JOIN public.user d on c.user_id = d.id\\n52    WHERE\\n53        a .sftp_dt like '2024-03-%'\",\n",
       "  \"587\\nLambda function to convert excel --> csv file types\\nFor the Accounts Payable project, we sometimes receive invoices in excel format. Unfortunately, AWS is unable to crawl these files directly - \\nwe will need to convert them CSV files first. We can do this very simply using the Pandas package in Python.\\nIn an ideal world, we would do this in an event-driven manner; i.e., whenever we receive a new excel file from a vendor, the python code \\nthat transforms this to a CSV is automatically run. A lambda function in AWS that is triggered by an S3 bucket event is perfect for this! \\nTo make an S3 event triggered lambda function, there is extremely good documentation by AWS found here.\\n \\nUnfortunately, most python packages are not automatically available for a lambda function to use. You use a deployment package to deploy \\nyour function code to Lambda. The deployment package acts as the source bundle to run your function's code and dependencies (if\",\n",
       "  'Let’s look into the columns: rdd_success_ind, card_account_active_ind, open_to_buy_ind – these were put into place a long \\ntime ago and there is a ton of overlap now\\nrdd_success_ind: \\n0 when the pay_cycle is weekly and there was an rdd_success beyond/after grace period (3 days) + 7 days = 10 days\\n0 when the pay_cycle is semi monthly or bi_weekly and there was an rdd_success beyond/after grace period (3 days) \\n+ 14 days = 17 days\\n0 when the pay_cycle is monthly and there was an rdd_success beyond/after grace_period (3 days) + 30 days = 33 \\ndays\\n1 otherwise!! (meaning an rdd_success was within the grace_period + timeframe given the pay_cycle)\\ncard_account_active_ind: \\n0 when the deserve card account status is suspended\\n1 otherwise (the card account is considered active if it was not suspended by deserve) → changed this to be 1 when \\nthe account status is ACTIVE, 0 otherwise…. unless we’re trying to capture if the account is frozen or not!',\n",
       "  'Onboarding New Vendors (DE-specific) contains stepwise instructions to onboard new vendors through the process in Google, \\nZapier, and the code.\\nLastly, Troubleshooting AP Revamp explains some of the common errors that may be thrown and how to address them.',\n",
       "  '625\\nTerraform\\nVideo topic ideas:\\nwhy do we have terraform?\\nwhat is a terraform plan? what is terraform drift?\\ngeneral overview of writing infrastructure-as-code in terraform (making resources, attaching them to each other, etc.)',\n",
       "  \"11       highest_credit_or_original,\\n12       terms_frequency,\\n13       actual_payment_amount,\\n14       current_balance,\\n15       amount_past_due,\\n16       original_charge_off_amount,\\n17       date_last_payment\\n18from perpay_risk_datamart.equifax_raw_data as a\\n19left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n20where (payment_history_profile like '%1%' or payment_history_profile like '%2%' or account_status != '11' or pay\\n21and enter_date = max_enter_date\\n22order by account_status;\\n23\\n24------------------------------------\\n25-- OPT INS\\n26-- The min_enter_date filter is getting perpay plus opt ins that are newly reported\\n27select\\n28    distinct\\n29       a.consumer_account_number,\\n30       date_of_account_info,\\n31       enter_date,\\n32       payment_rating,\\n33       payment_history_profile,\\n34       account_status,\\n35       credit_limit,\\n36       highest_credit_or_original,\\n37       terms_frequency,\",\n",
       "  'then can easily be turned into a metric layer by serializing the table by company and date, and then pulling out the first row for each \\ncompany and all of the subsequent rows for which the Pinwheel eligibility changes.\\nTesting\\ntest_for_successive_rows_with_no_difference_in_eligibility tests that for each individual company, Pinwheel eligibility changes \\nfor successive records. test_for_eligibility_in_first_row checks that the chronologically first record for each company indicates that \\nthe company is eligible for Pinwheel. \\nValidation\\nValidating with Core Eligibility Table',\n",
       "  'When DMS copies tables from one redshift db to another in a continuous mode, it does so in batches. Over a set time period, data is copied \\nfrom the source and stored in memory on the DMS instance. After a the copy time is concluded, it then sends the batched data to the target. \\nBoth processes incur i/o time. If the batched data gets too large, it will not only take more time and incur latency but also eat up more \\nmemory on the box. Once ram has been totally used, it moves to hard disk and eventually swap. At time all memory is maxed out, errors will \\nbegin to occur in the data and the instance will completely fail to update tables. \\nOn 2022-10-11, the ana-prod-core-to-redshift-taxing-tables-task fell over due to the exact issue above. By viewing the performance metrics \\nin the console for the task and the instance it was clear the latency and errors were being driven by the high memory usage. The immediate',\n",
       "  'missed in the upload process. This should be double checked by doing a search in QB and manually uploaded if it’s true (you can \\nask accounting to manually upload them, just provide the invoice_numbers). A message will be sent to the alerts channel.\\n5: Backup Measures\\nWe have some backup measures in place for when things go awry. Specifically, we implemented a task that can be triggered using the \\n{\"delete_qb_duplicates\": 1} config to delete duplicated bills from QB. It’s logic lies in the delete_duplicated_bills() function in \\nquickbooks_api_class.py and utilizes a POST request. We rely on Fivetran data to decide which bills to submit through the POST request \\nfor deletion. If one or more bills in the duplication cluster have been paid, this deletes the remaining unpaid bills in that cluster. If no bills \\nhave been paid, all but one are deleted, where the one kept is chosen at random.',\n",
       "  '5. Engineering sends this information to Deserve (not sure the specifics behind this, just that the creation of the payment in the \\npublic.card_cardpayment table triggers something on Deserve’s side) so Deserve knows to start sending webhooks about the \\npayment\\nNew Logic\\nThis is the logic beyond 05/08/23.\\n1. Engineering receives an ACH payment from JP Morgan\\n2. Engineering creates an autopay payment (new record) in the public.auto_pay_payment table\\n3. Almost simultaneously, Engineering creates a payment (new record) in the public.card_cardpayment table (FK is card_payment_id in \\nthe public.auto_pay_payment table)\\na. The borrower’s card account balance is updated now and the borrower can see this on their side.\\n4. Engineering sends something to Deserve letting them know a payment has been made with a flag whether or not to hold the payment \\nout (whether it’s an ACH payment)',\n",
       "  '618\\nDocker Containers\\nVideo topic ideas:\\nWhy do we use Docker for Airflow?\\nWhat are all of these different containers (worker, scheduler, webserver, notebook, redis)?\\nWhen would you need to edit the docker files?\\nHow do we bring the containers up and down, and why would we need to?',\n",
       "  \"18)\\n19SELECT DISTINCT  CONCAT('ALTER DEFAULT PRIVILEGES IN SCHEMA ' , CONCAT(schema_name , CONCAT(' REVOKE ALL PRIVILEGES\\n20\\n21\\n22-- Default privs for users\\n23WITH \\n24user_user_acl AS (\\n25SELECT * FROM svv_default_privileges WHERE grantee_type ='user' AND schema_name IS NULL\\n26)\\n27--SELECT * FROM default_acl;\\n28SELECT DISTINCT  CONCAT('ALTER DEFAULT PRIVILEGES FOR USER ' , CONCAT(owner_name , CONCAT(' REVOKE ALL PRIVILEGES O\\n1WITH\\n2user_schema_privs AS (\\n3SELECT DISTINCT  namespace_name , identity_name FROM svv_schema_privileges WHERE identity_type ='user' AND namespac\\n4)\\n5SELECT DISTINCT  CONCAT('REVOKE ALL PRIVILEGES ON SCHEMA ' , CONCAT(namespace_name , CONCAT(' FROM ' , CONCAT(identi\\n6\\n7-- Tables\\n8WITH\",\n",
       "  \"31            if datetime .strptime (str(year)+ '-12-31' , '%Y-%m-%d' ) < end_dt:\\n32                end_date = str(year) + '-12-31'  + 'T17:23:59Z'\\n33            else:\\n34                end_date = str(end_dt.strftime ('%Y-%m-%d' )) + 'T17:23:59Z'\\n35            url_addition = f'ReportExport/adv_action_listing_pm_only?START_DATE= {start_date }&END_DATE= {end_date }&SUBAID= {program_id }&SHOW_AD=1'\\n36            (url, auth) = self.utils.construct_url_and_auth (url_addition )\\n37\\n38            logging .info(f'[ImpactApi] Getting data for year {year}')\\n39            # Performs the GET request\\n40            response = requests .get(url, auth=auth, headers ={'accept' : 'application/json' })\\n41            response = self.utils.check_response_status (auth, response , url)\\n42\\n43            # The GET request above returns another URL. We hit the endpoint again using the returned URL to download the CSV\",\n",
       "  'meeting notes keep a brief running log of what the current outstanding asks are:\\nThat said, when it’s time for the rotation to change over it’s probably worth taking five minutes to catch the next person up on what the \\ncurrent progress is.\\nProbably doesn’t need to be said, but the credit reporting code requires a lot of precision, so when making the updates requested or when \\nupdating the code for any reason, be sure to test thoroughly!\\nOperations Adhoc Questions\\nThe ops team generally handles credit disputes that come in. One of our customers can dispute how their trade line is being reported with \\nPerpay, and ops will resolve the dispute and enter the data we want to report for the customer into E-Oscar. Occasionally ops will reach out \\nto clarify how we came to a certain data point.',\n",
       "  '528\\nRisk Ext Interns:\\n--- \\nThere are also project-based groupings for particular cases\\nCredit Reporting Project:  @Kevin Goshorn @Zach Binkley \\nIntern Projects: \\nIdentity Graph Interns:\\nDE Interns Testing Project: @Gabe Garcia \\nCard Investigations Project: @Conor Horan \\nDS Training: \\n = Current, \\n = No Access, \\n = Unsure\\nSch\\nemaAir\\nflo\\nwad\\nmi\\nnper\\npa\\ny_\\nad\\nmi\\nnDE \\nSup\\neru\\nser\\nsDE \\nUs\\nersDE \\nInt \\nInt\\nern\\nsDE \\nExt \\nInt\\nern\\nsDS \\nSup\\neru\\nser\\nsDS \\nUs\\nersDS \\nInt \\nInt\\nern\\nsDS \\nExt \\nInt\\nern\\nsDA \\nUs\\nersDA \\nInt \\nInt\\nern\\nsDA \\nExt \\nInt\\nern\\nsRis\\nk \\nUs\\nersRis\\nk \\nInt \\nInt\\nern\\nsRis\\nk \\nExt \\nInt\\nern\\nsCr\\nedi\\nt \\nRe\\npo\\nrti\\nng \\nPr\\noje\\nctDE \\nInt\\nern\\ns \\nTes\\ntin\\ng \\nPro\\njectCard \\nInve\\nstiga\\ntions \\nProje\\nctDS \\nTra\\nini\\nngInt\\nern \\nPr\\noje\\nctsIde\\nntit\\ny \\nGra\\nph \\nInte\\nrns',\n",
       "  \"11       highest_credit_or_original,\\n12       terms_frequency,\\n13       actual_payment_amount,\\n14       current_balance,\\n15       amount_past_due,\\n16       original_charge_off_amount,\\n17       date_last_payment\\n18from perpay_risk_datamart.equifax_raw_data as a\\n19left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n20where (payment_history_profile like '%1%' or payment_history_profile like '%2%' or account_status != '11' or pay\\n21and enter_date = max_enter_date\\n22order by account_status;\\n23\\n24------------------------------------\\n25-- OPT INS\\n26-- The min_enter_date filter is getting perpay plus opt ins that are newly reported\\n27select\\n28    distinct\\n29       a.consumer_account_number,\\n30       date_of_account_info,\\n31       enter_date,\\n32       payment_rating,\\n33       payment_history_profile,\\n34       account_status,\\n35       credit_limit,\\n36       highest_credit_or_original,\\n37       terms_frequency,\",\n",
       "  '9\\nUsage and Contribution\\nDatahub (production location, staging location) serves as the central data catalog for not only the data teams but for Perpay as a whole. This \\nresource provides not only table and column-level information for data held within Perpay’s data warehouse, but also the lineage of the \\nupstream assets used to create the table/column and higher-level conceptual information. The platform is also connected to the data \\nwarehouse and orchestrator to ensure that what is being presented in Datahub is currently representative of the data environment.\\nAlthough schemas, tables, lineage, and other metadata can be automatically pulled in from the machinery that builds our data assets and \\nservices, the contextual information needs to be inputted by us measly humans (and/or AI-assisted). Therefore, this document outlines the \\nstandard operating procedure and style considerations for any DRAAFT team member to contribute to Datahub in order to maintain',\n",
       "  '432\\nPermissions Restructure\\nPermissioning Groups and Levels\\nDepartments\\nSecurity Levels\\nOutline of Schema Permissions\\nAction Plan\\nAppendix\\nPreliminary Research on Redshift Permissioning\\nQuirks of The `public` Schema\\nA Note On Superusers\\nQueries For Generating Queries To Revoke Old Privileges\\n(Page under construction.)\\nAt the moment, the permissioning in our various data stores are not set up to have a sufficient level of granularity to support the growing \\nneeds of our organization. For example, the data engineering team can often have visiting team members such as interns or junior \\nmembers of other teams whose access to sensitive data should be fairly limited, however, we often give them the same permissions used \\nby more senior members of the team. Another similar problem arises when we consider the data needs of other teams: the Product \\ndepartment and the Marketing department are interested in different subsets of our data, their permissions should match these needs with',\n",
       "  '249\\nBackfilling Iterable Workflow IDs\\n1. Kick off adhoc_iterable_workflow_id_update_dag\\n2. Go to the iterable_shortio_export dag and clear the iterable daily insights tasks to rerun it\\n3. Rerun data model full (specifically, the marketing daily tasks-- Marketing_DailyDataModel_Fact which will be followed by \\nMarketing_DailyDataModel_SourceMetrics)',\n",
       "  '609\\nIntegrations',\n",
       "  '425\\ninto the tasks for the five individual synchs. Assuming that the API will work, then each synch can be triggered when the corresponding set \\nof tasks has finished running. One additional complication is that some of the synchs will need to run before the others. For example, we \\nknow for certain that the Account JSON needs to be updated first, and other dependencies may exist as well.\\nComparing Workflows\\nCurrent\\nFuture',\n",
       "  \"268\\nour customers adheres to these guidelines. Customers that have data that does not adhere to these guidelines flow into \\nrelational_m2_errors for card and marketplace_relational_m2_errors for marketplace. The tables contain a historical record of all \\nMetro2 errors that have ever been caught, so filter to the most recent run and you can see all the customers that were withheld and the \\nreason that they were withheld.\\nResolution\\nCheck out which accounts were withheld and their reason for being withheld. The raw data we would have sent had the accounts not been \\nwithheld can be found in experian_temp for marketplace and relational_m2 for card. Try to figure out why the account is being withheld. \\n1. Do we have an error in the logic that generates the account data? \\n2. Is the logic for the compliance or Metro2 check outdated or incorrect? \\n3. Is this a weird edge-case that we have never encountered before? '\\n4. Is there weird data in a public table?\",\n",
       "  'add more functionality to these models so that the company can keep a tight grasp on how our new products are fairing out in the wild.\\nDefinitions and data model structure is defined with stakeholders\\nNew data model logic is tested in staging with appropriate emphasis on readability (Jinja) and resource optimization\\nData validation tests are added for all vital features in the new model\\nThis can utilize great expectations as well as normal pytests \\nLogic is integrated into the production environment\\nDocumentation is added (or updated) for the new data model in data analytics space\\nHow do we prioritize projects?\\nBelow are a list of project and task types. They are listed in order of importance to how they should be addressed. Typically, multiple are \\nbeing tackled concurrently, however.  \\nImmediate, catastrophic issues with the data platform and or infrastructure\\nSlack is the main channel where issues are boiled up\\nLarge product-driven initiatives (ex pinwheel integration)',\n",
       "  'you can be sure you have the necessary \\ntime to address the underlying issues. \\nIntegration A simple health check on an integration \\nservice will indicate whether that service \\nis running. For more detailed monitoring, \\na metric monitor can be used to gauge \\nspecific information about an integration. \\nLive Process A process monitor is a health check which \\nsimply returns the status of matching \\nprocesses. This check is highly useful for monitoring \\nmany services running on multiple hosts. \\nWe don’t use this now, as we’re only \\nmonitoring a single host.\\nLogs Log monitors alert when a specified type \\nof log exceeds a user-defined threshold \\nover a given period of time. Common use cases for log monitors \\ninclude code exception errors or build job \\nnotifications.\\nNetwork A network monitor checks whether a \\ngiven endpoint is active. Monitors also \\ncan alert over a percentage on a cluster \\nbased on custom network tags defined \\nin the Agent and host tags.',\n",
       "  \"To approve anything that has a variance but is still ready to be inputted into QuickBooks, change the ready_for_qb column to 'Y'.\\nFeel free to add notes in the notes column; this will not affect the process. (These will appear in the Pending sheet after the next run).\\nIn the beginning of the job execution, rows in the DoInvoiceMatching sheet from the last job execution that are not ready for Quickbooks are moved to the Pending sheet. These rows \\nshould be reviewed.\\nTo approve anything that has already been looked into:\\nChange the ready_for_qb column to 'Y'\\nCopy/paste the row into the DoInvoiceMatching sheet\\nRemove the row from the Pending sheet\\nNote the entry_date column. This was when the row entered this sheet, giving you an idea of how long these invoices have been pending.\\nPart 3: Exporting to Quickbooks:\\nIn the following job execution, the reviewed data in part 2 that is marked as ready for Quickbooks will be formatted and added to the NewEntries sheet in the QuickBooks Bills\",\n",
       "  '249\\nBackfilling Iterable Workflow IDs\\n1. Kick off adhoc_iterable_workflow_id_update_dag\\n2. Go to the iterable_shortio_export dag and clear the iterable daily insights tasks to rerun it\\n3. Rerun data model full (specifically, the marketing daily tasks-- Marketing_DailyDataModel_Fact which will be followed by \\nMarketing_DailyDataModel_SourceMetrics)',\n",
       "  'one testing methodology should be used)\\npriority weight (optional): a numeric weight to further prioritize the task group in execution order\\nNon-Standard Task Groups\\nFor tasks that are not going to fit cleanly into the TempTestCommit paradigm, there are three SQLOperator classes \\n(DropSQLExecuteQueryOperator, RenameSQLExecuteQueryOperator, GenerateSQLExecuteQueryOperator) that can be used to initialize \\ncustom task groups. The DropSQLExecuteQueryOperator drops a given table_name in a given schema_name if it exists, and the \\nRenameSQLExecuteQueryOperator will rename the given {{schema_name}}.{{current_table_name}} to the given {{schema_name}}.\\n{{new_table_name}}. The GenerateSQLExecuteQueryOperator is a task that will execute some sql statement that is passed through the \\nsql param.\\nHere is one example of a custom task group that is used in the metric layer. It rebuilds the card activation event descriptions table if a rerun \\nparameter is passed in the DAG config.\\nFile Structure',\n",
       "  '73\\nAppendix B: Product Data\\nNote: stored in perpay_marketing_datamart.iterable_catalog\\n256 ],\\n257            \"carted_products\": [],\\n258            \"purchased_products\": [44444, 77789],\\n259            \"purchased_products_detail\": [{ // All products that went into repayment.\\n260 \"product_id\": 44444,\\n261 \"purchased_dt\": \"2018-10-16\"\\n262 },\\n263 {\\n264 \"product_id\": 77789,\\n265 \"purchased_dt\": \"2019-11-29\"\\n266 }\\n267 ]\\n268 \"application_started_products\": [553325],\\n269 \"awaiting_payment_products\": [51555],\\n270 \"repayment_products\": [541111],\\n271 \"verification_products\": [987111]\\n272        },\\n273        \"account_management\": {\\n274                    \"days_past_due\": 25,\\n275                    \"pathway_modifier\": null,\\n276                    \"workable_phone\": 1,\\n277                    \"workable_text\": 1,\\n278                    \"workable_email\": 1,\\n279                    \"strategy_email\": \"Standard\",\\n280                    \"strategy_email_override\": null,',\n",
       "  \"7    sum(amount_redeemed) as sum_amount_redeemed\\n8from offers_borrowercreditamounthistory\\n9where borrower_id = 2427261 and loan_id = 4860890 and status = 'used'\\n10group by borrower_id, loan_id;\\n11\\n12-- Case 2\\n13select\\n14    borrower_id,\\n15    loan_id,\\n16    sum(starting_balance) as sum_starting_balance,\\n17    sum(ending_balance) as sum_ending_balance,\\n18    sum(amount_redeemed) as sum_amount_redeemed\",\n",
       "  '93\\n(reevaluate: how should the thresholds be decided? should we increase them?)\\nLogs\\nDataDog allows us to view our logs from the server, container, cloud, or others.\\nAWS\\nSetup the forwarder lambda, enable logging for AWS service, configure the triggers\\nECS, EC2, EKS logs can be connected\\nDocker\\nRun docker command\\nUser Autodiscovery to customize source & service value so Datadog can identify log source for each container\\nLog Management \\nNPM (network performance monitoring)\\n**not supported on macOS platforms\\nInfrastructure > Network Performance\\nGraphs: View network traffic between tagged objects: containers, hosts, services, apps\\nVisualize the network traffic flow\\nNetwork Performance Monitoring Setup \\nTagging of processes\\nHost-level tags\\nHost, pod, user, service\\nUser level tags\\nECS: task_name, task_version, ecs_cluster\\nBest practices for tagging your infrastructure and applications | Datadog',\n",
       "  'dags/analytics/accounts_payable_revamp/lib/po_matching/automated_po_matches.py).\\nThe SQL that removes the ‘Yes’ and ‘No’ records from the accounts_payable_int.po_variances table is in \\nautomated_po_variances.py (PATH: dags/analytics/accounts_payable_revamp/lib/po_matching/automated_po_variances.py. The \\nsame file contains SQL to update the status of ‘Awaiting Decision’ records.\\nThere is a toggle in Saasant that prevents duplicate invoice numbers (document numbers) to be uploaded. This is ON because we \\ndo NOT want duplicate invoice numbers for manual QB bulk uploads. \\nThere is also a toggle in QB for the automated process, described in the next sub-section.\\nWe’re not sure what tooling accounting uses to actually pay the bills. But, we do know that it doesn’t allow duplicate invoice numbers \\nwhich is why we try hard to avoid submitting any to Quickbooks. That’s also why we do PO Matching on the line item level, but the \\nactual bill payment on the invoice level.',\n",
       "  '20\\nCurrent Analytics Architecture\\nUpdated diagram (Jan 2024):\\n \\nOlder diagram (Dec 2022):',\n",
       "  \"shouldn’t be able to see the payment. Is this an old sheet? … Because you shouldn’t be able to see that payment now\\nFollow up Q: this sheet that I linked: Commerce Report Variances ? No that's up to date, why do you ask? … Okay- that was there as \\nof yesterday when I sent the message (4pm EST)\\nFollow up R: It was deleted on 07-24 … For shawnrose393@gmail.com, there was a ReturnedPayment that day which incremented the \\nloan principal balance to $29.33 first, then another payment decremented it to $13.96. So it is another ReturnedPayment case\\nFollow up Q: we pull the mosiharris@gmail.com payment in that should be gone from the payment_detail table:\\ndo you think this was just a mistake or is this another table we should be concerned about in reversal updates?\\n1select * from payment_detail where payment_id = 4431651 and loan_id = 4749938;\",\n",
       "  \"92\\nOur MonitorsAgent can be extended to define a new \\nstatus check, which can be alerted upon.\\nSLO SLO alerts notify you when your SLO's \\nerror budget is close to exhaustion or if \\nthe rate of consumption has exceeded \\nyour specified threshold for a period of \\ntime.\\nSLOs are service level objectives. They \\nprovide a framework for defining clear \\ntargets around application performance \\n(help balance feature development with \\nplatform stability). The dashboard will show \\nerror budget based on this \\nExample objectives: latency less than x, \\nsuccess rate greater than y \\nWatchdog Watchdog continuously scans your \\nsystems in search of anomalous \\nbehavior. Set up a Watchdog monitor to \\nget notified automatically when something \\nis detected. \\nImport Monitor from JSON Import own monitor from JSON  \\nHost DataDog Host \\n{{host.name}} AlertAlerts when our host \\n(airflow-prod, ana-prod-\\nairflow-ec2) has gone \\ndown  \\nIntegration New Airflow Dag \\nDuration: High on \\n{{host.name}} for\",\n",
       "  \"276\\n \\n \\n \\nThis (less passing) query uses the deserve_payments_eod sftp for card payments:\\n22        SUM(a.amount) AS payments\\n23    FROM\\n24        atomic_int .platform_payments a\\n25        INNER JOIN random_sample b\\n26            ON a.borrower_id = b.borrower_id\\n27    WHERE\\n28        a .status IN ('completed' )\\n29        AND a.destination_rsn_cd = 0\\n30        AND convert_timezone ('UTC','EST',created_ts )::date between [Quarter start dt] and [Quarter end dt]\\n31    GROUP BY 1\\n32),\\n33-- Capture all rewards that were redeemed for the borrowers in Quarter\\n34rewards_redeemed_unsummed as (\\n35    SELECT\\n36        b .borrower_id ,\\n37        d .email,\\n38        a .account_id ,\\n39        a .sftp_dt,\\n40        a .rewards_redeemed_usd\\n41    FROM\\n42        deserve_data_int .deserve_rewards_daily_summary_report a\\n43        INNER JOIN random_sample b on a.account_id = b.deserve_card_account_id\\n44        LEFT JOIN public.borrower c on b.borrower_id = c.id\",\n",
       "  'k. PACC4 = PAAC4\\n8. 3  Save As the excel file to the desktop as a CSV.\\na. In CSV format - the SEQUENCE_NUMBER / RECORD NUMBER should n o t have leading zeroes. Excel may show it with leading \\nzeroes and a tick (`) mark, meaning it’s stored as text in Excel. Export to CSV and make sure there are no tick marks in it. \\n9. Go on to the Perpay admin page the click the drop down and click credit_scores\\n10. Upload the csv, set the date to the 28th of the month the data was pulled is for (prior month) and then only click the submit button \\nonce! As soon as you click the first time, minimize that page and just let it go while it uploads the data. (Do not touch the webpage)\\n11. Delete the PS_1 file from your local since it contains sensitive information\\n12. Your done!Yes - the template and PS_1 file are different by 1 letter - this was noticed after a handful of data sends without issue - as of 2024-\\n06-04, no problems expected to come of this.',\n",
       "  '274\\nRewards Validation (CBC)\\nObjective\\nCeltic would like a report to validate rewards. For this report to truly reflect \"validation\" we need to log two different data sources and see if they’re equal. The first source is taking the transactional data and \\nperforming how the rewards should be applied. The second source is taking what was actually applied to the account. \\nThis report/test ensures that the automated rewards allocation is working as expected/marketed. \\nExample\\nBelow is an example rewards report. It reflects 10 specific borrowers during March 2024. \\nMarch \\'24 Rewards Validation (CBC).xlsx \\nThe results can be captured by running the query:📄 Table of Contents\\nObjective\\nExample\\nProcedure\\n1WITH\\n2-- Typically we would collect a random sample which is why the CTE is named as follows.\\n3-- However, for the March 2024 examples, we\\'re using a list of 10 particular borrowers.\\n4random_sample as (\\n5    SELECT\\n6        borrower_id ,\\n7        deserve_card_account_id',\n",
       "  '193\\n(note: this table is already a historical table as that is how Domostats presents this table)\\n \\nSlack Alerts:\\nA slack alert will notify the team when a card has not been viewed, updated, or shared in 60 days, and the page on which the card \\nresides has also not been viewed, updated, or shared in 60 days. \\nA slack alert will notify the team when a page has not been viewed, updated, or shared in 60 days.\\nA slack alert will notify the team when a dataset does not have any cards built on it, and the dataset has not been run in 60 days.certification_id integer ID of the certification \\ncertification_process string String indicating the certification \\nprocess (for now always \\n\"COMPANY\")\\ncertification_status string Current status of the certification \\n(ACTIVE, CANCELED, DENIED, \\nDEXPIRED, REQUESTED)\\nlast_modified_dt date The date on which the certification \\nwas last updated\\nobject_id integer The Domo ID of the Domo object \\nbeing certified (eg CARD)',\n",
       "  \"Step 2: Designing the queries\\nSuppose we had a real estate company and wanted to build a report that counted the number of restaurants within a 5 miles radius for each \\nproperty in the database, given the restaurant is also in the same state as the property. The simplest way to do so might look something like\\n \\nHowever, this query is difficult to break into smaller components to test.\\nTo apply the testing framework described above, it would be better to split this up into two queries and apply templating so we can stub out \\nthe tables with mock data.\\n1from utils.dag_helpers import generate_bash_tests_task\\n2generate_bash_tests_task (\\n3  dag,\\n4  'TestSessionFactDefinitions' ,\\n5  'analytics/data_model_session/tests/test_session_fact_definitions.py' )\\n1create table {{ temp_schema }} .property_restaraunt_options as\\n2select\\n3    p.property_id ,\\n4    count(1)\\n5from {{ public_schema }} .properties p\\n6join {{ public_schema }} .restaurant_address ra\\n7    p.state = ra.state\\n8where\",\n",
       "  '606\\nV2 Proposal:',\n",
       "  'Rewards Tracking\\nThe Perpay credit card is a part of the Mastercard World program and there are associated benefits along with that outlined here. For \\nexample, there is a Peacock promotion within the Mastercard World program that gives $3 a month when you use a Mastercard World card \\nto buy the subscription. At program launch, these credits would populate as payments that would remain in a PROCESSING status in \\nperpetuity. In January 2024, Deserve began updating the mapping of these rewards to come through as transactions \\ntransaction_category of MERCHANT_CASHBACK. \\nThe transition from from reporting these rewards as payments to reporting these rewards as transactions has been fuzzy. This has left \\nseveral of these rewards to have both an associated payment and an associated transaction. This necessitates that we identify which card \\npayments originate from Mastercard rewards rather than money that arrived to Perpay (deposits, refunds, payments on canceled orders,',\n",
       "  'During our planning, we came up with several questions relevant to this project:\\n1. Does dropping and recreating tables remove their permission settings? The answer seems to be YES.\\n2. Is there a difference between DROP and DROP CASCADE we need to consider? DROP CASCADE helps remove the views created \\nbased on the table being dropped. As far as I can tell, we do not use views, so DROP CASCADE isn’t something we need to worry \\nabout.DE DS DA Risk Engineerin\\ngAccountingMarketing Product E-\\ncommerce\\nAdmin         \\nSuperuser Superuser        \\nUser User User User User User User User  \\nInternal \\ninternsInternal \\ninternsInternal \\ninternsInternal \\ninterns     \\nExternal \\ninternsExternal \\ninternsExternal \\ninternsExternal \\ninterns     Risk& Analytics External Teams',\n",
       "  '9 COMPLETED Andrew Initial Loan Amount (link): There have been manual entry \\nissues over time that have caused the first recorded balances in \\npayment_loanprincipalbalancehistory to differ from the \\nloan amounts in the loan table.Oddly enough, the loan balance \\ndoesn’t match the first record of \\nloanprincipalbalancehistory. We \\ntold talha he can push off the \\ninvestigation for this because it \\naffects records prior to 2023, and \\nwe are focused on more current \\nvariances at the moment.\\n*marking complete since manual \\ncleanup was done 2022-01-01 \\nand beyond\\n10 COMPLETED\\nPR hereAbby/An\\ndrewPayments = Borrower Credits (link): The payments = \\nborrower_credits, which should both be removed from the \\nstarting_balance to form ending_balance_calc. However, the \\nvariance seems to be the exact amount of both, so it’s almost \\nas if one isn’t counting. Seems like almost none are reversal \\ncases, so maybe the public.payment_detail table tracks',\n",
       "  '465\\naccount_opening_fee\\naccount_opening_fee_adjustment\\nmonthly_feeA transaction in a SETTLED state is as defined by Deserve.\\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\ntransactions field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nREGULAR.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all ACCOUNT_OPENING_FEE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:',\n",
       "  '133\\n(Error) Company Switch Mid-Run\\nIn the airflow_prod_alerts channel:\\nError explanation\\nUsually this error is an indication that there was a company switch mid run (which occurred from a timing issue). \\nInvestigation\\nUsually look into loan_fact or user_fact first for duplicates; most tables rely on these. \\nIf there is a duplicate, see where it’s data differs across row- e.g. if there was a company switch mid-run the company values will differ \\nacross rows.\\nSolution\\nBecause it was due to timing, it should self resolve if re-ran. Thus, we clear the tasks that rely on public.company in DM full. Be careful when \\nyou clear tasks- you want to clear the correct one (verify the name when you clear it) and clear the actual table build, not just the test that \\nfailed. \\n** something to keep in mind: now that dm full tasks are more independent as of [November 2, 2022], you need to check other data',\n",
       "  '1913:DROP TABLE IF EXISTS {{temp_schema }}.payment_status_hist ;\\n1dags/analytics/metric_layer/store_withdrawal_status_metric/store_withdrawal_status.py\\n219:        FROM {{pub_schema }}.withdrawal_request_status\\n347:        {{pub_schema }}.withdrawal_request a\\n449:        LEFT JOIN {{pub_schema }}.withdrawal_request_status c ON a.id = c.withdrawalrequest_id\\n5\\n6dags/analytics/data_model_accounting/account_reconciliation/core_account_daily_rollforward/core_account_daily_ro\\n7223:        FROM {{pub_schema }}.withdrawal_request a\\n8225:        LEFT JOIN {{pub_schema }}.withdrawal_request_status c on a.id = c.withdrawalrequest_id AND c.status i\\n9226:        LEFT JOIN {{pub_schema }}.withdrawal_request_status d on a.id = d.withdrawalrequest_id AND d.status i\\n10227:        LEFT JOIN {{pub_schema }}.withdrawal_request_status e on a.id = e.withdrawalrequest_id AND e.status i\\n11228:        LEFT JOIN {{pub_schema }}.withdrawal_request_status f on a.id = f.withdrawalrequest_id AND f.status i',\n",
       "  '284\\nCumulative Dollar Flow Variances over Time\\nThese should already be accessible to the accounting team, the primary stakeholder of the tables. Reach out to @Abigail Rehmet or @JD \\nHerr if you would like access to a card. \\nCard Report Definitions & Nuances\\nDaily Equation\\nFor card account reconciliation, on every day the following equation should hold true for any given account. Each argument links to nuances \\nassociated with that argument, if none exist more information on the argument can be found in the card field definitions under Additional \\nResources.\\nstarting_balance\\n+ transactions\\n+ account_opening_fee\\n– account_opening_fee_adjustment\\n+ monthly_fee\\n– monthly_fee_adjustment\\n+ late_payment_fee\\n– late_payment_fee_adjustment\\n+ shipping_fee\\n– debit_card_processing_fee_adjustment\\n+ international_transaction_fee\\n+ interest_charge\\n– interest_charge_adjustment\\n– cash_back\\n+ cash_back_adjustment\\n– refund\\n+ credit_balance_refund\\n– dispute_won\\n+ dispute_lost_withdrawn',\n",
       "  '495',\n",
       "  '612\\nCensus\\nVideo topic ideas\\nWhy do we use it?\\nHow to set up new syncs?\\nTricks for making the most out of our allotted fields',\n",
       "  \"296\\n2\\n0\\n9\\n7\\n0 M\\ny\\nC\\nre\\ndi\\nt\\nN\\no\\nw\\n8\\n5\\n0\\n_\\nQ\\n1\\n2\\n0\\n2\\n4          3\\n6\\n9      p\\na\\ny       X\\nT\\n_\\nLI\\nN\\nK s:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          o\\np \\nn\\no\\nw, \\np\\na\\ny \\nla\\nte\\nr, \\nb\\nui\\nld \\ncr\\ne\\ndi\\nt                     M\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0                 e              e       e         e               e    G\\nLI\\nS\\nH                      2\\n4-\\n0\\n1-\\n0\\n2\\nT\\n1\\n7:\\n2\\n4:\\n2\\n5-\\n0\\n5:\\n0\\n0  B\\ne\\ngi\\nn\\nni\\nn\\ng\\ns, \\nL\\nL\\nC'\\n]          e             \\n1\\n9\\n2\\n0\\n8\\n1\\n7 P\\nu\\ns\\nh\\nn\\na\\nm\\ni_\\nQ\\n4\\n2\\n0\\n2\\n4                                                  1\\n4\\n3\\n6\\n9      P\\ner\\np\\na\\ny       T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK ht\\ntp\\ns:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          []                                                                                                                                                      $\\n1,\\n0\\n0\\n0 \\nto \\nS\\nh\\no\\np \\nN\\no\\nw, \\nP\\na\\ny \\nL\\nat\\ner                                                                    H\\nT\\nM\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0.\\n0\",\n",
       "  \"11      when c.is_active = 'false' then 1\\n12      else 0 end as standard_exclusion_ind,\\n13    f.status as plus_status,\\n14    e.enabled_ts as plus_enabled_ts\\n15from {{temp_schema}}.user_attribute a\\n16left join {{temp_schema}}.user_attribute_2 b on a.user_id = b.user_id\\n17left join {{pub_schema}}.user as c on c.id = a.user_id\\n18left join (\\n19    select a.borrower_id, min(a.created) as enabled_ts\\n20    from {{pub_schema}}.feature_history as a\\n21    left join {{pub_schema}}.feature_enrollment as c on c.id = a.feature_enrollment_id\\n22    left join {{pub_schema}}.feature as b on c.feature_id = b.id\\n23    where a.status = 'enabled'\\n24    and b.type in ('perpay_plus', 'perpay_plus_v2')\\n25    group by 1\\n26  ) as e on e.borrower_id = a.borrower_id\\n27left join (\\n28    select a.*\\n29    from {{pub_schema}}.feature_enrollment as a\\n30    left join {{pub_schema}}.feature as b on a.feature_id = b.id\\n31    where b.type in ('perpay_plus', 'perpay_plus_v2')\\n32  ) as f on f.borrower_id = a.borrower_id;\",\n",
       "  '195\\ncustom flake8 plugin as explained here. We can also choose to lint our code if we wanted it to fit certain standards. If we find a way to only \\ncheck affected files in our PR, and apply stricter linting, we could begin a gradual process towards coding standards instead of having to \\nreformat all of our code in one PR. Over time, we could bring files into these standards as we work on them.',\n",
       "  '103        left join card_int.card_account_attribute as f on p.card_account_id = f.card_account_id\\n104        left join card_int.card_account_performance j on p.card_account_id = j.card_account_id\\n105        left join minimum_payment_2 o on o.borrower_id = c.borrower_id\\n106        left join public.card_account q on q.borrower_id = c.borrower_id;\\n107\\n108-- create the new cdp table\\n109drop table if exists users_herr.iterable_customer_data_save;\\n110create table users_herr.iterable_customer_data_save as select * from perpay_marketing_datamart_ext.iterable_customer_data;\\n111\\n112-- Update the card object\\n113update users_herr.iterable_customer_data_save as a\\n114 set card_dict = b.card_dict\\n115from users_herr.card_update b\\n116left join public.user c on b.user_id = c.id\\n117where a.user_uuid = c.uuid;\\n118\\n119-- Make the switch - uses renaming to make the switch super fast\\n120create table census_source_int.iterable_customer_data_temporary as select * from users_herr.iterable_customer_data_save;',\n",
       "  '501\\nvariance_with_workarounds\\n A flag indicating whether or not the variance is caused by a partial return. 1 means likely is caused by partials. 0 means not likely \\ncaused by partials. \\nBusiness Context:\\nIt will take Eng a bit to rework modeling to capture exact partial return amounts. In the meantime, this identifies the variance so we can \\ncalculate the variance with a partial return workaround in the following field, variance_with_workarounds.\\nRollforward Table Logic:\\nHandled in a CASE statement. If the absolute value of the variance is less than the absolute value of a returned payment, and the \\nreturned payment exists, then the variance is likely caused by a partially  returned payment, so the flag is set to 1. Otherwise, the flag is \\nset to 0.\\nSource Tables:\\nN/A directly since it’s a calculation. \\nDefinition:\\nThe variance is set to 0 if any of the workaround inds are 1. This reflects the final variance even after temp solutions have been coded.\\nBusiness Context:',\n",
       "  'Invoice Date\\nInvoice Number\\nPO Number\\nTotal\\nWith column titles:\\nLine Items\\nModel = SKU from po_source (SKU in Magento/House)\\nDescription\\nQtyShip\\nNetPrice\\nAmount\\n2: These fields are optional:\\nWithout column titles:\\nDiscount\\nFreight\\nTax\\nDropship\\nTracking Number\\nWith column titles:\\nPer (within Line Items)\\nBrand (within Line Items)\\n3: If these fields are parsed as null, an error is thrown in the #accounts_payable_alerts channel\\nInvoice Number\\nModel\\nTotal\\nAmount\\nLine Items\\nInvoice Date\\nPO Number',\n",
       "  'some cases.Conor raised a ticket about this \\nand we have yet to hear back. \\nHe believes the SFTP files are \\ngenerated by Deserve whereas \\nthe processor (Corecard) \\nactually maintains the ledger, so \\nit’s possible there’s a delay \\nbetween the two that caused \\nthis.',\n",
       "  'Accounting Commerce Loan Daily Rollforward397\\n                          DE Notes on Initial Loan Discrepancies400\\n                     Card Account Balance Daily Ledger402\\n                          Known Deserve Incidents407\\n                Flattened Merged Users408\\n                Withdrawal NACHA File Generator414\\n                Experian Account Review Ingestion417\\n                Deserve Clarity Underwriting SFTP Ingestion419\\n                Deserve Clarity Underwriting Ingestion421\\n           WIP423\\n                Optimizing DM Full424\\n                Pinwheel Investigation426\\n                Pinwheel Metric Layer428\\n                Migrate Data Models From Amplitude to Segment430\\n                Card Datamodels Finalization431\\n                Permissions Restructure432\\n                Adjusting for Cards in the Wild437\\n                Clarity Score Drift from FTBs459\\n                VARIANCES - Accounts Receivables Reconciliation461',\n",
       "  'ID). Below are the recommended elements to include in Glossary entries while the normal additional owner and tags values can be selected \\nas well.\\n1. Description (required)\\na. A short to medium form description of the term and how it pertains to Perpay. What are its rules? When does it change? What affects \\nit?\\n2. Project Links\\na. Please add links to any relevant projects that provides greater detail on the term. And example is the underwriting risk score where a \\nlink to the data science project documentation would be natural to add so that the user can have more background on the concept if',\n",
       "  \"h                                            H\\nT0.\\n0                                    tr\\nufa\\nlsfa\\nlsfa\\nlsfa\\nlsE\\nN2\\n0                            ['\\nR []                                          fa\\nls                                                                                                                                                                                                p\\nti\\no\\nng\\nni\\ndg\\nn\\nn\\na\\nm\\nep\\na\\ng\\neal\\nlb\\na\\nc\\nk\\nsa\\nd\\ns\\ne\\nr\\nvi\\nn\\ng\\nu\\nrle\\nt\\ne\\nm\\npl\\na\\nt\\neni\\nta\\nrt\\ny\\ns\\ne\\nr\\nv\\na\\nbl\\ne\\na\\nd\\nc\\nr\\ne\\na\\nti\\nv\\ne\\nh\\nei\\ng\\nh\\nta\\nrt\\ny\\ns\\ne\\nr\\nv\\na\\nbl\\ne\\na\\nd\\nc\\nr\\ne\\na\\nti\\nv\\ne\\nw\\nid\\nt\\nhr\\nal\\nt\\ne\\nr\\nn\\na\\nti\\nv\\ne\\nt\\na\\ngnl\\nin\\nk\\nn\\na\\nm\\nen\\nal\\nlo\\nw\\nc\\nu\\nst\\no\\nm\\np\\nr\\no\\nm\\no\\nc\\no\\nd\\nem\\nlc\\no\\nd\\ne\\nty\\np\\neis\\na\\nti\\no\\nn\\nc\\nh\\na\\nr\\ng\\nee\\npl\\nin\\nki\\nn\\nge\\na\\nd\\nytr\\na\\nc\\nki\\nn\\ngc\\no\\nd\\ne\\ntr\\na\\nc\\nki\\nn\\ngg\\neti\\nm\\ne\\nst\\na\\nrt\\nd\\na\\nt\\neti\\nm\\ne\\ne\\nn\\nd\\nd\\na\\nt\\nee\\nd\\nm\\ne\\ndi\\na\\np\\na\\nrt\\nn\\ne\\nr\\nse\\nd\\nm\\ne\\ndi\\na\\np\\na\\nrt\\nn\\ne\\nr\\ng\\nr\\no\\nu\\np\\nso\\nnle\\nrm\\nes\\nc\\nri\\np\\nti\\no\\nnt\\nep\\neo\\np\\neo\\nd\\nu\\nct\\nst\\ne\\ng\\no\\nri\\ne\\nsn\\ntt\\ny\\np\\nen\\nt\\na\\nm\\no\\nu\\nn\\nt\",\n",
       "  \"2. Is the logic for the compliance or Metro2 check outdated or incorrect? \\n3. Is this a weird edge-case that we have never encountered before? '\\n4. Is there weird data in a public table? \\n5. Do we need to ask ops to reach out to the customer to update their account info? \\n6. The resolution for each case will look different, but these are some common sources of held-out borrowers\\nTouchbase Takeaways\\nOccasionally asks will come in from compliance to update how we are doing our credit reporting. These are disseminated at a biweekly \\ncredit reporting touchbase that the DE on rotation will be attending. KG and compliance will share the context for the asks, and you’ll have \\nan opportunity to answer any questions. This is also an opportunity to share any updates on the asks from the previous touchbase. The \\nmeeting notes keep a brief running log of what the current outstanding asks are:\",\n",
       "  '85\\nVersioning and Sunset Date\\nPerpay currently rely on multiple third party APIs for marketing, data analytics, customer outreach, etc. Many of these platforms provide \\nupdates to their product, and older versions of the API could be deprecated as the product line evolves. This document provides a list of \\nthird-party APIs used in the airflow schedule processes at Perpay and the corresponding versioning/sunset date for each API. \\nAPI status: \\n*: starred API providers use a wrapper package for using their APIs. \\nA more detailed list of the specific functions used by each API providers is stored in a shared google sheet. The sheet contains additional \\ninformation to function used, file locations, and link to documentation. \\nLink to google sheet: airflow DAG API Versions QuickBook Base ver. Not scheduled\\nDocparser Base ver. Not scheduled\\nAmplitude Base ver. Not scheduled\\nAlgolia* algoliasearch = 2.5.0 Update to >= 3.0.0 recommended. No \\nscheduled sunset date.',\n",
       "  '290\\ncore\\nSome variances were a result of Engineering processes that required manual cleanup. These are tracked using the resources below.\\nJira card\\nGoogle sheet\\nIn some scenarios, Engineering will need to update their models to improve monitoring of all funds in and out of customer accounts. \\nThese updates require some time, so DE has implemented temporary workarounds for these processes that do not yet exist. More \\ncontext can be found here. All teams at Perpay should have access to see this Google sheet, but reach out to @Abigail Rehmet if you \\ndon’t.',\n",
       "  \"Variance 9: Initial Loan Amount\\nQuestions: \\n1. It appears how the initial loan amount is defined in this table has changed over time. For the start_bal_loans, it looks like the initial loan \\namount was the starting_balance of the loan's first record in the payment_loanprincipalbalancehistory table. However for the \\nend_bal_loans, it looks like initial loan amount was the ending balance of the loan's first record in the \\npayment_loanprincipalbalancehistory table. This is causing variances as we cannot tell exactly what starting balance to use for the \\nbeginning of the loan. Does anybody know the timeline of how the beginning of a loan has been represented in \\npayment_loanprincipalbalancehistory?\\n2. For the start_bal_loans, it also looks like the starting balance of the first row does not match the initial loan amount as reported in the \\nloan table. Does anybody know why this is? This is also causing some variances because we were expecting the start of the loan's\",\n",
       "  '335\\nData Discrepancies v2 (11/29/22)\\nPerpay Core Issues:\\n12 auto_pay_payment id: \\n891 \\ndeposit id: 2744109Payments that are \\nmarked returned in the \\npayment_achtransacti\\non table are not marked \\nreturned in the deposit \\ntable. 542 returned \\ntransactions (~41% of \\nreturned transactions)Appears to be resolved \\nas of 10-29-2022.\\n Backfill complete! - \\nTaylor\\n13 auto_pay_payment id: \\n416 \\ndeposit id: 2685390Deposit amount is \\ndifferent from auto pay \\npayment & ACH \\nTransaction amount. 1 payment This one’s weird!  I will \\nexplain and we can \\ndecide if any action \\nneeds to be taken.  This \\nscenario should not \\nhappen again as we’ve \\nmade changes to the \\nACH payment \\nprocessing.Issue # Example Issue Impact Resolution',\n",
       "  \"meaning there was a successful run previously but you don't want to use that output and will instead be generating a new output, \\nunderstand that you need to set the last DAG status to FAILED. This is because the code decides the file ID based on the number of \\nsuccessful runs that came before it and the NACHA files sent throughout any given they need to be ID'ed with A, B, C... in order. So \\noverwriting an old file necessitates that you set the previous successful run to FAILED so that the count, hence the ID comes out \\ncorrect.\",\n",
       "  'last refresh, and there is a consistency across the data at that \\ntime snapshot.incrementally (manually or automatically) for the data to be up \\nto date. However, updating a MV too frequently can impact \\nperformance, so finding a balance that works for our \\narchitecture will require foresight.\\nRedshift makes MVs powerful\\nMVs in redshift can query from S3\\nMVs can be built on top of other MVsMemory\\nWhile compute costs decrease, MV’s take up a lot of storage \\nspace, which can result in increased costs\\n1[\\n2    DISTSTYLE { EVEN | ALL | KEY } \\n3    DISTKEY ( distkey_identifier )\\n4    SORTKEY ( column_name , ... )\\n5]',\n",
       "  \"fivetran_quickbooks_sandbox schema. This may be important to consider depending on what you're trying to test.\\nI like to copy the table into the fivetran_quickbooks schema (the schema actually utilized by the DAG) with select invoices to mimic the \\nSandbox QB info.\\nReset\\nA lot of times on staging, you’ll run the job, modify something in the sheet to see that it’s working, then want to reset. This process varies \\nbased on what you’re trying to do, but the most common form of reset goes as follows:\\ndelete the invoice from QB staging\\nif you want to delete a single one, you can find it, click the dropdown arrow to its right and hit delete\\nif you want to bulk delete for a vendor, you can mark that vendor as inactive, which essentially does the same (puts a suffix like \\n_deleted after each invoice). this allows you to recreate the vendor with a balance of $0.00\\ndelete the invoice histories\",\n",
       "  'wait actually, can you elaborate on\\nRows below are the history table for the principle balance of that loan. We should’ve cleaned up the last 4 rows when we deleted the \\npayment.\\nare there other relevant tables that are not adjusted when payments are undone?\\nNo, there shouldn’t be.\\nQuestion (4.2): ah okay, thanks for the clarity. Going to include accounting in this to understand if we need to track those payments in this \\ntable from their perspective ----- (For Accounting) From an accounting standpoint, is it necessary to track these ACH reversal cases in this \\ntable to maintain an accurate ledger of account balance between the date of deposit and the date of reversal? Or, can we act as if the \\ndeposit never occurred which aligns with what engineering currently does?\\nResponse (4.2): For accounting I think we are fine tracking as if it never occurred, as long as it clears within a few days.',\n",
       "  '530\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  seg\\nmen\\nt_str\\nipe\\ncore\\n_sp\\nendi\\nng_l\\nimit\\ns_in\\nt\\nperp\\nay_a\\ncco\\nunti\\nng_\\ndata\\nmart\\n_ext\\nperp\\nay_\\nmar\\nketi\\nng_\\ndata\\nmart\\n_ext\\ndom\\no_e\\nxt\\nperp\\nay_r\\nisk_\\ndata\\nmart\\n_ext\\nacc\\nount\\ning_\\nwor\\nksp\\nace\\nperp\\nay_f\\npa_\\ndata\\nmart\\n_int\\nperp\\nay_e\\ncom\\nm_d\\nata\\nmart\\n_ext\\nperp\\nay_\\ndata\\n_go\\nvern\\nanc\\ne_ex\\nt\\nseg\\nmen\\nt_lo\\ngs',\n",
       "  '31\\n2023 Q3 OKRs\\n Timeline\\nRelated pagesTeam\\nOwner:\\nEnd-of-quarter objective \\nscore:0.0-1.01.0 Month 1\\nMonth 2\\nMonth 3\\n     \\n     \\n      \\n     \\n     \\n      \\n     \\n     Objectives Key results Owner Partner \\nwithExpected \\nEoQ key \\nresult \\nscoreCurrent status\\nFor OKR pro tips from Atlassian teams visit:https://www.atlassian.com/team-playbook/plays/okrs',\n",
       "  \"in the card you were referring to. However, it's worth noting that the core table is not yet complete, so I wouldn't recommend relying on it \\nsolely for refund calculations at this point. I want to emphasize that both domo cards you're examining seem accurate but represent different \\nperspectives/contexts.If you prefer the commerce loan table to reflect the entire refund amount, we can certainly explore making that \\nadjustment. But I think it's important to consider that doing so would alter the table's context. On a row-by-row basis, you wouldn't be able to \\ndirectly infer that the entire amount went toward the loan balance.\\nIf we decide to pursue this approach to have the refunds column reflect the total refund amount, my suggestion would be adding a new \\nfield, perhaps amount_towards_account_balance, to capture any funds directed toward the account balance rather than the loan balance.\",\n",
       "  '200\\nGit 101: Staging, Committing, Pushing, and Creating a PR\\nBecause of the DE testing and the PR process, this is a guide for DEs only. There is a separate procedure for pull requests in the \\nAnalytics space for other members. \\nThese are command line commands. You may also use the IDE interface to commit, push, and create PRs if you’re more comfortable with \\nthat!\\nJD tends to use the command line and Abby tends to use the UI when it comes to this stuff so if you have questions regarding a specific \\none, they can both help with either but will be best at helping with their respective preferences\\n Working on a new branch\\n1. Start on the master branch. Make sure it’s up to date by pulling\\na. git checkout master\\nb. git pull\\n2. Create a new branch \\na. git checkout <type>/<branch_name?>\\ni. ex: git checkout -b feature/new_dm\\n3. Flip between branches by checking out\\na. git checkout <branch>\\nCommitting and Pushing\\n1. Stage files you want to commit\\na. git add . for all changed files',\n",
       "  '72\\n173                    }\\n174\\n175                ],\\n176                \"14_days\": [\\n177                    {\\n178                        \"category_path_deep\": \"fashion-beauty/personal-care\",\\n179                        \"category_path_1\": \"fashion-beauty\",\\n180                        \"category_path_2\": \"personal-care\",\\n181                        \"category_path_3\": NULL,\\n182                        \"category_path_4\": NULL,\\n183                        \"category_path_5\": NULL,\\n184                        \"category_path_6\": NULL,\\n185                        \"visits\": 10\\n186                    },\\n187                    {\\n188                        \"category_path_deep\": \"electronics/computers/ipads-tablets\",\\n189                        \"category_path_1\": \"electronics\",\\n190                        \"category_path_2\": \"electronics/computers\",\\n191                        \"category_path_3\": \"electronics/computers/ipads-tablets\",\\n192                        \"category_path_4\": NULL,',\n",
       "  'Good → reformatted\\nAdHoc Reports\\nAre we still looking at product_card_activation for anything?\\nZach M would know\\nCard data model hist\\nCard account statement history data model ✅  PR is up\\nPR 2023-02-03: Update to the Card Account Statement History DM- logic & format changes\\ndocumentation: Account Statement History \\nCard account statement history fact\\n Good → reformatted\\nCard account statement history balance\\nNeed RDD at time – portfolio view (“$100 for $500” etc) → done',\n",
       "  '385\\nFinding Account Level Discrepancies\\nThe current_balance is calculated as the sum of balance in the account table for all users in the merged cluster. The variance value \\nis equal to the difference between this current_balance and the calculated ending_balance.',\n",
       "  '329\\nIssue # 3:\\nPayments appear as “processing” in Perpay, when they appear as “completed” in the SFTP files. \\nIssue # 4:\\nPayments appear as returned in Perpay, but appear as completed in the SFTP files. According to Deserve Admin, Perpay is correct, and \\nSFTP files are wrong. \\nImpact: 5 transactions\\n14       b.payment_amount as deserve_payment_amount,\\n15       b.completed_at as completed_at,\\n16       b.sftp_dt,\\n17       c.card_payment_id\\n18from public.card_cardpayment as a\\n19left join (select * from users_frank.card_sftp_payment where current_ind = 1) as b on a.deserve_id = b.payment_i\\n20left join public.auto_pay_payment as c on a.id = c.card_payment_id\\n21left join public.card_account as d on a.card_account_id = d.id\\n22left join perpay_general_datamart_int.user_attribute as e on d.borrower_id = e.borrower_id\\n23where  b.payment_id is null and -- indicates that the payment is missing from deserve sftp files',\n",
       "  '260\\nChanging External Schema Columns\\nSometimes we need to add new columns to tables we don’t want to recreate, like Amplitude, because recreating these large tables would \\ntable a very long time and a large amount of resources. Instead the following steps should be used when adjusting an external table.\\n1. Go into the AWS console, Glue, search for the table, select it. You can click “edit schema” on the top right and add columns with \\nparticular types.\\na. \\n2. Make necessary code changes to account for the new columns. Additionally, if this data is derived, you’ll have to generate a script to \\npopulate historical data.',\n",
       "  \"538\\nDE Access Controls\\nAs a team that shares many responsibilities across the stack, it's important that everyone knows everyone's level of access. Note that \\npermissions are not doled out like hot cakes, as data engineers we interact with crucial resources and infrastructure and therefore as a team \\nmember gains more experience, higher permissions are given.\\n = Current, \\n = Upcoming, \\n = No Access, \\n = Unsure\\nProd CLI\\n  \\n  \\nStaging \\nCLI\\nProd Admin \\nRedshift\\nStaging \\nAdmin \\nRedshift\\nProd \\nRedshift \\n(User \\nschema)\\nStaging \\nRedshift \\nProd \\nAirflow \\n(Admin)\\n \\nProd \\nAirflow \\n(Read \\nOnly)\\nStage \\nAirflow \\nAdmin\\nLocal \\nAirflow\\nMagento \\nStaging DB\\n \\nMagento \\nProd DB\\n \\nMagento UI\\n  \\nR e s o u r c e J D D e r y a A b b y A n d r e w E r n e s t G a b e H o n g k a i B e n n e t t Y o u s u f\",\n",
       "  '17    card_transaction_id,\\n18    deserve_id,\\n19    created_ts,\\n20    transaction_status,\\n21    rank() over (partition by card_transaction_id order by created_ts asc) as rank,\\n22    case\\n23        when created_ts = first_value (created_ts) over ( partition by card_transaction_id order by created_ts d\\n24        else 0 end as current_ind\\n25from status_1\\n26where ((transaction_status <> transaction_status_previous) OR (transaction_status_previous IS NULL));\\n27\\n28\\n29select\\n30    count(*) as cnt,\\n31    card_transaction_id\\n32from users_frank.card_transaction_status\\n33group by 2 having cnt > 2 order by cnt desc;\\n1select *\\n2from public.card_transaction_snapshot\\n3where card_transaction_id = 129165\\n4order by created desc;',\n",
       "  '353\\nPages Events\\nThis page explains the difference between segment_storefront_schema.pages and segment_web_schema.pages.\\nThe Difference Between the Pages Table in Each Schema\\nsegment_storefront_schema.pages tracks \"page loaded\" events for sites corresponding to the storefront, \\nwhile segment_web_schema.pages tracks this event for the website only.\\nFor example:\\nAn event in segment_storefront_schema.pages would come from a storefront page (in terms of the URL), similar to\\nhttps://shop.perpay.com/featured/gifts-for-him-2022?p=3&modal=tireWidget\\nor\\nhttps://shop.perpay.com/search?query=Gifts%20for%20couples\\nThe url corresponds to some marketing campaign on the storefront or a search query on the storefront.\\n \\nIn contrast, a URL in the segment_web_schema.pages table would be look like\\nhttps://app.perpay.com/sign_up\\nwhere they person is signing up to be a Perpay customer, but the site isn\\'t directly on the storefront.',\n",
       "  \"577\\n5. Go to the console and run the glue crawler\\na. Log into the console →. assume the staging role → navigate to Glue → crawlers → select the crawler → click Run\\n6. Create the external database in Redshift. You can either:\\na. Set up your local environment and run this script\\nb. Manually run this query if you have admin privileges:\\nc. Note: Segment automatically creates the folder in the S3 bucket as ‘segment-logs’. Redshift does not like the ‘-' character, so we \\nreplace this ‘-’ with '_’ when we are defining this schema. \\n7. You can now query the external database!\\na.  I would recommend that you run  SET json_serialization_enable TO true;  before querying. This way you can do the typical \\nselect * statements you are used to - if you do not set this to be true, you will need to specify which path of the json you intent to \\nquery.\\nb. Helpful article about the super type & jsons in Redshift Spectrum\\n8. Create a metric layer based on the event that you would like. (Airflow PR).\",\n",
       "  \"Using the Python API, we can investigate attributes of specific schemas and datasets and change what's visible to the users of DataHub. \\nWe've also used the Python API to access lineage to help with the project to move data off our staging environment. To get access to this, a \\ntoken will need to be generated from the UI and sent to a new user. Here you can manage permissions at a broad level - admin, editor, and \\nreader - and then to use the Python API or CLI, pass through this token with the server endpoint. This is managed by the utility through a \\nsecrets file that is created in your home directory and should be outlined in the utility README.md file. (note: PR not merged; not viewable). \\nFrom here, you can interact with the class underlying the utility notebook. \\nData Ingestion with DataHub\\nDataHub connects to our Redshift database and ingests metadata to get table/column profiling. These are set up in a job structure and can\",\n",
       "  '552\\nFortress Schedule of Receivables\\nmarketplace schedule of receivables\\ncard schedule of receivables\\nMarketplace Schedule of Receivables\\nData Provided\\nloan_id  \\naccount_id  \\nborrower_id  \\nuser_id  \\nfirst_name  \\nlast_name  \\nstate state of profile address, not shipping address\\norigination_date The date of the timestamp of the first repayment \\nstatus for the order\\nmarketplace_original_term (in months) A calculation between the customer’s pay cycle at \\nthe time of entering repayment and the payment plan \\nfor the order\\nmarketplace_remaining_term (in months) The length of the original term minus the amount of \\nmonths that have elapsed since repayment\\nmarketplace_original_receivables_balance The amount of the order minus coupons (does not \\nexclude borrower credits)\\nmarketplace_daily_starting_receivables_balance The amount remaining on the order at the beginning \\nof the observation day (pulled from AR)',\n",
       "  '603\\nPros:\\nGreat Expectations will contribute boiler plate setup code to create data documentation with valid values descriptions, while the current \\nimplementation using pytest has no visualization aspect. In addition, boiler plate code comes out of the box to have wider valid values \\ntesting than we have with pytest.\\nCons:\\nIf leveraging S3 for Data Docs, the documentation would only be accessible within technical teams. However, if it is necessary for the \\ndocumentation to be accessed by the broader organization, I’m sure we could lean on the engineering team to suggest a place we could \\nstore the files.\\nOtherwise: None. No extra cost (well storage in S3 but that’s very cheap), very quick to set up (I created the example doc above in 20 \\nminutes).',\n",
       "  'This is a sheet of DE/DS/Analytics employees and their access to schemas and tables in the database. \\nThis is a sheet of DE employees and their access to databases, infrastructure, and tools.\\nThis is a sheet of all other teams  and their access to schemas and tables in the database.\\nData Retention\\nDOMO Datasets, Pages, Cards\\nDOMO is …\\nRedshift Tables\\nR\\nData Sources\\nData sources like Amplitude, Segment, Fivetran, Datadog\\nAWS Resources\\nDMS, Glue\\nChange management\\nWe use GitHub to track codebase changes. In',\n",
       "  'It would allow for the feeling of ownership without a singular developer and maintainer.\\nDE initiatives\\nCoding standards (travis linting, pep8?)\\nComments on every cte and python function\\nUse travis to enforce in specific directories are more and more are updated\\nCreating a release-based update paradigm, similar to engr. \\nWe release updates on Monday and Wednesday… (internally we can do hot fixes… shhhh…)\\nUpgrade Airflow to new version, evaluate data triggers\\nMigrate spectrum data replication from DMS to S3 and glue\\nImplement Cell structure paradigm \\nStart updating metric layer tables to only pull from spectrum data\\nWildcards',\n",
       "  '270\\nDatahub Deployment\\nPage under construction…\\nPre-Terraform Steps\\nDocker Images and Environment Files\\nThe main repo you will need for this step is here. The README file of the repo goes through each step thoroughly, so do read before a new \\ndeployment and ensure you’ve gone over all requirements before using the command make all. This will be especially important if you are \\nattempting to add new features or bumping the Datahub version.\\nWhy do we use non-standard Docker images in ECR in this way? Two reasons: First, they are non-standard, the images need to be built to \\naccount for the requirements of ECS and our particular deployment approach. Second, in rapid cycles of deployment, if we use standard \\nimages on Dockerhub, we will run into request limits and get throttled. Using ECR avoids this.\\nA great resource you can reference is the Github repository of Datahub, especially their docker-compose files (the linked page is for v0.13.1',\n",
       "  'Glue job runs to convert csv to parquet, places parquet in rymax_statements folder in perpay_accounting_datamart folder\\nrymax_csv_to_parquet_glue_job\\nhttps://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs\\ns3://ana-stage-spectrum-s3-15xvv/perpay_accounting_datamart/rymax_statements/\\nEvent rule rule-perpay_accounting_datamart-crawler will trigger lambda function that will trigger the perpay_accounting_dtaamart-\\ncrawler to run',\n",
       "  \"Looking Forward\\nGoing forward, we need to check out stateful ingestion. If it keeps our old UI state and only updates the tables, that's perfect. There is likely \\nsome middle ground where it won't update all table structure or the UI, and we can't pick and choose what we'd like. This should be tackled \\nsomehow with regards to Redshift ingestion overwriting the UI. A patched fix would be running a script right after ingestion that references a \\ntable that holds state between ingestion. This isn't too clean - it'd be better to have DataHub run as we'd like. \\nIn regards to the Airflow pushes leaking datasets to the DataHub UI/frontend, there's a new feature introduced in the latest DataHub version \\n(0.13.3) called materialize_iolets. This seeks to tackle our exact problem in a true/false config. If it's actually that simple, that part of the \\nequation would be fixed. We shall see.\",\n",
       "  '620\\nDMS\\nVideo topic ideas:\\nWhat is DMS?\\nWhat do we use DMS for (segment, magento, core)?\\nWhat could cause a DMS task to fail?\\nWhen should you stop, resume, full reload a DMS task?\\nHow do I debug a DMS failure?',\n",
       "  '612\\nCensus\\nVideo topic ideas\\nWhy do we use it?\\nHow to set up new syncs?\\nTricks for making the most out of our allotted fields',\n",
       "  'table. This is used in downstream processes.\\nIn perpay_general_datamart_int.returns_issues_fact, we join on house_sku, with the assumption that house_sku is unique. This creates \\nduplicate rows in the Returns Issues Data Model, which in turn causes harsh tests to fail, and Domo will not update.\\n \\nAdditionally, the problematic SKUs seem to self-resolve and new ones pop up.\\n \\nDH-2007813\\nThe parent SKU DH-2007813 has a parent SKU PWH-PER 2007813 which has a parent SKU DH-2007813 [PROD] - SPECTRUM - WARNING: Test for Parent/Child Multisourcing relationship failed, please check the logs\\nProblematic SKUs on 01/03/2023\\n1becba55-\\na279-4bd2-\\na9d2-\\nfd502bd570f83414d14e-\\na3eb-4db8-\\n9b9d-\\n7c2eb440921\\nePPA-\\nB084LMTR98DH-2007813 Perpay - \\nAmazonD&H 2020-03-27 \\n19:00:59.0032\\n282021-01-13 \\n20:30:11.1905\\n57\\n3414d14e-\\na3eb-4db8-\\n9b9d-\\n7c2eb440921\\neaf87000b-\\ncace-4512-\\nabb3-\\n1fc4c1528849DH-2007813 PWH-PER \\n2007813D&H Pottstown \\nWarehouse2021-01-13 \\n20:30:11.1905\\n572021-02-03 \\n20:30:17.6067\\n57\\naf87000b-',\n",
       "  '27\\n28select a.borrower_id as internal_borrower_id,\\n29       b.borrower_id as sftp_borrower_id,\\n30       a.src_dt,\\n31       a.days_past_due as internal_days_past_due,\\n32       b.days_past_due as sftp_days_past_due,\\n33       a.current_balance as internal_current_balance,\\n34       b.current_balance as sftp_current_balance,\\n35       a.total_successful_payment_amount as internal_total_successful_payment_amount,\\n36       b.total_successful_payment_amount sftp_total_successful_payment_amount,\\n37       a.deserve_card_account_status as internal_deserve_card_account_status,\\n38       b.deserve_card_account_status sftp_deserve_card_account_status,\\n39       a.deserve_card_account_status_reason as internal_deserve_card_account_status_reason,\\n40       a.minimum_payment as internal_minimum_payment,\\n41       b.minimum_payment sftp_minimum_payment,\\n42       a.statement_balance as internal_statement_balance,\\n43       b.statement_balance sftp_statement_balance,',\n",
       "  \"1...\\n2create table {{temp_schema}}.borrower_attribute_1a as\\n3  select distinct a.borrower_id,\\n4         cast(a.created as date) as feature_enrollment_created_dt,\\n5         a.status as feature_status,\\n6         case when c.pay_cycle = 'weekly' then cast(24 as float)/cast(52 as float)\\n7              when c.pay_cycle = 'bi_weekly' then cast(24 as float)/cast(26 as float)\\n8              when c.pay_cycle = 'semi_monthly' then cast(24 as float)/cast(24 as float)\\n9              when c.pay_cycle = 'monthly' then cast(24 as float)/cast(12 as float)\\n10              else 0 end as per_pay_plus\\n11  --- !!!!!!!!!!!!!!! Uses feature_enrollment and feautre tables\\n12  from {{pub_schema}}.feature_enrollment as a\\n13  left join {{pub_schema}}.feature as b on a.feature_id = b.id\\n14  left join {{temp_schema}}.borrower_attribute as c on c.borrower_id = a.borrower_id\\n15  where b.type in ('perpay_plus', 'perpay_plus_v2')\\n16  and a.status not in ('canceled');\\n17...\",\n",
       "  'summaries of how we understand business concepts like deposits, refunds and payments. This approach with MVs ensures the data \\nis consistent and that there will be no timing errors. The resulting atomic_int tables built off these views allow us to join tables to \\nconceptualize concepts like balance and deposits. The AR MV in the metric layer stores precomputed data chosen for its relevance to \\nbuilding a table that represents a business concept. This organizational approach represents an attribute of our metric layer: \\norganizing data in tables based on business groupings that can serve as building blocks for our reporting layer.\\nDespite this, the only tables that are capable of incremental refresh are related to loan, account, and card since they do not have the \\naforementioned limitations of incremental refresh in their queries. Due to this, most of the MVs in the account_reconciliation schema \\ndo full refreshes.',\n",
       "  'Let’s look into the columns: rdd_success_ind, card_account_active_ind, open_to_buy_ind – these were put into place a long \\ntime ago and there is a ton of overlap now\\nrdd_success_ind: \\n0 when the pay_cycle is weekly and there was an rdd_success beyond/after grace period (3 days) + 7 days = 10 days\\n0 when the pay_cycle is semi monthly or bi_weekly and there was an rdd_success beyond/after grace period (3 days) \\n+ 14 days = 17 days\\n0 when the pay_cycle is monthly and there was an rdd_success beyond/after grace_period (3 days) + 30 days = 33 \\ndays\\n1 otherwise!! (meaning an rdd_success was within the grace_period + timeframe given the pay_cycle)\\ncard_account_active_ind: \\n0 when the deserve card account status is suspended\\n1 otherwise (the card account is considered active if it was not suspended by deserve) → changed this to be 1 when \\nthe account status is ACTIVE, 0 otherwise…. unless we’re trying to capture if the account is frozen or not!',\n",
       "  '11\\nwanted/necessary.\\nUpcoming Improvements\\nDatahub has many, many features. The rollout of Datahub will be step-wise in order to first release with the core functionality and needs in \\nplace, then follow up with further enhancements. The following are some of the functions/services that Datahub provides but will not be a \\npart of the initial launch\\nDomains / Business Terms\\nAn object grouping mechanism to tag assets (schemas/tables/columns/glossary terms) that have related context with respect to the \\norganization of the company. And example could be Risk, were risk appetite data models and tables would be correlated with underwriting \\nrisk scores.\\nData Product\\nThis field highlights what kind of service the object provides. Is it DOMO Reporting, data modeling, API integration, etc. We have the \\ncapabilities to add any “product” we want and tag objects as we see fit so it will be paramount to iron out specific procedures before allowing \\nuse.\\nRelated Terms',\n",
       "  'visits across the two tables have a discrepancy of a few minutes)\\nUpdate: We looked into this issue more closely -- most of the overlapping URLs between Web/Storefront correspond to the paths / and \\n/dashboard (which are known to be shared). If we disregard those, then there are only ~60 overlapping rows, so we think the impact on \\nthe device usage data should be relatively limited. \\n6/card\\n7/orders\\n8/dashboard/\\n9/referrals/broseman4',\n",
       "  'schemas.\\nreporting_daily\\ndata_model_finance_planning\\ndata_model_intercom_conversations\\ndata_model_ecommerce \\ndata_model_platform_credits \\ndata_model_daily_summary \\ndata_model_ecommerce \\ndata_model_messaging_daily \\ndata_model_payment_plans \\nmarketing_product_catalog (writes to staging S3 bucket)',\n",
       "  '522\\n \\n12261:            LEFT JOIN {{pub_schema }}.withdrawal_request_status b on a.withdrawal_id = b.withdrawalrequest_id\\n13\\n14dags/analytics/data_model_accounting/account_reconciliation/core_account_daily_rollforward/TEMP_core_account_dai\\n15269:            {{pub_schema }}.withdrawal_request a\\n16271:            LEFT JOIN {{pub_schema }}.withdrawal_request_status c on a.id = c.withdrawalrequest_id AND c.stat\\n17272:            LEFT JOIN {{pub_schema }}.withdrawal_request_status d on a.id = d.withdrawalrequest_id AND d.stat\\n18273:            LEFT JOIN {{pub_schema }}.withdrawal_request_status e on a.id = e.withdrawalrequest_id AND e.stat\\n19274:            LEFT JOIN {{pub_schema }}.withdrawal_request_status f on a.id = f.withdrawalrequest_id AND f.stat\\n20311:                LEFT JOIN {{pub_schema }}.withdrawal_request_status b on a.withdrawal_id = b.withdrawalreques\\n21\\n22dags/analytics/data_model_accounting/account_reconciliation/account_balance.py',\n",
       "  'AWS Data Migration Service (DMS)95\\n           The Source Layer96\\n                Handling Upstream Schema Changes - Considerations104\\n      Coding standards109\\n           SQL Queries110\\n           Python114\\n           Formatting SQL with SQLFluff117',\n",
       "  'Further work has been scheduled for a later date.\\nIn Progress: The initiative has been researched, pitched and approved for implementation. Implementation is in progress.\\nBacklog: The initiative has had some research or is set to move forward, but needs more time or resources to continue.',\n",
       "  \"443\\nThe logic for detail will need to change. ❌\\nCard Borrower History Daily Fact\\nSimilar to card user fact and card user detail, we use enrollments to get enabled timestamp for the card, but are not slimming back the \\npopulation. The population is determined by card accounts through deserve_accounts_eod_report. Here’s where setting the enabled ts \\nhappens.\\n8            min(created) as first_card_enabled_ts\\n9        FROM {{pub_schema}}.feature_enrollment\\n10        WHERE feature_id = 67\\n11        GROUP BY borrower_id\\n12    )\\n13    SELECT\\n14        a.user_id,\\n15        c.id AS company_id,\\n16        c.name AS company_name,\\n17        h.rdd_applied_for,\\n18        CASE\\n19            when position('semi_monthly' IN b.pay_cycle_json) > 0 then 'semi_monthly'\\n20            when position('monthly' IN b.pay_cycle_json) > 0 then 'monthly'\\n21            when position('bi_weekly' IN b.pay_cycle_json) > 0 then 'bi_weekly'\\n22            when position('weekly' IN b.pay_cycle_json) > 0 then 'weekly'\",\n",
       "  \"Furthermore, permissions should be viewed as dynamic entities, subject to regular review and adjustment. As roles evolve and \\norganizational needs shift, permissions must be reassessed to align with the principle of least privilege.\\nBy adhering to these principles, we fortify our data governance framework and demonstrate our commitment to responsible data \\nstewardship. Let's collaborate diligently to ensure that access privileges are granted judiciously and revoked promptly when no longer \\nnecessary.\\nPermissions are organized by teams, aligning with the duties of employees. Each team has its own set of privileges, categorized into \\ndifferent levels. \\nProduct\\nProduct Users: @Michael Bohs @Conor Horan @Sean McLoughlin \\n---\\nAccounting\\nAccounting Users: @Preston Mahoney @Nick Sample \\n---\\nEngineering\\nEngineering Users:  perpay_engineering\\n---\\nMarketing\\nMarketing Users: @Nikos Petrides \\n = Current, \\n = No Access\\nproduct_workspace\\n   \\n  \\n  \\ndeserve_data_int\\n   \\n  \\n  \\nshared_resources_int\",\n",
       "  'Completed Direct Deposits: A completed direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of completed_accounting. The amount of the row is applied as a deposit to the associated \\nborrower_id on the date of the created_ts timestamp. The completed_accounting status comes before the completed timestamp \\nthat is used in engineering tie-out. The completed_accounting timestamp more closely represents when Perpay actually receives the \\nmoney, whereas the completed timestamp more closely represents when Perpay can attribute the deposit to a user.\\nReturned Direct Deposits: A rejected direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of returned_accounting. The amount of the row is deducted from the core balance of the associated \\nborrower_id on the date of the created_ts timestamp.\\nPayment Tracking',\n",
       "  'reconcile.py (data_model_accounting/daily_account_reconciliation_data_model/reconcile.py)\\ncore_account_daily_rollforward.py\\nreconcile.py (data_model_accounting/account_reconciliation/reconcile.py)\\ndebit_cards.py (data_model_accounting/account_reconcilliation/credit_cards.py)\\nach_payments.py\\naccount_balance.py\\ndata_model_accounting_dag.py\\ngenerate_am_workable_file.py (call_lists/lib/queries)\\nborrower_company_form_check.py\\ninbound_phone_test_report.py\\n1-- Only returns the payments in the auto_pay_payment table\\n2SELECT \\n3  a.*\\n4FROM\\n5  perpay_accounting_datamart_ext .deserve_payments_eod_report a\\n6  LEFT JOIN public.card_cardpayment b on a.payment_id = b.deserve_id\\n7  INNER JOIN public.auto_pay_payment c on b.id = c.card_payment_id',\n",
       "  '21\\nNotes\\nSchema names: schemas are suffixed with _ext if they are external schemas, and _int if they are internal. External schemas can only contain external tables, and internal schemas \\ncan only contain internal tables. This will not impact how to read from the tables.\\nTechnical Notes\\nSegment: There is no tracks table. Also, Segment data is stored in S3 in a separate AWS account, queried from Glue and Redshift in the staging/production accounts.\\nInteracting with external tables: the Redshift Spectrum .py class handles the boiler plate interactions with external tables. More info in https://github.com/Perpay/perpay-\\nairflow/blob/feature/spectrum-cont-e/REDSHIFT_SPECTRUM_README.md .\\nAuthenticating with AWS: to do anything outside of querying Redshift, you will need your personal IAM keys. More information on this will be found in the \\nREDSHIFT_SPECTRUM_README.md.\\nMore Technical Notes',\n",
       "  '4. Engineering sends something to Deserve letting them know a payment has been made with a flag whether or not to hold the payment \\nout (whether it’s an ACH payment)\\n5. Deserve receives the message, determines how long the payment should be held out for, and then sets payments in hold-out to \\nPROCESSING state for that duration\\n6. Post-hold, the payment in the public.card_cardpayment hits a COMPLETED or RETURNED state\\na. If the payment is completed, nothing happens to the card account balance.\\nb. If the payment is returned, the corresponding fees will post the following cycle\\nWhat this means\\nAll this means is the holdout period logic lies on Deserves side now, and the public.card_cardpayment table gets populated earlier \\nthan it did before. Even though the holdout period logic is variable now instead of 2 days, I don’t think this changes much of anything. In',\n",
       "  \"One can view payment_achtransaction as the ACH “analog” of auto_pay_payment\\npublic.ach_transaction_history serves as the history table for public.ach_transaction. That is, there exists a 1-to-many \\nrelationship between ach_transaction and ach_transaction_history (the same transaction could have reached multiple \\nstatuses during its history). \\n \\nThe following diagram illustrates the information above:\\n \\nEngineering’s Deposit Process\\nauto_pay_payment and achdetail deposit_ids are mutually exclusive. These tables both trickle into the deposit table. The \\ndeposit table also receives data from scheduled_payment table. Occasionally we also get physical checks which don't have an \\nautomated process as they are such a small population of our deposits.\\nauto_pay_payment process:\\nUser creates a schedule through the UI.\\nWhen the scheduled time is up, engineering creates an entry in auto_pay_payment.\",\n",
       "  '617\\nJump Boxes\\nVideo topic ideas:\\nwhat are the jump boxes and why do we use them?\\nhow do we access the jump boxes?',\n",
       "  'provide continue to be accurate and up to date long term. Our testing & validation guidelines can be found here. \\nWhat standards are required for code integrations?\\nCode Review performed by another Data Engineering teammate \\nIf the code is updating something using business logic (not just raw data), it is recommended that a code review/QA is performed by a \\nStrategic Analyst with the relevant business context.\\nCode has been thoroughly tested in the Staging environment\\nAppropriate testing has been implemented to ensure data and code accuracy\\nCode follows our standards for quick & efficient queries \\nTODO: document these standards\\nCode is written so that it is as easy as possible for other members of the team to easily review and understand one another’s work\\nAppropriate logging has been implemented to enable easy debugging.\\nWhat are the overall responsibilities of a member on the DE team?',\n",
       "  \"Codezip_code Report 5 or 9 digit zip codes. zip of user_attribute, or if not \\npresent, 5 digit zip of magento \\naddress. \\nAddress \\nIndicatoraddress_indicato\\nrIf the address is reported, can be \\nconfirmed, military, or other statuses.Y = consumer’s known address  \\nResidence Coderesidence_code O = Owns, R = Rents code. Blank  \\nSegment \\nIdentifiersegment_identifi\\nerReporting segment on employer info: N1  \\nEmployer name employer_nameName of employer company_name if not like Other \\n> user_attribute.co\\nmpany_name\\nFirst Line of \\nEmployer \\nAddressemployer_addre\\nss_line1 Blank  \\nSecond Line of \\nEmployer \\nAddressemployer_addre\\nss_line2 Blank  \\nEmployer City employer_city  Blank  \\nEmployer State employer_state  Blank  \\nEmployer Postal \\n/ Zip Codeemployer_zip  Blank  \\nOccupation occupation  Blank  \\nReserved reserved3 Blank   \\n1 case when m.borrower_id is not null then 'DA' -- Delete merged account\\n2      when n.borrower_id is not null then '13' -- Paid in full, canceled Perpay Plus\",\n",
       "  \"7       a.completed_at,\\n8       a.sftp_dt,\\n9       b.id as perpay_id,\\n10       b.created,\\n11       b.amount,\\n12       b.card_account_id,\\n13       b.status\\n14from users_frank.card_sftp_payment as a\\n15left join public.card_cardpayment as b on a.payment_id = b.deserve_id\\n16where a.current_ind = 1\\n17and b.deserve_id is null\\n1\\n2select case when account_id in ('149eaf20-8288-4ce3-87da-425b0ed06934', 'bf085378-a8ad-4244-b921-9f926d1f7416', \\n3          then 1 else 0 end as deserve_employee_ind,\\n4       a.id as perpay_id,\\n5       a.created,\\n6       a.amount as perpay_amount,\\n7       a.card_account_id,\\n8       e.email,\\n9       d.deserve_id as deserve_account_id,\\n10       a.status as perpay_status,\\n11       b.payment_id,\\n12       b.account_id,\\n13       b.payment_status as deserve_payment_status,\",\n",
       "  '265\\nUpdate Card Iterable Card Object without Data Model Full\\nSometimes there is a need get updated card object data into iterable due to Marketing wanting to release new invites/enablements. Waiting for everything to complete, including DM Full (as of right now) \\ncan take hours. The below is a method to perform the update in 15 minutes.\\nNote: Any change to the census_source_int table will be reflected in Iterable. If the table is left deleted, Census will begin deleting records from Iterable. For right now, only JD should do the following…\\n1. Arik/Engineering confirms enablements are sent\\na. Check the feature enablements to make sure they are updated\\n2. Kick off the metric layer dag and let complete\\n3. Kick off the card data model dag and let complete\\n4. Run the following\\n1-- Creates a table that has the new card object and the uuid\\n2drop table if exists users_herr.card_update;\\n3create table users_herr.card_update as \\n4with ca_cte as (\\n5    select\\n6        borrower_id,',\n",
       "  '394\\nEngineering Commerce Loan Daily Rollforward\\nData Dictionary\\nuser_id Integer The user id associated with the loan being \\nobserved \\nloan_id Integer The id of the loan being observed  \\nsrc_dt Date The observation date  \\nobservation_day Integer The total days of observation on the loan at \\nthe src_dt \\nloan_status Varchar The last accounting-relevant loan status \\n(awaiting_payment, repayment, \\ncharged_off, complete, refunded) \\nstarting_balance Numeric(\\n38,2)The starting balance on the observation date. \\nOn the first day of observation, it is sourced \\nfrom the balance ledger. On every other day it \\nis derived from the previous day’s ending \\nbalanceCalculating \\nDaily \\nBalances\\npayments Numeric(\\n38,2)Payments applied to the loan balance on the \\nobservation date (not including borrower \\ncredits)Payment \\nTracking\\nborrower_credits Numeric(\\n38,2)Borrower credits applied to the loan balance \\non the observation datePayment \\nTracking\\nreturned_payments Numeric(',\n",
       "  'on. These result in the Redshift query engine not putting together the best graph for running queries and leads to all the stated \\nproblems. Additionally, even though we store some tables externally, we are not using any of the benefit of this kind of storage, we just \\nincur slower query times. We cannot store very large tables externally without setting the correct dist, sort, and partition keys.\\nUtilize AWS Glue for all JSON/CSV ETL\\nHistorically, when we had to add a json object into our DB, we first had to pull it down onto the worker node, flatten with python and \\npandas, then write it back to the DB and insert. The amount of very specific code that is required for this kind of ETL is significant as',\n",
       "  \"red.\\nmarked as Awaiting Decision: it will be kept in the sheet with this status on the next job run. This means that you're discussing w \\nvendors. Row turns yellow.\\nmarked as Not Addressed: default status for new records. If the next job runs and there were old records under this status, they will be \\nkept on the sheet. Row is uncolored.\\nAnother field that may be changed by accounting is the chosen_payment_option field. This gives them liberty to decide whether the \\ninvoice prices or PO (ecommerce) prices should be reflected in QB. Again, this is a drop town switch to prevent as many manual input errors \\nas possible when picking an option. When the status of a row changes, the sheet will maintain its update the following run.\\nIf chosen_payment_option is…\\nmarked as Invoice Price: default status for new records. The prices sent to QB (and in the accounts_payable_int.po_matches table) \\nwill be the invoice prices.\",\n",
       "  '94\\nIncidents\\nThis is not set up yet, but can be set up quickly and easily.\\nDeclaring an incident\\nAn incident is declared if the issue is or might impact customers or services\\nWhen an incident is declared, a screenshot of a graph on dashboard is taken to note it, assigning it a severity and leaving a message \\nabout it\\nCan assign users within the organization to the incident\\nIntegration with slack- can communicate in Slack channel or on DataDog UI\\nSlack: automatically create slack channel for each new incident or send all incident updates to a global channel\\nhttps://app.datadoghq.com/incidents/introduction\\nIntegrations\\nOur current list of installed integrations is as follows:\\nApache Airflow\\nAWS\\nWe configured this integration by setting up the CloudWatch API (CloudFormation), as outlined in Getting Started with AWS . The \\nDatadog AWS integration crawls the CloudWatch API for AWS-provided metrics. \\nCloudFormation',\n",
       "  \"573\\nSylvia's Transition\\nThis space will include documentation for Sylvia’s transition, prior to 12/23/2022.\\n \\nTable of contents:\\nCalculating AWS Costs \\nSegment Data Ingestion (via S3) \\nMulti-Sourcing IssueARCHIVED \\nAccounts PayableUNDEFINED \\nAccounts that need new owners \\nDomo Data GovernaceARCHIVED \\n \\n \\nOther action items:\\n Transfer file ownership to JD\",\n",
       "  '542\\nemail A unique identifier for a user in the \\nIntercom systemAny email in our database\\nborrower_status_at_creation The status of the borrower based \\non their latest valid deposit before \\nthe conversation was createdChargedOff, Complete, Current, \\nB1, B2, B3, B4, B5\\nrisk_tier_at_creation The borrower’s risk tier as of \\nconversation creationT1, T2, T3, T4\\nconversation_parts The number of messages in the \\nconversation, excluding bot \\nmessagesPositive Integers\\nuser_responses The number of responses by the \\nuser to the initial messagePositive Integers\\nintelegencia_responses The number of responses by \\nIntelegencia to the initial messagePositive Integers\\nperpay_responses The number of responses by a \\nPerpay employee to the initial \\nmessagePositive Integers\\nbot_messages The number of bot messages in \\nthe conversationPositive Integers\\nconversation_created_ts The timestamp of conversation \\ncreationTimestamp\\nlast_message_ts The timestamp of the latest \\nmessage in the conversation Timestamp',\n",
       "  \"Redshift\\n1-- Redshift\\n2-- Note: Concurrency scaling is when AWS has to pull in additional resources when\\n3--   the cluster is under heavy load (near 100% CPU)\\n4select * from (\\n5select\\n6concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\\n7cast(case\\n8 when usagetype = 'Node:ra3.4xlarge'  then 'Compute'\\n9 when usagetype = 'RMS:ra3.4xlarge'  then 'Storage'\\n10 when usagetype = 'CS:ra3.4xlarge'  then 'Conc. Scaling'\\n11 when usagetype = 'USE1-DataScanned'  then 'Spectrum Scan'\\n12 else ''\\n13 end as varchar(100))as usage_type ,\\n14 totalcost:: float\\n15from users_herr .aws_bill_total\\n16where productcode = 'AmazonRedshift'  and\\n17   linkedaccountname is not null and \\n18   linkedaccountname = 'Production Account'\\n19   --linkedaccountname = 'Staging Account'\\n20   and totalcost > 0\\n21   and invoiceid is not null\\n22)\\n23pivot (sum(totalcost ) for usage_type in ('Compute' , 'Storage' , 'Conc. Scaling' , 'Spectrum Scan' ))\",\n",
       "  \"standard operating procedure and style considerations for any DRAAFT team member to contribute to Datahub in order to maintain \\nconsistent organization and rigorous quality.\\nMetadata Locations\\nDatahub offers four key areas where detailed, long-form information can be added by our team. While short-form information like table \\nnames, column names, and data types is automatically pulled in, it's up to us to enrich the catalog with meaningful context. This section \\nhighlights the specific locations where we need to add comprehensive descriptions and provide links to representative examples, ensuring \\nthat our data is both accessible and understandable.\\nSchema Descriptions (here)\\nA schema represents the highest level of data organization within the Perpay data warehouse. It sets the overarching theme for all data sets \\nand data models it contains. Adding descriptive information to any schema accessed by stakeholders will give them a clearer understanding\",\n",
       "  \"67\\nPayment Rating:\\nPayment History Profile:\\nFor each month, the account status is calculated and the code for that month is calculated as follows:\\nData Transmission:\\nThe Metro 2 file will be uploaded onto an SFTP server provided by Experian. This is when the data will be added to the formatted Experian \\ntable so we can keep track of what has been sent to Experian.\\n3      when p.borrower_id is not null then 'DF' -- Delete an account due to confirmed fraud (block notification)\\n4      when b.borrower_status in ('Current', 'B1', 'Complete') then '11' -- Current account. Q: OK complete borro\\n5      when b.borrower_status = 'B2' then '71' -- 30-59 days past due\\n6      when b.borrower_status = 'B3' then '78' -- 60-89 days past due\\n7      when b.borrower_status = 'B4' then '80' -- 90-119 days past due\\n8      when b.borrower_status = 'B5' then '82' -- 120-149 days past due;\\n9      when b.borrower_status = 'B6' then '83' -- 150-179 days past due; Only applies to balance <= 50\",\n",
       "  'canceled loan in the loan table.\\nReturned Marketplace Payments: Returned marketplace payments are sourced from the returned_payment table. For every row in the \\nreturned_payment table the amount is considered to be a returned marketplace payment on the date of the created timestamp. The \\nreturned payments are attributed to the user through the borrower_id in the payment table for the given payment_id.\\nUse of Borrower Credits: Borrower credits are sourced from the offers_borrowercreditamounthistory table. For any row in the table \\nwith a status of used or partially_used and a starting_balance - ending_balance greater than zero, the amount equal to the \\nstarting_balance - ending_balance is considered to be a dollar amount of borrower credits used by that borrower_id on the date of \\nthe created timestamp. (NOTE: There are instances of user merges where a payment is attributed to a lead user, but the associated',\n",
       "  'The current data lake house architecture currently has 4 layers built on top of each other (in this order): \\n1. public schema - data sourced by DMS from other departments and sources (raw)\\n2. metric layer - numerically curated data (basic data cleaning and validation done) sourced from public that is also organized to be usable \\nin a business context\\n3. reporting layer - tables designed with a deliberate business context feature in mind\\n4. domo layer - business intelligence tool to report data from the reporting layer\\nIdeally, each layer should source all of its data from the layer directly before it to have an efficient pipeline of data that can be easily \\nvalidated in the metric layer and reporting layer. \\nCurrently, one issue in this architecture is that certain tables in the reporting layer source data directly from the public schema along with the',\n",
       "  \"3    select * from accounts_payable_int.variance_sheet_output_history\\n4);\\n1drop table if exists users_rehmet.variance_sheet_output_history_additions;\\n2create table users_rehmet.variance_sheet_output_history_additions as (\\n3    WITH\\n4    -- Get the most recent record of the invoice numbers your looking for. Because there's no known SKU, don't n\\n5    recent_records AS (\\n6        select\\n7            invoice_number,\\n8            MAX (created_ts) as max_ts\\n9        from accounts_payable_int.variance_sheet_output_history\\n10        where invoice_number in (\\n11            -- add your list of invoice_numbers here\\n12            '2459777-IN',\\n13            '2459847-IN',\",\n",
       "  '500\\nending_balance_calc\\nvariance\\npotential_partial_return_indSource Tables:\\n \\nDefinition:\\nWhat DE believes the ending balance should be after taking into consideration the flow of all relevant customer funds within the day.\\nBusiness Context:\\nIn Engineering terms, a customer has a card account balance if they have funds available to withdraw from their card account. \\nThere are several ways that customer funds have been applied to the card account balance on Engineering’s side for UX purposes, but \\nhave not yet been applied to receivables balances from an accounting standpoint. Therefore this field makes accommodations for the \\nitems listed below to more accurately represent a customer’s card account balance in the report.\\nRollforward Table Logic:\\nWe take the starting_balance and add any transactions, fees, charges, etc within the day that would increment the card account',\n",
       "  '173\\n(code for reference)\\n \\nReferring to the code above, we can analyze how easy_render.py works and what it did well and what it fell short of. Lets first take a look \\nat what it does well and the logic we can migrate:\\n1. Class Design: Continued use of a class (CombinedRenderer) to encapsulate rendering functionality, promoting modularity and \\nreusability.\\n2. Environment-Specific Logic: Retained logic for handling different schemas based on environment variables, maintaining adaptability to \\ndifferent deployment settings.\\n3. Local Configuration: Continued dynamic import of local configurations after paths are set, ensuring context-specific configurations.\\n \\nThese are all things I wanted to add to my new implementation in combined_render.py, however there are also some things that are \\nmissing from the code that needed to be added:\\n1. Lack of Error Handling: Does not handle potential errors, such as missing files, invalid paths, or rendering failures, which could lead to',\n",
       "  \"575\\n \\nThere are some costs that we cannot parse out from Engineering/DS costs (example: EC2 instance). Implementing cost allocation tags \\nwould allow us to tag our resources by department/environment, which would help separate costs further.\\nIf you filter on LinkedAccountName = ‘Production Account’ and ProductCode = ‘AmazonRedshift’, all UsageType categories aside from \\n‘USE1-DataScanned’ are regular Redshift costs. 'USE1-DataScanned’ is the additional cost of Spectrum, scanning objects in S3 via \\nRedshift. \\nOther resources:\\nRedshift Pricing\\nRedshift Spectrum Query Charges\\nRedshift S3 Pricing\\nCost Efficiency @ Scale in Big Data File Format (Uber blog post)\\n \\n4where invoiceid is not null)\\n5select sum(totalcost) as product_cost\\n6from remove_totals\\n7where linkedaccountname = 'Segment Account';\",\n",
       "  'recommendations.\\nhttps://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/Databricks \\nDelta LakeAbility to SQL query the Data Lake\\nHighly performantCannot be a source connected to Domo, so would need \\nseparate data warehouse\\nCannot be a source for DMS (or other popular CDC frameworks \\nfor that matter), so creating a staging environment and putting \\ndata into the above mentioned data warehouse would require an \\nextra intermediate storage\\nCannot be a destination for DMS, would require using another \\nCDC framework\\nCannot be a destination for Segment\\nSnowflake Highly performant\\nCan function as both data warehouse and \\ndata lakeCannot be a source for DMS (or other popular CDC frameworks \\nfor that matter), so creating a staging environment and putting \\ndata into the above mentioned data warehouse would require an \\nextra intermediate storage\\nCannot be a destination for DMS, would require using another \\nCDC framework\\nAWS S3 /',\n",
       "  '346\\n \\nMerging card accounts:\\n \\nRec Logic:\\n18    DISPUTE_TYPE = \"dispute\"\\n19        - if DISPUTE_CREATED update reference on transaction\\n20\\n21Event Names\\n22    TRANSACTION_UPDATED_NAME = \"transaction_updated\"\\n23        - create or update transaction from api data (possible to use webhook)\\n24    TRANSACTION_DELETED_NAME = \"transaction_deleted\"\\n25        - delete transaction\\n26    REWARDS_CREATED_NAME = \"rewards_created\"\\n27        - attempt to redeem rewards from api with more generous exponential backoff to account for rewards somet\\n28    REPORT_GENERATED_NAME = \"report_generated\"\\n29        nothing\\n30    REWARDS_REDEEMED_NAME = \"redeem_to_external_system\"\\n31        nothing\\n1Let’s say we want to merge user A into user B.\\n2General validation first:\\n3- The merge will fail if multiple users have card accounts\\n4- The merge will fail if  user B already exists on Deserves end.\\n5- The merge will fail if user A has an application in awaiting payment.',\n",
       "  '626\\nDatadog\\nhow we monitor using datadog and how to set up new monitoring?\\nmaking dashboards\\nhow to not spend all of DE’s budget on datadog',\n",
       "  'Using a Robust Data Loader\\ndlt’s (data loading tool) headline slogan on their front page is: the python library for data teams loading data in unexpected places. This \\ncaught my eye.\\nIt is an open-source python library meant to fit into and supercharge loading capabilities in data deployments. In the “ELT” acronym, this \\nlibrary is part of the “E” or “L” step for both open source and internal sources. They have special capabilities that can be beneficial for \\nhandling upstream schema changes.\\nHow dlt handles schema changes and data contracts\\nThis resource will allow new tables (both child tables and tables with dynamic names) to be created, but will throw an exception if \\ndata is extracted for an existing table which contains a new column.\\nSetting up the contract \\nYou can control the following schema entities:\\ntables - contract is applied when a new table is created\\ncolumns - contract is applied when a new column is created on an existing table',\n",
       "  '452\\nIterable Send\\nThe Iterable Customer Data Table / Generate Data Task\\nThis impacts what we’re sending to iterable in the card dict. Particularly, the enrollment table dictates the 1 or 0 flag, enabled_ind, for the \\ncard. \\nIn another spot, it’s used for vantage score. I think this part will be unaffected by the cards in the wild, as we look for feature_ids of 35 and \\n133 which seem specific to scores (I think are pp+ features only).\\nMarking generate_data (iterable send) as ❌  since it’s affected in at least one way by the enrollment table. \\nAccount Rec\\nAccount Balance\\nThe only time enrollments are used are for pp+ revenue v non pp+ revenue as fields perpay_plus_revenue and \\nother_feature_revenue. Any other feature revenue, whether involving in the wild cards or not, would fall under other_feature_revenue, \\nso this file should be unaffected. \\n1...\\n2    card_dict_cte as (\\n3        select\\n4            b.id as user_id,\\n5            p.card_account_id,',\n",
       "  \"is null). The final table is saved to the temp_int schema, making temp_int.invoicing_docparser_output.\\n(3) check if build_matching_tables is true or false\\nif true (default)- we enter the block\\n1: Logs '[InvoiceMatching] Executing matching transformation queries, populating google sheet...'\\n2: Calls _execute_matching_queries. This creates an empty external table for the new entries in quickbooks. It’s called \\nperpay_accounting_datamart_ext.invoicing_new_quickbooks_entries\\n- Heads up, the table will throw a message every time explaining it won’t be created because it already exists. \\nThis function then creates the temp_int.invoicing_master_invoice and temp_int.invoicing_level_match tables in \\nredshift via dags/analytics/invoice_matching/lib/queries/generate_invoice_matching_query.py.  \\n- The invoicing_master_invoice table does string manipulation of the vendor data using the \\ntemp_int.invoicing_vendor_key_matching table (created in step 2.2) joined onto the\",\n",
       "  '203\\nAccounts Payable/Invoice Matching Procedures',\n",
       "  \"time of matching the ACHDetail. Thus, \\nthere should be no variance.\\nJesus seems to be a one-off case that \\ndoesn’t even exist in our table today. I’m \\nassuming Preston looked at the table the \\naround same day the movement occurred, \\nbut a new deposit hadn’t been created yet \\nand the old hadn’t been cleaned. \\nSo, marking this complete. Let me know if \\nanyone has questions.\\n-Abby\\n6 NOT ST… TBD Unknown, leftover variances: Once all of the above are \\nresolved, we will dig into the other potential sources for any \\nleftover variances.Abby & Andrew are actively hunting for \\nadditional variance cases and will include \\nthem in the list below this one. There is a \\npossibility of some cases overlapping with \\nthose mentioned above. However, if it's not \\nevident that they overlap (our initial \\nassessment suggests they do not), they will \\nstill be included for further examination.\\n7 NOT ST…  Missing accounts (link): Some users don’t exist in this table\",\n",
       "  '137\\n(Error) Card Metric Layer Failures\\nIn the airflow_prod_alerts channel:\\nor a similar error where a bunch of card failures occur\\nIn airflow:\\n[PROD] ERROR has occured!\\nDAG: metric_layer\\nTask: Test_CardBalance\\nError: Bash command failed. The command returned a non-zero exit code 1.\\n[PROD] ERROR has occured!\\nDAG: metric_layer\\nTask: Test_CardMinPayment\\nError: Bash command failed. The command returned a non-zero exit code 1.\\n[PROD] ERROR has occured!\\nDAG: metric_layer\\nTask: Test_CardCreditLimit\\nError: Bash command failed. The command returned a non-zero exit code 1.\\n[PROD] ERROR has occured!\\nDAG: metric_layer\\nTask: Test_CardTrans\\nError: Bash command failed. The command returned a non-zero exit code 1.\\nfor Test_CardBalance\\ndef test_for_nulls(self):\\n     # test for if any nulls are present in the table\\n     query = f\"\"\"select\\n                     *\\n                 from {self.test_schema}.card_balance\\n                 where id is null or\\n                     card_account_id is null or',\n",
       "  'issues arise. Because the e-commerce PO data is on the item-level, we expect each commerce PO sku and po_number to match invoice \\nsku and po_number. These are the attributes used to join the tables together. In addition, we expect invoice line items to match invoice \\nsummaries on file_name_<hash_unique_to_file>, which is used to join invoice data together.Zapier access keys can be found in 1pass under AWS Access Key- prod-zapier-external-user. These allow Zapier to connect \\nto the Amazon S3 PROD account.\\nThe data is brought into a table named ecomm_po_data in the accounts_payable_int schema.\\nThe query that is executed to create the table is in ingest_ecomm_po_data.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/ingesting_data/ingest_ecomm_po_data.py).\\nThe data is brought into a table named qb_bills in the accounts_payable_int schema.\\nThe query that is executed to create the table is in qb_bills.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/ingesting_data/qb_bills.py).',\n",
       "  \"application. Thus, in a broader sense, each report seeks to track every day of an account’s active history and provide details on all \\nrelevant debits and credits to the account at the aggregate level.\\n \\nTable of Contents\\nEach page will lay out our report definitions and calculations. These are based on DE’s understanding and may be incorrect. Through the \\nhelp of the teams listed above, we hope to correct them all.\\nCard Report Definitions\\nCard Report V ariances\\nCore Report Context\\nCore Report Definitions\\nCore Report V ariances\\nCommerce Report Definitions\\nCommerce Report V ariances\\nTying out Engineering's Account Balance Flow\",\n",
       "  '86        ELSE \\'2%\\'\\n87        END as rewards_multiplier ,\\n88    CASE\\n89        WHEN c.provisioned_date < \\'2024-03-29\\'  THEN (b.payments * .03)::decimal(10,2)\\n90        ELSE (b.payments * .02)::decimal(10,2)\\n91        END as \"Total Rewards Validation\" ,\\n92    a.total as \"Total Rewards Applied To Account\" ,\\n93    CASE\\n94        WHEN c.provisioned_date < \\'2024-03-29\\'  AND (b.payments * .03)::decimal(10,2) <> a.total THEN \\'Fail\\'\\n95        WHEN c.provisioned_date >= \\'2024-03-29\\'  AND (b.payments * .02)::decimal(10,2) <> a.total THEN \\'Fail\\'\\n96        ELSE \\'Pass\\'\\n97        END as \"Pass/Fail\"\\n98FROM\\n99    rewards_redeemed_summed a\\n100    LEFT JOIN card_payments b on a.borrower_id = b.borrower_id\\n101    LEFT JOIN card_provisioned_date c on a.borrower_id = c.borrower_id\\n102  ;\\n1WITH\\n2-- Collect a random sample of 500 borrowers that were active in Quarter.\\n3random_sample as (\\n4    SELECT distinct\\n5        a .borrower_id ,\\n6        a .deserve_card_account_id\\n7    FROM',\n",
       "  'ones are fine. \\nIn summary,\\n[PROD] ERROR has occured!\\nDAG: dag_organizer\\nTask: data_model_full.Test_MarketingDailyDataModel_Domo\\nError: Bash command failed. The command returned a non-zero exit code 1.\\nFAILED \\n../../opt/***/dags/analytics/data_model_full/tests/test_marketing_daily_data_model_post_iterable.py::TestMarketingDailyDataModel::t\\nest_row_count_marketing_daily_data_model\\nUTM campaign problem\\nResult of a workflow change\\nIterable definition Iterable value Our definition (in the \\ntemp_dev_int.iterable_\\ndaily_insights tables)Our value Meaning',\n",
       "  '160\\neven further down \\n(4x dups per \\nuuid+session_id \\ncombo).\\nUnsure if this is \\nintentional or not \\nbecause the \\nrecords have \\ndifferent \\ninformation, but ran \\na query to count \\nhow many \\ncompanies are like \\nthat and no others \\nare. So reached out \\nto engineering to \\nask\\n05/16/2\\n3https://g\\nithub.co\\nm/Perp\\nay/perp\\nay-\\nairflow/\\ncommit/\\n075d4d\\n34b1dd\\n1cc835\\n768889\\n0656d3\\n1971e1\\n0804 05/01/23 Only send credit \\ncard credit \\nreport to \\nexperian. We needed to make \\nthe borrower \\nids/consumer \\naccount numbers \\nunique between the \\ncard credit reports \\nand the \\nmarketplace credit \\nreports, per this Jira \\ncard. To ensure we \\ndid everything \\ncorrectly and the \\nreport is validated, \\nwe only sent to Lila \\n(experian rep) first. N/A temp- send \\ncard report to \\nexperian onlyN/A Awaiti\\nng a \\nconfir\\nmatio\\nn from \\nLila \\nthat \\nthe \\nfile \\nwas \\nloade\\nd \\ncorrec\\ntly.\\nN/A \\nthis is \\na \\nperma\\nnent \\nchang\\ne now. \\nWe’ve \\ndecide\\nd the \\nsingle \\nrecord \\nwas \\nwonky \\nand \\nshould \\nbe \\nkept \\nout. It  05/09/23 Added a',\n",
       "  \"comes from the public.perpay_house_catalog_product table)!\\n \\n** there has been no resolution to this warning as of now, and the warning keeps becoming more prevalent.\\nDesired resolution\\nProgrammatically determine which relationship is correct, and only add the correct one to the Product Catalog Data Model\\nOR\\nCome to the conclusion that there is nothing wrong with these relationships, and update our logic (i.e., join on parent sku instead of \\nhouse sku in the returns issues data model) accordingly.\\n14    group by 1\\n15    having cnt>1\\n1with multi_sourcing_relationships as (\\n2    select a.id as child_id ,\\n3           b .id as parent_id ,\\n4           a .magento_row as a_magento_row ,\\n5           b .magento_row as b_magento_row ,\\n6           case when not is_valid_json (a.magento_row ) or coalesce (a.magento_row , '{}') = '{}' then null else jso\\n7           case when not is_valid_json (b.magento_row ) or coalesce (b.magento_row , '{}') = '{}' then null else jso\",\n",
       "  'summary of how many rows are present in the record in total, and broken out by account status. Both the header and trailer record are \\nblank filled to be 572 characters long.\\nThen, compliance checks are run. If any of the checks fail, the record will not be sent.\\nCertain columns (such as name, address, account_status, credit_limit, etc) can never be 0 / blank.\\n1An account that establishes a maximum credit limit for a consumer,\\n2such as a credit card or charge account.\\n3Payment amounts are revolving based on the outstanding balance amount.',\n",
       "  \"update for these totals.The code to update the variance sheet lies in a class called ManualPOMatching, within the manual_po_matching_class.py file (PATH: \\ndags/analytics/accounts_payable_revamp/lib/po_matching/manual_po_matching_class.py)\\nIf the rows closer to the bottom don’t turn color upon the ready_for_qb change but are supposed to, you may need to modify the \\nconditional formatting to account for the number of rows in the sheet. It’s currently set up to go to row 1008.\\nThe only case the old records will not be cleared from the sheet: if there are no new records to insert into the sheet and records \\ncurrently on the sheet are marked as ‘Yes' or ‘No’. However, the tables are still being updated, so the ‘Yes’s will still go to the \\npo_matches table and the 'No's are still removed from the po_variances table. The sheet will clear on whichever following run has \\nnew data to insert into it.\",\n",
       "  \"82       original_charge_off_amount,\\n83       date_last_payment,\\n84       scheduled_monthly_payment_amount,\\n85       special_comment\\n86from perpay_risk_datamart.equifax_raw_data as a\\n87left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n88left join (\\n89    select * from public.forgiveness_period\\n90    where type != 'disaster_relief'\\n91    and cast(created as date) >= '2020-12-01'\\n92    ) as c on c.borrower_id = a.consumer_account_number\\n93where c.borrower_id is not null\\n94and enter_date = max_enter_date;\",\n",
       "  '496\\nCommerce Report Definitions\\nThe below fields exist in the perpay_accounting_datamart_ext.commerce_loan_daily_rollforward table. Each expand panel contains \\ninformation regarding the field definition, its business context as understood by DE, the table logic (how it is accounted for in the table), and \\nthe source tables used to calculate it.\\naccount_id\\nborrower_id\\nuser_idDefinition: \\nThe Perpay platform account_id associated with each borrower.\\nBusiness Context: \\nThis serves as the id for the account for anyone using a Perpay product. It is generated by engineering and can be used as a FK to a lot \\nof Engineering’s public tables. \\nRollforward Table Logic:\\n \\nSource Tables:\\n \\n \\nDefinition:\\nA unique id associated with each Perpay borrower.\\nBusiness Context:\\nAn id for anyone who has had a loan hit a revenue status in the past. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables. \\nRollforward Table Logic:\\n \\nSource Tables:\\n \\nDefinition:',\n",
       "  'went to 0. Otherwise, null \\n(open account). \\nDate of Last \\nPayment\\n date_last_pay\\nmentMost recent date a payment was \\nreceived, full or partial.If the account is charged off \\nand the borrower most \\nrecently paid True Accord \\n(after paying us), then the last \\npayment date to TA. \\nOtherwise, the maximum not-\\nnull last_pmt_dt for the \\nborrower.borrower_hist_\\ndaily_status.las\\nt_pmt_dt\\nInterest Type \\nIndicator\\n interest_type_i\\nndicatorF = Fixed, V = Variable/Adjustable \\ninterest.Blank  \\nReserved\\n  reserved2 Blank fill   \\nSurname\\n  surname Last name, cleaned for generation \\ncodes and suffixes. user_attribute.l\\nast_name\\nFirst Name\\n  first_name First name, cleaned for titles and \\ngeneration codes. user_attribute.fi\\nrst_name\\nMiddle Name\\n  middle_name Middle name Blank',\n",
       "  '580\\nRoadmap for Completion\\nThis page serves as a living document for completing the full roll out of the automated AP process. The sections describe the questions and \\nprocedures that still need to be fleshed out.\\nThe overall plan is to push Almo all the way through to act as a POC for the entire (non-csv) process. \\nIn Progress\\nCommerce has been informed that we need to know what level of variance is acceptable between out books and the vendors - we await \\ntheir reply\\nSylvia has reached out to Alec to determine what file format is easiest for him so that we can instruct the vendors to be more uniform. It \\nis assumed that CSV would be easier for us in this process due to the more standerd format compared to a PDF\\nNext Steps\\nNeed to incorporate pagination into how the API response is being handed from Docparser so that we can be sure all data would be \\nmaking it though to Redshift',\n",
       "  '22            \"perpay_plus\": {\\n23                \"segment\": \"Not CR Eligible, Opted In\",\\n24                \"current_backend_status\": \"opted_in\",\\n25            }\\n26            \"activity\": {\\n27                \"date_joined\": \"2019-11-09\", //YYYY-MM-DD \\n28                \"last_login\": \"2020-05-10\", //YYYY-MM-DD \\n29                \"last_transactional_click_dt\": \"2019-12-11\", //YYYY-MM-DD \\n30                \"last_transactional_open_dt\": \"2020-03-09\", //YYYY-MM-DD \\n31                \"last_promotional_send_dt\": \"2020-04-09\", //YYYY-MM-DD \\n32                \"last_promotional_open_dt\": \"2019-12-20\", //YYYY-MM-DD \\n33                \"last_promotional_click_dt\": \"2019-12-20\", //YYYY-MM-DD \\n34            },\\n35            \"company\": {\\n36                \"company_name\": \"CVS\",\\n37                \"company_risk_level\": 2,\\n38                \"payroll_provider_name\": \"Gusto\",\\n39                \"payroll_provider_type\": \"Gusto\",\\n40                \"payroll_portal_url\": \"www.gusto.com\",',\n",
       "  '86\\nMonitoring our Infrastructure\\nTable of Contents\\n1. Introduction to Datadog\\n2. Integration with AWS\\nCouldFormation\\nPrerequisites\\nPost-integration capabilities\\n3. Key Benefits of Using Datadog as a Monitoring Tool\\nComprehensive Monitoring\\nReal-time Alerting\\nEnhanced Troubleshooting\\nCost Optimization\\n4. Costs\\n5. Additional Resources\\n1. Introduction to Datadog\\nWe use Datadog to monitor our infrastructure! Datadog is a popular cloud monitoring and analytics platform that provides comprehensive \\nobservability of the performance and health of our infrastructure and services. With its extensive features and integrations, Datadog helps us \\ngain deep insights, troubleshoot issues, and optimize the efficiency of our systems.\\nOur dashboards: https://app.datadoghq.com/dashboard/lists\\nOur alerts: https://app.datadoghq.com/monitors/manage\\n2. Integration with AWS\\nDatadog integrates with AWS to provide monitoring and observability for our AWS resources. We aim to observe resources such as EC2',\n",
       "  'this product in the last 30 days. Counts for \\nproducts with parents are rolled up to the \\nparent, and this field will be 0.\\ncount_repayment_last_365_days bigint Count loans that went into repayment for \\nthis product in the last 365 days. Counts for \\nproducts with parents are rolled up to the \\nparent, and this field will be 0.\\ncategory_name_1 varchar(94) The category name of the category.\\ncategory_name_2 varchar(94) The category name of the next level sub \\ncategory.\\ncategory_name_3 varchar(94) The category name of the next level sub \\ncategory.',\n",
       "  '6                    from public.feature_enrollment\\n7                    where feature_id in (35, 133)\\n8                    group by 1\\n9                    having cnt > 1;\\n10                \"\"\"\\n1...\\n2create table {{temp_schema}}.ab_test_backend_signup_report_1 as\\n3  select\\n4    a.*,\\n5    b.created as perpay_plus_opt_in_ts,\\n6    b.status as perpay_plus_status,\\n7    d.first_plus_eligible_dt,\\n8    d.current_eligible_ind as current_plus_eligible_ind,\\n9    f.number_of_payments as first_loan_duration,\\n10    g.first_pay_cycle,\\n11    case\\n12      when (g.first_pay_cycle in (\\'weekly\\') and first_loan_duration = 24)',\n",
       "  '17    card_transaction_id ,\\n18    deserve_id ,\\n19    created_ts ,\\n20    transaction_status ,\\n21    rank () over (partition  by card_transaction_id order by created_ts asc) as rank,\\n22    case\\n23        when created_ts = first_value (created_ts ) over ( partition  by card_transaction_id order by created_ts d\\n24        else 0 end as current_ind\\n25from status_1\\n26where ((transaction_status <> transaction_status_previous ) OR (transaction_status_previous IS NULL));\\n27\\n28\\n29select\\n30    count(*) as cnt,\\n31    card_transaction_id\\n32from users_frank .card_transaction_status\\n33group by 2 having cnt > 2 order by cnt desc;\\n1select *\\n2from public.card_transaction_snapshot\\n3where card_transaction_id = 129165\\n4order by created desc;',\n",
       "  '600\\nData Modeling Pipelines & DBT\\nDBT is a tool developed by Fishtown Analytics (go Philly!) to facilitate reproducible pipelines for data transformation. Many organizations \\nhave adopted DBT to scale data engineering while incorporating core software engineering principles like testing, versioning, and \\nenvironment control.\\nResources:\\nDBT intro, on the DBT webpage. DBT is particularly powerful by handling boilerplate code and decreasing code duplication, leaving more \\ntime for data / analytics engineers to create truly custom pipelines. DBT also provides a structure for a testing suite, used for data QA.\\nhttps://docs.getdbt.com/docs/introduction\\nTristan Handy, CEO and founder of Fishtown Analytics (builders of DBT), appears on the Software Engineering Daily podcast, transcript \\nlink below. He explains how DBT raises what the data analyst can do, while elevating SQL queries using Jinja templating, a key to',\n",
       "  \"15  -- !!!!!!!!!!!!!! Unions with other events here\\n16  select user_id, created_ts, event from card_enabled\\n17    UNION\\n18  select user_id, created_ts, event from deserve_events\\n19    UNION\\n20  select user_id, created_ts, event from card_activation\\n21    UNION\\n22  select user_id, created_ts, event from first_card_payment\\n23    UNION\\n24  select user_id, created_ts, event from segment_events_userid\\n25    UNION\\n26  select user_id, created_ts, event from message_userid\\n27    UNION\\n28  select user_id, status_ts as created_ts, event from first_purchase\\n29),\\n30...\\n1...\\n2-- !!!!!!!!!!!!!!!!!!!!!!!!! This CTE captures enabled only\\n3enabled as (\\n4    select\\n5        distinct borrower_id     \\n6    from {{pub_schema}}.feature_enrollment\\n7    where feature_id = 67 and status = 'enabled' \\n8)\\n9....\\n10select\\n11    distinct\\n12    d.id as borrower_id,\\n13    d.user_id,\\n14    d.account_id,\\n15    coalesce(c.id, j.card_account_id) as card_account_id,\\n16    c.deserve_id as deserve_card_account_id,\",\n",
       "  \"437\\nAdjusting for Cards in the Wild\\nWe are going to be moving to cards in the wild soon. This means we won't be able to rely on enablements to set our populations. This page \\nprofiles what this impact will be.\\n \\nFiles\\nCredit Reporting\\nCard Reports\\nMarketplace Reports\\nExperian Pull Data\\nMetric Layer Files\\nCard Activation Events\\nCard Key Relations\\nCard RDD Config\\nPlatform Payments\\nCard Data Models\\nCard User Fact\\nCard User Detail\\nCard Borrower History Daily Fact\\nCard Borrower Account Detail\\nCard Borrower Performance\\nMarketplace Data Models\\nBorrower Attribute\\nLoan Risk Factors Hist\\nPlus Borrower Attribute\\nPlus Borrower Fact\\nPlus Borrower Status History\\nPlus Funnel Fact\\nUser Attribute\\nDaily Summary Plus\\nRisk Data Model\\nRisk Appetite Platform User Fact\\nIterable Send\\nThe Iterable Customer Data Table / Generate Data Task\\nAccount Rec\\nAccount Balance\\nMonthly V antage Scores\\nMonthly V antage Scores\\nNew Test Monthly V antage Scores\\nTest Monthly V antage Scores\\nOthers\\nFS Daily Perpay Plus Revenue\",\n",
       "  'notify on the status of this heartbeat \\nacross one or more hosts, which will give \\nyou a good indication whether your hosts \\nare responsive.EC2 is our only host\\nMetric Metric monitors are useful for a \\ncontinuous stream of data. Metrics \\ncollected via the Datadog Agent or the API \\ncan be alerted upon if they cross a \\nthreshold over a given period of time. \\nAnomaly Anomaly monitors detect when a metric is \\nbehaving differently than it has in the Because historical data is taken into \\naccount, we could use this to track alerts or Type Use (description from DataDog) Notes',\n",
       "  '148\\nAMZ-B07NQNS9CR|B07NQNS9CR\\nIssue: The Child SKU has multiple parent SKUs (parent SKU of top row is extended).\\nBBY-BB20840922-- this one has not resolved since 12/13/2022\\nIssue: The parent SKU BBY-BB20840922 has a parent SKU PS-MX3X2LLA7c2eb440921\\ne\\nb44b6eb3-\\n5b47-46df-\\n9cd5-\\nc3a82fe083b43414d14e-\\na3eb-4db8-\\n9b9d-\\n7c2eb440921\\neAMZ-\\nB084LMTR98DH-2007813 Amazon - \\nAutoD&H 2020-04-03 \\n15:36:02.0237\\n602021-01-13 \\n20:30:11.1905\\n57\\n3a600a47-\\n95e9-4b67-\\n8d39-\\n346ee2562cb\\n1cea3e3e6-\\n111b-4bd5-\\nb5a2-\\n20bd62a8475\\n5AMZ-\\nB07NQNS9C\\nR|B07NQNS9\\nCRPPA-\\nB07NQNS9C\\nR|B07NQNS9\\nCRAmazon - \\nAutoPerpay - \\nAmazon2020-04-03 \\n15:36:03.7739\\n142020-04-01 \\n13:25:24.7782\\n01\\n3a600a47-\\n95e9-4b67-\\n8d39-\\n346ee2562cb\\n162916a69-\\n6608-41f0-\\nb806-\\ne54ae49ec7b\\n5AMZ-\\nB07NQNS9C\\nR|B07NQNS9\\nCRPPA-\\nB07NQNS9C\\nRAmazon - \\nAutoPerpay - \\nAmazon2020-04-03 \\n15:36:03.7739\\n142020-04-23 \\n14:01:08.5576\\n07child_id parent_id child_sku parent_sku child_vendor parent_vendo\\nrchild_created parent_create\\nd\\nfe2b1bcf-\\nd088-4be8-\\n8c5a-',\n",
       "  'capabilities to add any “product” we want and tag objects as we see fit so it will be paramount to iron out specific procedures before allowing \\nuse.\\nRelated Terms\\nThe ability to long glossary terms to one another, with upstream/downstream lineage (manually added). This creates a “knowledge network” \\nwhat would quickly highlight the interconnectivity between terms and concepts.',\n",
       "  '595\\nFocusing on the Deserve SFTP information for this user (the first query):\\nUser was provisioned on 07/11\\nNormally, the first entry in the deserve accounts eod report for this user would be the day that the user was provisioned. However, the \\naccounts eod report was only generated after 7/14. As a result, this user’s first entry in the accounts eod report is 7/14.\\nThe user already has an outstanding balance of $9.00, which is the account opening fee.\\nThis does not show up under the “fees_accrued” field, because this shows up as a transaction. You can run the following query to \\nsee it:\\nOn 11/17, Conor and Deserve discussed this discrepancy (that the account opening fee is not being considered a “fee”, but rather a \\n“transaction”. \\nConor said he would keep DE in the loop about the outcome of this conversation.\\nIn card borrower hist daily, we use logic to consider this transaction a fee, not a transaction.',\n",
       "  \"10                partition by fh.borrower_id\\n11                order by fh.start_time\\n12                rows between 1 following and unbounded following) as end_date,\\n13            fe.feature_id,\\n14            fh.status\\n15        from {{pub_schema}}.feature_history fh\\n16        left join {{pub_schema}}.feature_enrollment fe on fe.id = fh.feature_enrollment_id\\n17        where fe.feature_id in (35, 133) and fh.status in ('enabled', 'disabled', 'canceled')),\\n18...\\n1...\\n2                with plus_status_history as (\\n3                    select\\n4                        b.borrower_id,\\n5                        b.start_time as status_time,\\n6                        cast(b.start_time as date) as status_date,\\n7                        cast(first_value(b.start_time) over (\\n8                            partition by b.borrower_id\\n9                            order by b.start_time\\n10                            rows between 1 following and unbounded following) as date) as end_date,\",\n",
       "  '391\\nWithdrawal Failed Without a Failed Status\\nCases where a withdrawal’s current status is failed, but there is no corresponding failed status in the withdrawal request status table.\\nCard Payment Hits Returned Status and Not Returned to Core Balance\\nCases where a card payment hits a returned status, but the amount of the card payment was not returned to the core balance and the \\ndeposit didn’t fail.\\nUsed Borrower Credit Without Associated Payment\\nCases where a borrower credit was used, but there is no corresponding payment or payment detail from that date for that borrower.\\nDirect Deposit Not Reflected on Core Balance\\nCases where there is a valid direct deposit, but there is no corresponding payment and the deposit does not reflect on the user’s core \\nbalance.\\nWithdrawal Failed and Balance Not Returned\\nCases where there is a failed withdrawal, but the user’s core balance does not reflect this money being added back.5 9 7 1 9 1 2 1 9 3 6User ID with Discrepancy Withdrawal Request ID',\n",
       "  'error), invoice matching is performed at the PO level when the invoice_matching DAG is ran. This will pull in all invoices that have not yet \\nbeen analyzed or automatically sent to Quickbooks. I want there to be a new column in our master table to display which invoices we’ve \\nanalyzed or sent to Quickbooks. Once invoices are pulled, I want to flip the flag from 0 to 1. If there is no pricing issue, the invoice will be \\nimmediately uploaded to Quickbooks. \\nInvoices that fail invoice matching are placed in the Invoice Matching sheet, where manual intervention is required to determine whether the \\nvariances are okay or not based on matches with reports pulled from Magento. Once investigated, the invoices with variances are either \\nmarked “Y” or “N” to indicate whether they should be added to Quickbooks and paid for. \\n^Even though we want to be as automated as possible, manual intervention will still be required for some invoices… that’s just the',\n",
       "  'try to avoid code that’s repeated\\nhelpful to use variables, constants, functions, classes, data structures\\nthis reduces the need to change things in multiple places\\ncontext considerations\\nwho will be reading your comments? what other code does this affect? keep these in mind when documenting and adding comments',\n",
       "  '108\\nHow do you handle upstream schema changes in your pipelines? \\nWhat is a Data Contract? Monte Carlo\\nWhat is a Data Contract? IBM\\nSchema Evolution on the Data Lakehouse Onehouse’s approach of handling upstream data changes - emphasis on backward-\\ncompatible queries\\nCrux Makes Dealing With Schema Changes Easy - are both proactive and reactive\\nInterfaces and Breaking Stuff - dbt’s founder shedding light on the importance of the conversation around the lack of contracts\\nSchema Evolution and Compatibility for Schema Registry on Confluent Platform | Confluent Documentation - offer a “Schema \\nRegistry” with the purpose of versioning schemas\\ndlthub \\n⭐ Schema Evolution | dlt Docs a well written out approach to schema evolution, and the tasks of numerical curation and business-\\ninfused structuring that data engineers have\\nGoogle Colab - DLT demo',\n",
       "  'vendor. They’ll reach back out when they’re done!\\n6: Finally, DE will turn the correct zap on!\\nThey will log back into https://zapier.com/ and back to Zaps > AP Revamp PROD. Then, turn on the CSV or PDF one depending on invoice \\ntype. Rather than deleting the other, I would leave it there in case the vendor sends us the other type. We can clean up the list every so \\noften if we feel confident it won’t change. Turn off your new zap for now by toggling the on/off toggle. We will turn it on after DE finishes their role in 3: Setting up the \\ncode.\\nTurn off your new zap for now by toggling the on/off toggle. We will turn it on after DE finishes their role in 3: Setting up the \\ncode.\\nUntil now, the vendor has been onboarded through the entire ingestion process! The remaining steps are for setting up the \\nprocessing of the invoice and submitting the bills through the automated process to Quickbooks.',\n",
       "  '327\\nImpact: 36,731 transactions (out of 191,214) – about 19% of transactions\\n10',\n",
       "  \"different arguments based on the if statements. Each if statement checks for the config that’s passed in or not. If the config is passed in, this \\nmeans we specified to run the job on all Docparser data for all vendors or for specified vendors (see this page for more info). If the config is \\nnot passed in (default), the except statement of the try/except block is hit, which does the following:\\n \\n(1) Logs '[INVOICE_MATCHING_V2]: Running Docparser data for the past day'\\n(2) Calls get_invoice_data where rerun_docparser_full is False. This calls get_data from \\ndags/analytics/accounts_payable/lib/docparser_api.py, which calls the following (in order):\\nget_and_write_objects: Gets data from Docparser. Specifically, all documents that have been parsed after yesterday. Keep in mind, \\nwhen you update a docparser rule, the files will be reparsed upon the update. Thus, until the following day, all the files for that vendor \\nwill be included in this.\",\n",
       "  'indicating that the payment was returned. With these payments, there will exist data with a RETURNED status, a non-null completed_at \\ntimestamp, and no corresponding data with a COMPLETED status.\\nThe effect of the Deserve ACH holdout takeover on completed payments was not always understood correctly. This was the cause of the \\nsince resolved Variance  in the known variances documentation.\\nInitiated Payments After 2023-05-08\\nWhen a borrower makes a payment, engineering will record it and ping Deserve to generate the payment on their end as well. When \\nengineering pings Deserve to generate this payment has changed over time. The actual account balance changes when Deserve creates \\nthe payment on their end. \\nBeginning on 2023-05-08 engineering tells Deserve to create a payment immediately upon payment initiation. This timing is preferred to \\nwaiting until payments reach a completion status because of instances where a borrower initiates a payment before the due date and the',\n",
       "  '331\\nData Discrepancies v1 (11/21/22)\\nHigh Impact Issues\\nPerpay Core Issues:\\nPerpay Core Issues - Latency:\\nNote: for simplicty, we will consider anything longer than 2 hours “late”1 transaction ID: \\n206089Transactions that exist in \\npublic.card_transaction do not \\nexist in \\npublic.card_transaction_snapsho\\nt36,731 transactions (~19% \\nof transactions)This is fixed! PR has \\nbeen merged to make \\nsure this doesn’t \\nhappen with future \\ntransactions and older \\ntransactions have \\nbeen backfilled to \\nhave snapshots. The \\nfix was deployed on \\n12/29/22, snapshots \\nwere backfilled on \\n12/30/22.\\n2 transaction IDs: \\n5ae53d2c-b816-\\n5c84-92a6-\\n17c367cbe28b\\n120eba31-d983-\\n50bc-81e7-\\n88b3df9383e5\\n3b51342f-72e5-\\n5cd2-9138-\\n7828c9dbc727The total transaction is different in the \\nPerpay Core database (as compared \\nto Deserve Admin & SFTP files).3 transactions This is fixed! These \\nwere deserve issues \\nwhen they used to \\nsend us webhooks \\nbefore updating their \\nown database. I’ve \\nupdated these',\n",
       "  '144                        \"count\": 3,\\n145                        \"ind\": 1,\\n146                        \"first_dt\": \"2019-02-01\"\\n147                    }\\n148                }\\n149            }\\n150        },\\n151        \"ecommerce\": {\\n152            \"category_affinity:\": {\\n153                \"7_days\": [\\n154                    {\\n155                        \"category_path_deep\": \"electronics/computers/ipads-tablets\",\\n156                        \"category_path_1\": \"electronics\",\\n157                        \"category_path_2\": \"electronics/computers\",\\n158                        \"category_path_3\": \"electronics/computers/ipads-tablets\",\\n159                        \"category_path_4\": NULL,\\n160                        \"category_path_5\": NULL,\\n161                        \"category_path_6\": NULL,\\n162                        \"visits\": 3\\n163                    },\\n164                    {\\n165                        \"category_path_deep\": \"fashion-beauty/personal-care\",',\n",
       "  'We could also use this to load different versions of stateful data, for example for creating a “slowly changing dimension” table for \\nauditing changes. For example, if we load a list of cars and their colors every day, and one day one car changes color, we need \\nboth sets of data to be able to discern that a change happened.\\n2. Merge\\nWe can use this to update data that changes.\\nFor example, a taxi ride could have a payment status, which is originally “booked” but could later be changed into “paid”, \\n“rejected” or “cancelled”\\nMore:\\nQuotes from people at companies including Huggingface, Snowflake, Creatext, Harness and more.\\n“Success story” from a fintech company using dlt\\nDo nothing?\\nA crude solution is to continue what we currently do: detect the issue in metric layer DAGs and handle it manually. Tests and a source layer \\nmay not be worth the extra resources since it will pause the pipeline regardless, and why not just deal with it at the later point at which we',\n",
       "  'card_transaction_id      True\\n    amount                           True\\n    status                              True\\n    transaction_type            True\\n    transaction_category    True\\n    credit_indicator              True\\n    mcc_category_code      True\\n    merchant_name             True\\n    pos_vendor                     True\\n    card_account_id            True\\n    borrower_id                     False\\n    user_id                               False\\n    status_ts                          True\\n    rank                                  True\\n    current_ind                     True\\n    dtype: bool',\n",
       "  'and Deserve\\n This has been \\nresolvedIssue # Example Issue Impact Resolution\\n8 transaction ID: \\n5ae53d2c-b816-5c84-\\n92a6-17c367cbe28bTransactions are duplicated in the \\ndeserve_daily_settled_transact\\nions_report242, (<1% of transactions)\\n  Should be raised \\nto Deserve.\\n9 transaction ID: \\n2eb0528e-e022-566e-\\na25b-a36ebb5382e4Transactions that are one \\ntransaction in Perpay Core & \\nDeserve Admin are two transactions \\nin the  \\ndeserve_daily_settled_transact\\nions_report(<1%)\\n  Should be raised \\nto Deserve.Issue # Example Issue Impact Resolution\\n10 transaction IDs: Transactions that appear in the \\nSFTP 4 transactions\\n  I found errored \\nwebhooks for all of Issue # Example Issue Impact Resolution',\n",
       "  'learn how these \\npayments are created.  \\nBased on outcome, it’s \\npossible we need to \\nmake new engineering \\nticket to handle these \\nwebhooks.\\nEng will make a ticket \\nto handle these \\nwebhooks.  Even \\nthough we don’t \\nexpect them, we \\nshould be prepared to \\ncreate them in our DB.',\n",
       "  '591\\nExample:\\nOptions that are specified across an entire Airflow setup:\\ncore.parallelism: maximum number of tasks running across an entire Airflow installation\\ncore.dag_concurrency: max number of tasks that can be running per DAG (across multiple DAG runs)\\ncore.non_pooled_task_slot_count: number of task slots allocated to tasks not running in a pool\\ncore.max_active_runs_per_dag: maximum number of active DAG runs, per DAG\\nscheduler.max_threads: how many threads the scheduler process should use to use to schedule DAGs\\ncelery.worker_concurrency: max number of task instances that a worker will process at a time if using CeleryExecutor\\ncelery.sync_parallelism: number of processes CeleryExecutor should use to sync task state\\nCelery Flower Resources:\\nFlower is a web based tool for monitoring and administrating Celery clusters.\\nCelery flower docs\\nRedis backend PR (perpay-airflow)\\nHow to launch flower\\nCelery Airflow Docs\\nHow to optimize airflow, including celery flower\\nPG Bouncer:',\n",
       "  \"perpay_accounting_datamart_ext.po_invoice_variance_master and save that to temp_int schema as \\ntemp_int.po_invoice_variance_master\\n- Save that temp schema to the external table, perpay_accounting_datamart_ext.po_invoice_variance_master\\n \\n3: Log '[InvoiceMatchingV2] Removing invoices marked as delete from DB - table: \\ndocparser_invoices_master ...\\n4: update the perpay_accounting_datamart_ext.docparser_invoices_master table\\n- Join the info from the sheet (from temp_int.invoice_matching_sheet_temp) onto \\nperpay_accounting_datamart_ext.docparser_invoices_master where delete = 'Y' and save that to temp_int \\nschema as temp_int.docparser_invoices_master\\n- Save that temp schema to the external table, perpay_accounting_datamart_ext.docparser_invoices_master\\n \\n5: Log '[InvoiceMatchingV2] Removing invoices marked as delete from DB - table: po_invoice_overview \\n...'\\n6: update the perpay_accounting_datamart_ext.po_invoice_overview table\",\n",
       "  '2449:  left join {{pub_schema }}.payment b on a.src_dt = cast(b.created as date) and a.borrower_id = b.borrower_id\\n25\\n26dags/analytics/data_model_accounting/account_reconciliation/ach_payments.py\\n2765:left join {{pub_schema }}.payment b on a.borrower_id = b.borrower_id\\n28\\n29dags/analytics/data_model_accounting/account_reconciliation/credit_cards.py\\n3065:left join {{pub_schema }}.payment b on a.borrower_id = b.borrower_id\\n31\\n32dags/bi_reporting/commerce/sales_tax_daily_2023_updated.py\\n33470:  FROM {{pub_schema }}.payment a\\n34564:  FROM {{pub_schema }}.payment a\\n1dags/analytics/card/card_data_model_full/payment/status.py\\n211:DROP TABLE IF EXISTS {{temp_schema }}.payment_status ;\\n313:CREATE TABLE {{temp_schema }}.payment_status AS (\\n4\\n5dags/analytics/card/card_data_model_full/payment/status_hist.py\\n611:DROP TABLE IF EXISTS {{temp_schema }}.payment_status_hist ;\\n713:CREATE TABLE {{temp_schema }}.payment_status_hist AS (\\n8',\n",
       "  'See the Reconciliation page (Engineering) for a high-level overview of how this process works \\nSee the Rec Troubleshooting & Rec Support Process pages (Engineering) for precise details of the steps performed by the \\nEngineering & Ops team members during rec\\nEvery month, Accounting also performs a Bank Reconciliation process to reconcile Quickbooks with the data provided by the banks. \\nFor further details, see the Bank Reconciliation page (Accounting)\\nRelevant Engineering tables\\npublic.achdetail\\npublic.achdetail has a foreign key header_id, which corresponds to the id column in public.achfile\\nThe deposit_id column in public.achdetail points to the id column of the public.deposit table\\nACH rejections (what Accounting calls “Return to Payroll”) & reversals are part of public.achdetail, but these rows don’t end up in \\npublic.deposit\\ndeposit_id’s are NULL if the deposit doesn’t happen or the ACH details are stale \\npublic.auto_pay_payment',\n",
       "  'There is a column history_type in waffle_flag_history that either contains the value ~ or -. We have logic in our current \\nPinwheel data model that checks this column and ignores the historical record if it contains -, does anyone remember what this was \\nmeant to represent?\\nDoes anyone know of anywhere else where there is historical pinwheel eligibility data besides waffle_flag, waffle_flag_history, \\npinwheel_company_legacy, and pinwheel_payroll_provider_legcay?',\n",
       "  '2WITH\\n3    feature_enabled as (\\n4        SELECT\\n5            borrower_id,\\n6            min(created) as first_card_enabled_ts\\n7        FROM {{pub_schema}}.feature_enrollment\\n8        WHERE feature_id = 67\\n9        GROUP BY borrower_id\\n10    )\\n11    SELECT',\n",
       "  'merged_to_email VARCHAR The lead node of the cluster, \\nshould be same value for any \\ngiven cluster_id.\\nmerged_from_user_id VARCHAR (should be INTEGER)user_id corresponding to the \\nmerged_from_email.\\nmerged_to_user_id VARCHAR (should be INTEGER)user_id corresponding to the \\nmerged_to_email.Column Data Type Description',\n",
       "  'Returned Payments\\nReturned payments represent the total amount of payments that were returned and had gone towards the loan balance. The returned \\npayments amount is pulled from Engineering’s public.returned_payments table. We can trace these back to specific loans with help of \\nthe atomic_int.platform_payments table. \\nPartially Returned Payments\\nAt present, there is no partial return modeling implemented within the Engineering department, making it extremely challenging for us to \\nmonitor partial returns. To address this limitation, we have implemented a workaround—a field called potential_partial_return_ind, \\nwhich serves as an indicator of likely discrepancies resulting from the absence of partial return modeling. When this flag is set to 1, it signals \\nthat the variance is likely due to issues related to returned payments. Conversely, if the flag is set to 0, the variance can be attributed to \\nother factors.',\n",
       "  '625\\nTerraform\\nVideo topic ideas:\\nwhy do we have terraform?\\nwhat is a terraform plan? what is terraform drift?\\ngeneral overview of writing infrastructure-as-code in terraform (making resources, attaching them to each other, etc.)',\n",
       "  'error_message varchar The error message from Domo \\nabout the error\\nerror_status varchar Either \\'concurrency_issue\\' , \\n\\'import_error\\' or \\n\\'unknown_error\\'\\nerror_phase varchar A string containing a comma-\\nseparated list of the phases of the \\nexecution that occurred prior to \\nthe error\\neg. \"INIT,QUEUE,IMPORT\" \\nindicates the execution occurred \\nduring the import phase\\nThe full list of possible phases are:\\nINIT\\nQUEUE\\nIMPORT\\nDATASTORE\\nINDEX\\nFINAL\\n(note: not all phases may be \\npresent in a particular execution)\\nVariable Name: Data Type: Description:',\n",
       "  'Daily \\nBalances\\ntransactions Numeric(\\n38,2)Sum of transactions of type REGULAR for \\nthe card account on the observation dateTransaction \\nTracking\\naccount_opening_fee Numeric \\n(38,2)Sum of transactions of type \\nACCOUNT_OPENING_FEE for the card \\naccount on the observation dateTransaction \\nTracking\\naccount_opening_fee_adjustmen\\ntNumeric \\n(38,2)Sum of transactions of type \\nACCOUNT_OPENING_FEE_ADJUSTMENT for \\nthe card account on the observation dateTransaction \\nTracking\\nmonthly_fee Numeric \\n(38,2)Sum of transactions of type MONTHLY_FEE \\nfor the card account on the observation \\ndateTransaction \\nTracking\\nmonthly_fee_adjustment Numeric(\\n38,2)Sum of transactions of type \\nMONTHLY_FEE_ADJUSTMENT for the card \\naccount on the observation dateTransaction \\nTracking\\nlate_payment_fee Numeric(\\n38,2)Sum of transactions of type \\nLATE_PAYMENT_FEE for the card account Transaction \\nTrackingColumn Data \\nTypeDefinition Additional \\nDetail',\n",
       "  'applied to our calculated account balance on the correct date in \\nall cases (or not at all). In addition, I’m seeing inconsistencies \\nbetween the amount of the payment that it is returned and the \\namount the balance increases.The returned_payments field was \\nbeing duplicated by a similar issue \\nto variance no 2. Thus, this was \\nfixed by referencing the new \\nloan_id column in \\nplatform_payments as well. In \\naddition, the ending_balance_calc \\ntable hadn’t taken Status Primary \\nDE \\nMember(\\ns)Summary Status Detail',\n",
       "  \"30    group by 1;\\n31\\n32-- Everyone who opted in and got to enabeld for perpay plus\\n33create table {{temp_schema}}.plus_borrower_fact_6 as\\n34    select a.borrower_id, min(a.created) as enabled_ts\\n35    from {{pub_schema}}.feature_history as a\\n36    left join {{pub_schema}}.feature_enrollment as c on c.id = a.feature_enrollment_id\\n37    left join {{pub_schema}}.feature as b on c.feature_id = b.id\\n38    where b.type in ('perpay_plus', 'perpay_plus_v2')\\n39    and a.status = 'enabled'\\n40    group by 1;\\n41...\\n1create table {{temp_schema}}.plus_borrower_status_history as\\n2    select a.borrower_id,\\n3           min(f.created) as first_opted_in_ts,\\n4           max(case when f.status is not null then 1 else 0 end) as first_opted_in_ind,\\n5           min(g.created) as first_pending_ts,\\n6           max(case when g.status is not null then 1 else 0 end) as first_pending_ind,\\n7           min(c.created) as first_enabled_ts,\",\n",
       "  '96\\nThe Source Layer\\nThis page provides an overview of materialized views by consolidating various documentation and implementation notes linked at the \\nbottom. This also provides a look into how DBT implements their source layer and materialized views.\\nBackground - Why do we need the source layer?\\nMaterialized V iews\\nWhat are they? Why use MVs? How do they work?\\nPros and Cons Overview\\nImplementation\\nBehind the Scenes, what’ s happening?\\nTesting\\nResults\\nTakeaways\\nHow does DBT  implement the source layer?\\nDBT’ s Staging Layer\\nDBT’ s Intermediate Layer\\nDBT’ s Mart\\nWhat does/should our implementation of the source layer and MVs look like?\\naccount_reconciliation…\\nMVs in our architecture\\nQuestions…\\nConclusion\\nLinks I read in researching this^\\nBackground - Why do we need the source layer?\\nThe current data lake house architecture currently has 4 layers built on top of each other (in this order): \\n1. public schema - data sourced by DMS from other departments and sources (raw)',\n",
       "  '351\\nab_test_backend_signup_report.py\\nexpected_company_pay_dates.ipynb (check with DS)\\n& more\\nHighlighted files:\\n1. I think it’s best to start with the two metric layer files. These feed into most of our data models so it’s starting as close to the source as \\npossible.\\na. card_ach_payment_tracking.py: I actually don’t think anything needs to change here but someone should double check me on that. \\nFor this, the auto pay payment information shouldn’t have changed. It will still populate when we receive the payment. It’ll be \\nimportant to note how the card_payment_created and card_payment_modified dates are used downstream, however, since those \\ncome from the public.card_cardpayment which is filled earlier than before.\\ni. one thing I didn’t realize is there is also a public.payment_achtransaction table. I’m assuming this is populated at the same \\ntime as the public.auto_pay_payment: edit: The ach transaction table represents the money we get from jpmorgan (for the',\n",
       "  '240\\nHere’s an example PR for reference.\\nRegardless of CSV or PDF……\\nupdate the constants.py file within the AP Revamp folder to include the name of the vendor as seen in QB. This is used in the \\naccounts_payable_int.qb_bills query to gather all vendor names from QB.\\n6.2: Testing new code\\nTest on stagginggg!!!\\nIf accounting performed steps 1-5, do a quick check to make sure the zaps and quickbooks vendor are set up correctly (mistakes \\nhappen, just double check so the process runs smoothly), and you are ready to test! Refer to the section above for staging tips.\\nMake sure that you add the new vendor name to Sandbox QB and check that the new invoice made it into the po_matches or \\npo_variances table (either into Sandbox QB or into the Manual PO Matching sheet).\\nRefer to this link for instructions on how to access Sandbox QB.\\n7: Update the vendors sheet\\nUpdate the Vendor Invoice Types and Terms Google Sheet in the AP Folder in the DE Drive.\\n8: Loop back to the accounting page',\n",
       "  \"449\\nFile status_history (plus borrower dm) ✅\\nPlus Funnel Fact\\nAlso used for pp+ purposes! Limits the population based on pp+ enrollments, but cards in the wild shouldn’t affect this.\\n11           min(e.created) as first_canceled_ts,\\n12           max(case when e.status is not null then 1 else 0 end) as first_canceled_ind\\n13    from {{report_schema}}.plus_borrower_fact as a\\n14    left join (\\n15            select b.*\\n16            from {{pub_schema}}.feature_history as b\\n17            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\\n18            left join {{pub_schema}}.feature as c on c.id = d.feature_id\\n19            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n20            and b.status = 'enabled'\\n21        ) as c on c.borrower_id = a.borrower_id\\n22    left join (\\n23            select b.*\\n24            from {{pub_schema}}.feature_history as b\\n25            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\",\n",
       "  'Note that the year/month value is 1 month greater than the data within. Example for 2022/11, the cost data would be for October of that \\nyear. \\nThe DE group has 3 different environments we support and are shown as LinkedAccountName in the table - productname can be used to \\nfurther break down the cost. The totalcost field can be summed to find the overall financial impact of the product. The usagetype, \\nitemdescription, usagequantity fields can be used to understand how the costs are being calculated.\\nJD has saved off and coalesced all of 12 tables for 2022 and is currently saved here (users_herr.aws_bill_total) for further concatenation.\\nApp Specific Queries\\nBelow is an app by app breakdown of all of the queries used to derive the monthly AWS costs.\\nRedshift\\n1-- Redshift\\n2-- Note: Concurrency scaling is when AWS has to pull in additional resources when\\n3--   the cluster is under heavy load (near 100% CPU)\\n4select * from (\\n5select',\n",
       "  'While ingesting datasets, DataHub creates these concrete \"data units\" - called entities - that are identified by Uniform Resource Names \\n(URNs). So far we\\'ve concerned ourselves with the following entity types: containers (the perpay database; all schemas), datasets (a \\nrelation/table), and datajobs (Airflow tasks, maybe DMS stuff too). Through Redshift ingestion, each dataset gets stored in a \\ndatabase.schema.table_name format, and you can click through each level here like a file system. The database has a URL and landing \\npage for documenting, and schemas and datasets do as well. Through the Airflow integration (\"Airflow push\", since the Redshift is a pull), \\nwe get creep in the UI. When Airflow pushes a table to DataHub, it adds it to the UI and backend as a new entity. While this may be \\nimportant for tracing full lineage and searching through all assets, it\\'s a nuisance in the front end. We have all these \"ghost tables\" in the',\n",
       "  'Daily \\nBalances\\ntransactions Numeric(\\n38,2)Sum of transactions of type REGULAR for \\nthe card account on the observation dateTransaction \\nTracking\\naccount_opening_fee Numeric \\n(38,2)Sum of transactions of type \\nACCOUNT_OPENING_FEE for the card \\naccount on the observation dateTransaction \\nTracking\\naccount_opening_fee_adjustmen\\ntNumeric \\n(38,2)Sum of transactions of type \\nACCOUNT_OPENING_FEE_ADJUSTMENT for \\nthe card account on the observation dateTransaction \\nTracking\\nmonthly_fee Numeric \\n(38,2)Sum of transactions of type MONTHLY_FEE \\nfor the card account on the observation \\ndateTransaction \\nTracking\\nmonthly_fee_adjustment Numeric(\\n38,2)Sum of transactions of type \\nMONTHLY_FEE_ADJUSTMENT for the card \\naccount on the observation dateTransaction \\nTracking\\nlate_payment_fee Numeric(\\n38,2)Sum of transactions of type \\nLATE_PAYMENT_FEE for the card account Transaction \\nTrackingColumn Data \\nTypeDefinition Additional \\nDetail',\n",
       "  \"N\\nG\\nLI\\nS\\nH                      2\\n0\\n2\\n4-\\n0\\n1-\\n0\\n8\\nT\\n1\\n7:\\n2\\n2:\\n0\\n7-\\n0\\n5:\\n0\\n0                              ['\\nR\\nx\\nM\\nar\\nk\\net\\nin\\ng \\nG\\nro\\nu\\np'\\n]          []                                          fa\\nls\\ne                                                                                                                                                                                                             \\n1\\n9Fi\\nx                              1\\n4P\\nerT\\nEht\\ntp[]                                                                                                                                                      S\\nh                                            H\\nT0.\\n0                                    tr\\nufa\\nlsfa\\nlsfa\\nlsfa\\nlsE\\nN2\\n0                            ['\\nR []                                          fa\",\n",
       "  '465\\naccount_opening_fee\\naccount_opening_fee_adjustment\\nmonthly_feeA transaction in a SETTLED state is as defined by Deserve.\\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\ntransactions field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nREGULAR.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all ACCOUNT_OPENING_FEE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:',\n",
       "  'Data Understanding\\nThe source of truth for account merges lives in the public.event table where there are two types of events we are interested in: \\nuser.update.merge and user.update.email_changed. For the former, we can many merged from emails being merged into a single',\n",
       "  '234\\nRecreating the qb_history table:\\n27    tax               varchar(4000)\\n28);\\n1drop table if exists temp_marketplace_int.qb_history;\\n2create table temp_marketplace_int.qb_history\\n3(\\n4    join_key          varchar(32792),\\n5    vendor_name       varchar(100),\\n6    invoice_number    varchar(100),\\n7    invoice_po_number varchar(16383),\\n8    invoice_sku       varchar(16383) distkey,\\n9    invoice_date      date encode az64,\\n10    invoice_net_price numeric(10, 2) encode az64,\\n11    invoice_qty       double precision,\\n12    invoice_total     double precision,\\n13    invoice_freight   double precision,\\n14    invoice_shipping  double precision,\\n15    invoice_dropship  double precision,\\n16    invoice_due_date  date encode az64,\\n17    po_matches_created_ts timestamp with time zone encode az64,\\n18    created_ts        timestamp with time zone encode az64\\n19);',\n",
       "  '579',\n",
       "  \"the bottom here, more on this later.\\nb. Sqlfluff: Executable Path : Check this to see if it's the same as the result of your which sqlfluff  command.\\n6. Now, you're ready to test things out. Open up one of Andrew's new .sql files in the metric layer and open the Command Palette and \\nsearch for Format Document. See what that does.\\n7. If you are a vim fan, then you can use the command line directly. Give this a try: sqlfluff format <file name>.sql --dialect \\nredshift --ignore templating .\\nConfig File\\nPlace the following in your .sqlfluff file as mentioned above:\\n1[sqlfluff]\\n2large_file_skip_byte_limit = 400000\\n3ignore = templating\\n4exclude_rules = structure.column_order\\n5\\n6[sqlfluff:indentation]\\n7# See https://docs.sqlfluff.com/en/stable/layout.html#configuring-indent-locations\\n8indent_unit = space\\n9tab_space_size = 4\\n10indented_joins = False\\n11indented_ctes = False\\n12indented_using_on = True\\n13indented_on_contents = True\\n14indented_then = True\\n15indented_then_contents = True\",\n",
       "  \"Solution\\nIf it hasn’t self resolved (post-investigation), we let engineering handle this one (because the root of the prob comes from the public.job \\ntable)!\\nReach out to Talha and tell him there are multiple primary jobs and he’ll run a query on their end to clean it up \\n \\n[PROD] - SPECTRUM - WARNING: Some borrowers have multiple primary job statuses, please check the Platform Job metric layer \\nlogs\\n1-- primary jobs\\n2select\\n3  b.borrower_id ,\\n4  b.status,\\n5  count(b.status) as cnt\\n6from \\n7  public.borrower a\\n8  left join atomic_int .platform_job b on a.id = b.borrower_id\\n9where \\n10  borrower_id is not null and \\n11  b.status = 'primary'\\n12group by 1,2\\n13having cnt > 1;\\n1select * from public.job where borrower_id = <borrower_id >;\",\n",
       "  \"455\\nSo, new_test_monthly_vantage_scores is fine. ✅\\nTest Monthly Vantage Scores\\nThese have to do with pp+ v1 and v2. I don’t even know if this file is used anymore since the new_test_monthly_vantage_scores exists.\\nSo, test_monthly_vantage_scores is fine. ✅\\nOthers\\nFS Daily Perpay Plus Revenue\\nThis only pertains to pp+. \\n22            from public.feature_history a\\n23            left join public.feature_enrollment b on b.id = a.feature_enrollment_id\\n24            left join public.feature c on b.feature_id = c.id\\n25            where c.type in ('perpay_plus', 'perpay_plus_v2') and a.status = 'enabled'\\n26            group by a.borrower_id),\\n27...\\n1...\\n2                with plus_status_history as (\\n3                    select\\n4                        b.borrower_id,\\n5                        b.start_time as status_time,\\n6                        cast(b.start_time as date) as status_date,\\n7                        cast(first_value(b.start_time) over (\",\n",
       "  '622\\nIAM\\nVideo topic ideas:\\nwhat is a role and what is a policy?\\nwhat are some of our best practices around permissions in AWS?',\n",
       "  'Field type casting (from FLOAT to INT, STRING to INT, etc), to get columns into the proper type for downstream joins or \\nreporting\\nRenaming columns for readability\\nFiltering out deleted or extraneous records',\n",
       "  'auto_pay_payment process:\\nUser creates a schedule through the UI.\\nWhen the scheduled time is up, engineering creates an entry in auto_pay_payment.\\nThen webhooks check with the bank. If we receive a success status from the bank we create a deposit line in the deposit table.\\nIf such a deposit is reversed, we simply update the deposit status to returned.\\nachdetail process:\\nACH files are received by engineering and parsed.',\n",
       "  'shape and flow of the merge network made sense, the lead user was not the same as the last merged to account.\\nThe following are examples of cases (1), (2), and (4):\\nA cluster with cyclic merges.',\n",
       "  '311\\nreformatted\\nRemove processing payment fields → removed\\nRemove pending purchase fields  → removed\\nBreak fees into types similar to above → broke fees up, updated the dependencies again and update documentation\\n \\nCard user data model ✅  PR is up\\nPR 2023-01-31: Update to the Card User DM- logic & format changes\\nConfirm all documentation – looks off  → looking into this, it looks like documentation was unfinished and preceded changes to \\nthe code. Outlined information to date here Card User Data Model (updated) based on the tables so that it doesn’t get \\nconfused with the current page in the DA space.\\nCard user fact \\nGood → reformatted\\nCard user detail\\nGood → reformatted\\nalso made a logic update\\nstaff ind value used to be collected in a cte, from the metric layer, atomic_int.card_key_relations(which gets it \\nfrom public.user). Instead, I changed it to come from card_int.card_user_fact',\n",
       "  'Definition:\\nThe sum of all REFUND transactions associated with a given card account on a given src_dt. The transactions are associated with the \\ndate in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The refund \\nfield uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is REFUND.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:',\n",
       "  'Business Context:\\nIn Engineering terms, a customer has a card account balance if they have funds available to withdraw from their card account. \\nThere are several ways that customer funds have been applied to the card account balance on Engineering’s side for UX purposes, but \\nhave not yet been applied to receivables balances from an accounting standpoint. Therefore this field makes accommodations for the \\nitems listed below to more accurately represent a customer’s card account balance in the report.\\nRollforward Table Logic:',\n",
       "  '147\\n(Warning) Parent/Child Multi-sourcing relationship\\nIn the airflow_prod_alerts channel:\\nWarning explanation\\nAs of November 23, 2022, we’ve started to see a weird situation where there is a child → parent → child relationship loop with some house \\nSKUs. This relationship used to be a one-to-one relationship (i.e., one product ID per house SKU). \\nExample:\\nThere have been other issues since, so more broadly the issue here is that there are rows in the Product Catalog Data Model that have \\nmultiple entries for the same house_sku. House_sku should be unique in this data model.\\n \\nTo tie House SKU to a Magento Product, the relation is created in the query that creates the perpay_general_datamart_int.product_ctlg_fact \\ntable. This is used in downstream processes.\\nIn perpay_general_datamart_int.returns_issues_fact, we join on house_sku, with the assumption that house_sku is unique. This creates',\n",
       "  \"50\\ntype object “update” or “paid off”\\ninqueryid object Unique id used internally by DataX \\nexternalid object Loan ID\\nnamefirst object First name of user\\nnamemiddle object ~ Left blank ~\\nnamelast object Last name of user\\nstreet1 object User's street address\\nstreet2 object ~ Left blank ~\\ncity object City user resides\\nstate object State user resides\\nzip object Zip code of user's residence\\nphonehome object ~ Left blank ~\\nphonecell object User's cell phone number\\nphonework object ~ Left blank ~\\nphoneext object ~ Left blank ~\\nemail object User's email\\ndob datetime64[ns] User's date of birth\\nssn object User's decrypted (if in correct env) social \\nsecurity number\\ndriverlicensenumber object ~ Left blank ~\\ndriverlicensestate object ~ Left blank ~\\nworkname object ~ Left blank ~\\nmonthlyincome object User's monthly income\\nworkstreet1 object ~ Left blank ~\\nworkstreet2 object ~ Left blank ~\\nworkcity object ~ Left blank ~\\nworkstate object ~ Left blank ~\\nworkzip object ~ Left blank ~\",\n",
       "  '6        - create or update card account from api data\\n7        - if ACCOUNT_CREATED create card from api data\\n8    APPLICATION_TYPE = \"application\"\\n9        - create or update application status from webhook data\\n10        - update application approval type\\n11    STATEMENT_TYPE = \"statement\"\\n12        - create or update statement from webhook data\\n13        - update internal representation of auto pay schedules\\n14    PAYMENT_TYPE = \"payment\"\\n15        - get or create card payment status\\n16    USER_TYPE = \"user\"\\n17        nothing',\n",
       "  'Git 101: Staging, Committing, Pushing, and Creating a PR200\\n                Staging Environment Procedures202\\n           Accounts Payable/Invoice Matching Procedures203\\n                Adding New Vendors to Docparser204\\n                Invoice Matching V1 DAG Runs206\\n                Invoice Matching V2 DAG Run212\\n                Rerunning Invoice Matching V2 for the month215\\n           AP Revamp216\\n                Accounting Notes for AP Revamp217\\n                A Technical Walkthrough of the Process223\\n                Dependencies & Diagram231',\n",
       "  'Business Context:\\nEngineering is able to initiate customer transaction activity (the flow of funds in and out of one’s card account), but the actual \\nincrementing or decrementing of the card account balance occurs on Deserve’s side. Once Engineering initiates a payment, withdrawal, \\nreturn- whatever it may be- Deserve changes the card account balance and sends Engineering webhooks with that updated balance. \\nEngineering then updates their tables to reflect the change. \\nRollforward Table Logic:\\nBecause this field reflects balance at the start of the day, when we pull current_balance from the source tables below, we do so on the \\nday prior, which reflects the card account balance at end of the day prior/the beginning of the current day.\\nSource Tables:\\nThis comes from atomic_int.card_balance, which pulls current_balance from webhooks first through the \\npublic.card_account_snapshot table. If not there, it uses current_balance from the deserve_accounts_eod_report SFTP. \\nDefinition:',\n",
       "  \"The permissions for external interns need to be revoked for the whole schema. \\nThe permissions for all interns, internal and external, need to be revoked for the following tables:\\npayment_riskdata\\nachdetail\\nuser\\nThe permissions to all external teams need to be revoked completely.\\nA Note On Superusers\\nWe will only have a handful of superusers for now, likely JD and Dante. However, it would be ideal for both of them to have separate \\nsuperuser and user accounts to ensure they have a way to query the database to their hearts' content without fearing making irreversible \\nand damaging changes. So we will create two superuser groups which will not actually have any superusers in them, but this will help with \\nkeeping track of who the actual superusers are, and we will additionally provide these users with separate superuser accounts.\\nQueries For Generating Queries To Revoke Old Privileges\\nFor GROUPs:\\n1-- Revoke schema privileges:\\n2WITH \\n3schema_group AS (\",\n",
       "  \"257\\nValidation Google Sheet\\n44       d.min_enter_date as first_report_dt,\\n45       e.opt_in_dt as feature_opt_in_dt\\n46from perpay_risk_datamart.equifax_raw_data as a\\n47left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n48left join (select consumer_account_number, min(enter_date) as min_enter_date from perpay_risk_datamart.experian_\\n49left join (\\n50    select distinct borrower_id, b.type from public.feature_enrollment as a left join public.feature as b on b.i\\n51    ) as c on c.borrower_id = a.consumer_account_number\\n52left join (\\n53    select a.borrower_id, min(cast(c.created as date)) as opt_in_dt\\n54    from public.feature_enrollment as a\\n55    left join public.feature as b on a.feature_id = b.id\\n56    left join public.feature_history as c on c.feature_enrollment_id = a.id and c.status = 'enabled'\\n57    where b.type in ('credit_reporting', 'perpay_plus')\\n58    and a.status = 'enabled'\\n59    group by a.borrower_id\",\n",
       "  'Information \\nIndicatorconsumer_infor\\nmation_indicatorContains a value that indicates a special \\ncondition of the primary consumer, such \\nas bankruptcy.Blank  \\nCountry Code country_code  US  \\nFirst Line of \\nAddressfirst_line_addres\\nsBilling/mailing address for the primary \\nconsumer. Cleaned for special \\ncharacters, malformed strings.user_attribute.street_addr, or if \\nnull, last created \\nanalytics_magento_sales_flat_or\\nder_address.streetuser_attribute.str\\neet_addr\\nSecond Line of \\nAddresssecond_line_add\\nressContains 2nd line of address such as \\napartment number. This can also be Blank',\n",
       "  '467\\nlate_payment_fee_adjustment\\nshipping_fee\\ndebit_card_processing_fee_adjustmentDefinition:\\nThe sum of all LATE_PAYMENT_FEE_ADJUSTMENT transactions associated with a given card account on a given src_dt. The \\ntransactions are associated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\nlate_payment_fee_adjustment field uses its join condition to only bring in the rows in this card_transactions CTE where \\ntransaction_type is LATE_PAYMENT_FEE_ADJUSTMENT.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the',\n",
       "  'price_special_from_date boolean Whether the product has a special price; \\ncurrently always False.\\nprice_special_to_date boolean Whether the product has a special price; \\ncurrently always False.\\nship_price double precision The shipping price of the product.Column Name Type Description',\n",
       "  'this time (account not opened)\\nD = No payment history available this \\nmonth.\\nE = Zero balance and current account \\n(only applied to credit cards and lines of \\ncredit)\\nH = Foreclosure completed\\nJ = Voluntary surrender\\nK = ReposessionBased on current definitions, \\nnever gets to 5 or 6. Based on \\nthe account status field at point \\nin time. If the account is not \\nactive on a particular month, the \\npayment rating for the month is \\nbased on the last known \\nborrower status. See further \\nclarification below.',\n",
       "  'As of writing this document, the exact folder organization is still being finalized, and this section will be updated with additional details once \\nthe structure is confirmed. The Glossary is intended to be the central syntax knowledge center for the DRAAFT team. Therefore, it is \\nrecommended that information shared in Slack or verbally be committed to the Glossary to prevent it from being lost and to avoid repeating \\nthe same questions in the future.\\nNote that not every table column needs to be a glossary term, nor does every glossary term need to correspond to a table column. However, \\nthe Glossary can be used to apply consistent definitions across multiple data tables without the need for independent upkeep (e.g., borrower \\nID). Below are the recommended elements to include in Glossary entries while the normal additional owner and tags values can be selected \\nas well.\\n1. Description (required)',\n",
       "  \"515\\nBefore we get too deep, I’d like to consider renaming all of the fields to better reflect what they capture. Strong names will avoid \\nambiguity and increase readability and maintainability for our team.\\nI’ve put 500 variances here I’ve identified, and am now doing the same for 1,000 variances here. I think Google Sheets provide a \\nuser-friendly format for referencing example cases, which could aid in better understanding the situation as a group. I'm open to 4 COMPLETED abs collections_newly_pending_deposits: newly pending \\ncollections deposits shouldn't be includedputting in pr1 - trying to get the table to a \\nbetter base state\\n5 COMPLETED abs withdrawals_newly_pending: This is p much the \\nopposite of 1. It’s when eng removes withdrawals from the \\nbalance. withdrawals newly pending need to be \\nsubtractedputting in pr1 - trying to get the table to a \\nbetter base state\\n6 COMPLETED abs commerce_borrower_credits_for_awaiting_payment_\\nnew: don't subtract\",\n",
       "  'went to 0. Otherwise, null \\n(open account). \\nDate of Last \\nPayment\\n date_last_pay\\nmentMost recent date a payment was \\nreceived, full or partial.If the account is charged off \\nand the borrower most \\nrecently paid True Accord \\n(after paying us), then the last \\npayment date to TA. \\nOtherwise, the maximum not-\\nnull last_pmt_dt for the \\nborrower.borrower_hist_\\ndaily_status.las\\nt_pmt_dt\\nInterest Type \\nIndicator\\n interest_type_i\\nndicatorF = Fixed, V = Variable/Adjustable \\ninterest.Blank  \\nReserved\\n  reserved2 Blank fill   \\nSurname\\n  surname Last name, cleaned for generation \\ncodes and suffixes. user_attribute.l\\nast_name\\nFirst Name\\n  first_name First name, cleaned for titles and \\ngeneration codes. user_attribute.fi\\nrst_name\\nMiddle Name\\n  middle_name Middle name Blank',\n",
       "  'What are some mythologies to writing clean and efficient SQL code?\\nWhat are some considerations for data governance and permissions?\\nHow do you handle security? We carry a lot of very sensitive information.\\nHow do you prioritize project and initiatives\\nEngr Stack questions\\nAny cloud experience?\\nHow do you balance performance and cost?\\nHave you managed a cloud stack before? Have you used Terraform?\\nAny Docker experience?\\nHave you created docker files/compose scripts?\\nAny CI/CD tooling experience?\\nHave you used Airflow before?\\nWhat are some considerations for DAGs?\\nHow do you keep issues from occurring where there are inter-dependencies between DAGs\\nAny experience with a BI Tool?\\nHow do you ensure that only correct data make it into the BI Tool?\\nAssuming that the amount of data in the BI tool contributes to cost, how do you justify limiting the amount of data going into the \\ntool?',\n",
       "  '473\\nending_balance\\nending_balance_calcIf the payment hits a RETURNED status after the holdout, that is when the system generated card account balance is increased by the \\npayment amount, as stated above. Likewise, if the payment never entered a holdout and gets RETURNED, the date of the RETURNED \\nstatus is the day the system generated card account balance is increased.\\nRollforward Table Logic:\\nSums all the payments that went towards the card and hit a RETURNED status within the day. \\nThis payment total is added to starting_balance in the equation for ending_balance_calc. \\nSource Tables:\\nThis directly comes from atomic_int.platform_payments, which captures card payments from the atomic_int.card_sftp_payment \\ntable, which pulls from the actual source of deserve_payments_eod_report SFTP.\\n5select * from perpay_accounting_datamart_ext.card_daily_rollforward where borrower_id = 7382228 order by src_\\n6\\n7-- CASE 2:\\n8-- atomic int\\n9select * from atomic_int.card_payment_status',\n",
       "  'later filtered down in their own respective join conditions by transaction_category for the construction of the final table. The \\nlate_payment_fee field uses its join condition to only bring in the rows in this card_transactions CTE where transaction_type is \\nLATE_PAYMENT_FEE.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.',\n",
       "  '283\\nVisual Representation\\nThe flow of funds at Perpay can be loosely defined by the diagram below. While the actual process contains much more complexity, this \\ndiagram is a helpful guide to understanding the types of transactions that can impact a user’s account.\\nDOMO Cards\\nWhile the database tables may not be accessible to everyone (we limit the permissions to the DE team for risk mitigation), each product has \\nthree associated DOMO cards linked below.\\nCard Reconciliation\\nRollforward\\nVariance Tracking by Day\\nCumulative Variances over Time\\nCommerce Reconciliation\\nRollforward\\nVariance Tracking by Day\\nCumulative Variances over Time\\nCore Reconciliation\\nRollforward\\nEnding Balance Variance Tracking by Day\\nCumulative Ending Balance Variances over Time\\nDollar Flow Variance Tracking by Day*An account is considered active if the associated customer had an account balance, entered repayment on a loan, or made any \\ndeposits on any day after 12/31/2020',\n",
       "  '490\\nemail\\nsrc_dt\\n… (there are more in between to be filled out) … \\nending_balance \\nDefinition:\\nThe email associated with the Perpay user.\\nBusiness Context:\\nAll users who have signed up on the Perpay platform have also provided an email.\\nRollforward Table Logic:\\nThis field is not used elsewhere in the query. That said, it is useful to have in this table because when working with the production table, \\nan email associated with an account will often need to be provided to external teams (for example Product, Accounting, etc.) for \\ninquiries about specific accounts.\\nSource Tables:\\npublic.user\\nDefinition: \\nThe observation date associated with the row. Not the same as observation_day.\\nBusiness Context:\\nThe date for which all of the other data in the row was observed.\\nRollforward Table Logic:\\nThe rollforward table is constructed as a daily table. Every field is calculated within CTEs throughout the construction of the table and',\n",
       "  'monthly_fee_adjustment field uses its join condition to only bring in the rows in this card_transactions CTE where \\ntransaction_type is MONTHLY_FEE_ADJUSTMENT.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all LATE_PAYMENT_FEE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The',\n",
       "  '9      \"enum\": [\"United States of America\", \"Canada\"]\\n10    }\\n11  },\\n12  \"if\": {\\n13    \"properties\": { \"country\": { \"const\": \"United States of America\" } }\\n14  },\\n15  \"then\": {\\n16    \"properties\": { \"postal_code\": { \"pattern\": \"[0-9]{5}(-[0-9]{4})?\" } }\\n17  },\\n18  \"else\": {\\n19    \"properties\": { \"postal_code\": { \"pattern\": \"[A-Z][0-9][A-Z] [0-9][A-Z][0-9]\" } }\\n20  }\\n21}\\n22\\n{ \"street_address\": \"1600 Pennsylvania Avenue NW\", \"country\": \"United States of America\", \"postal_code\": \"20500\" }\\n{\"street_address\": \"24 Sussex Drive\", \"country\": \"Canada\", \"postal_code\": \"10000\"}',\n",
       "  'Sesame Credit \\nSesame_Emai\\nl_GIF \\n1_1.3.22_mult\\nipass Credit \\nSesame_Emai\\nl_GIF \\n1_1.3.22_mult\\nipass 1194052 1561307 1561307 \\nimpact_Propel\\n_1865594 Impact Propel Propel_Holida\\ny1-2023 Propel_Holida\\ny1-2023 1379354 1865594 1865594 \\nimpact_Credit \\nSesame_1852\\n211 Impact Credit \\nSesame Credit \\nSesame_Emai\\nl_GIF \\n1_11.8.23 Credit \\nSesame_Emai\\nl_GIF \\n1_11.8.23 1194052 1852211 1852211 \\nimpact_ATM.c\\nom_1435210 Impact Earn Mone\\ny with ATM.co\\nm - Earn Cas\\nh, Save, and I\\nnvest With Th\\ne ATM App  ATM_Homepa\\nge LP_8.22 ATM_Homepa\\nge LP_8.22 2189953 1435210 1435210 utm_campaig\\nnplatform campaign_na\\nmeadset_name ad_name campaign_id adset_id ad_id\\nev\\nen\\nt_\\nda\\ntecli\\nck\\n_t\\no_\\nact\\nionact\\nion\\n_idoid\\n_c\\nolsta\\ntusori\\ngin\\nal_\\nre\\nve\\nnu\\nere\\nve\\nnu\\neori\\ngin\\nal_\\nact\\nion\\n_c\\nostact\\nion\\n_c\\nostbo\\nnu\\ns_\\nco\\nstcu\\nsto\\nm\\n_t\\not\\nalrat\\necli\\nen\\nt_c\\nostre\\nba\\ntedis\\nco\\nun\\ntpa\\nrtn\\nerpa\\nrtn\\ner\\n_idev\\nen\\nt_t\\nyp\\neev\\nen\\nt_t\\nyp\\ne_i\\ndpr\\nop\\nert\\ny_i\\ndadad\\n_idad\\n_ty\\nperef\\nerr\\nal_\\nid',\n",
       "  '367\\nSolution\\nA large cost associated with the selected queries was the practice of writing the interstitial tables to disk. It is much faster to build these \\ntables in memory which can be accomplished by using common table expressions (CTEs). Including a subquery within a JOIN statement \\nwould functionally accomplish the same thing as forming a CTE with a WITH clause, however CTEs were used as they are thought to be \\nmore readable, are able to be used within several JOIN statements in the same query, and for consistency within the codebase. All \\ninterstitial tables were either changed to CTEs where possible or removed entirely if unnecessary. One concern with turning all of the \\ninterstitial tables into CTEs is that the data would exceed the available working memory and have to be written to disk anyway. This would \\neffectively get rid of any time saved from using CTEs. To check whether a query includes any disk-based steps:\\n1. Run the query\\n2. Retrieve the Query ID from sql_qlog:',\n",
       "  '+ international_transaction_fee\\n+ interest_charge\\n– interest_charge_adjustment\\n– cash_back\\n+ cash_back_adjustment\\n– refund\\n+ credit_balance_refund\\n– dispute_won\\n+ dispute_lost_withdrawn\\n– dispute_provisional_credit\\n– dispute_write_off\\n+ returned_payments\\n– completed_payments_pre_may_8_2023\\n– initiated_payments_post_may_8_2023\\n= ending_balance = ending_balance_calc\\nNuances\\nStarting Balance\\nWe first observe a card account for account reconciliation purposes on the first day that the card account has a balance reported by \\nDeserve. For this process data is pulled from both the web-hook and SFTP files. The first date from either of these data sources that \\nDeserve reports a balance is the day that observation of the card account begins in card account reconciliation. \\nThe starting balance on the first day of observation is equal to 0. For all subsequent days, the starting balance is set equal to the ending \\nbalance of the previous day.',\n",
       "  '475\\nWe do the calculation ending_balance - ending_balance_calc\\nSource Tables:\\nN/A directly since it’s a calculation. \\nThe ending_balance and ending_balance_calc columns come from the logic described above in their specific sections.',\n",
       "  '87\\ncentralization ensures consistency across the different environments, simplifies the deployment process, and reduces the chances of \\nmisconfiguration. \\n3. It allows for change management and rollbacks: CloudFormation provides the ability to manage changes to our infrastructure using \\nchange sets. This feature allows us to preview the impact of modifications before applying them, reducing the risk of unintended \\nconsequences. If an issue arises during the integration process, CloudFormation supports rollbacks, so we can revert to the previous \\nstate.\\nPrerequisites\\nTo integrate Datadog with AWS, we need the following prerequisites:\\nAn active Datadog account\\nAn AWS account with the following IAM permissions (IAM permissions define what actions a user, group, or role is allowed to perform on \\nAWS resources. These permissions are granular and can be assigned at a very fine-grained level, allowing administrators to control \\naccess to specific resources and operations):',\n",
       "  '263\\nDeserve SFTP Ingestion\\nIn an ideal scenario, Deserve will send us a new set of files each day, comprising\\ndeserve_cleared_transactions_eod_report\\ndeserve_daily_settled_transactions_report\\ndeserve_net_settlement_eod_report\\ndeserve_payments_eod_report\\ndeserve_rewards_daily_summary_report\\ndeserve_applications_eod_report\\ndeserve_disputes_eod_report\\ndeserve_accounts_eod_report\\ndeserve_cards_daily_report\\nTypical Cadence\\nCurrently, our Deserve SFTP DAG structure executes once daily at 9:30 AM EST to ingest these new files. This takes about 7 minutes.\\nThere is also an option to manually trigger the DAG with a configuration to initiate a full rerun ({\"full_rerun\": true}). However, this \\nprocess consumes nearly 4 and a half hours. During this time, tables are dropped, and files are gradually appended by sftp_dt. This means \\nthat the full dataset becomes temporarily inaccessible for queries.\\nFluctuations in Deserve Report Data',\n",
       "  \"41\\nS3\\nDMS\\nGlue\\n1-- S3\\n2select * from (\\n3select\\n4concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\\n5cast(case\\n6 when usagetype = 'Requests-Tier1'  then 'Requests Tier 1'\\n7 when usagetype = 'Requests-Tier2'  then 'Requests Tier 2'\\n8 when usagetype = 'TimedStorage-ByteHrs'  then 'Storage'\\n9 else ''\\n10 end as varchar(100))as usage_type ,\\n11 totalcost:: float\\n12from users_herr .aws_bill_total\\n13where productcode = 'AmazonS3'  and\\n14   linkedaccountname is not null and \\n15   linkedaccountname = 'Production Account'\\n16   --linkedaccountname = 'Staging Account'\\n17   --linkedaccountname = 'Segment Account'\\n18   and totalcost > 0\\n19   and invoiceid is not null\\n20)\\n21pivot (sum(totalcost ) for usage_type in ('Requests Tier 1' , 'Requests Tier 2' , 'Storage' )) \\n22order by vintage asc;\\n1-- DMS\\n2select * from (\\n3select\\n4concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\",\n",
       "  'includes items where the sku and po_number didn’t match in the temp_marketplace_int.po_source_combo table, which means the \\nrecords could not be joined because they exist in the invoice data, but not in the e-commerce data. This also includes items where \\nthere’s a tax.\\nThe query that is executed to create the matches table is in automated_po_matches.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/po_matching/automated_po_matches.py), and the query that is executed to create the \\nvariance table is in automated_po_variances.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/po_matching/automated_po_variances.py).\\nNew data is added to the accounts_payable_int.po_matches table using an INSERT INTO. This way, we avoid recreation of the \\ntable each time. It also allows us to handle the logic to re-ingest the data from the Google Sheet. If this pipeline is ran for the first \\ntime in prod, it requires the table to exist.',\n",
       "  '342\\nCard Statement Calculations\\nDate logic in statements:\\nIn UTC (database):\\nFor the first statement:\\nstart_ts = timestamp card entered ‘pending’ state\\nend_ts =\\nif day(start_dt) = 30 or 31 → 29th of the following month\\nif day(start_dt) = 1 → 29th of the current month\\nelse → day(start_dt) of the following month\\ndue_dt =\\nend_dt + 23 days**\\nThe 30th, 31st, and 1st do not count as days\\nnext_statement_ts = day(end_ts) of the following month\\n \\nOnce the day values of these are set, they do not change month to month. So…\\nFor the 2nd statement (and so on):\\nstart_ts = next_statement_ts from previous statement\\nend_ts = day(start_dt) of the following month\\ndue_dt = day(due_dt from previous statement) of the following month\\nnext_statement = day(end_dt) of the following month\\n \\nIn EST (i.e., actual statements):\\n \\nHow are the fields calculated?\\nprevious_balance\\nThe balance on the statement prior to the current statement. If it’s the first statement, this value is 0.00.\\ntransaction_total',\n",
       "  'transactions, the average latency is 3 hours.\\n \\n22limit 100;\\n1with transaction_1 as (\\n2  select *,\\n3         lag(status, 1) over (partition by card_transaction_id order by created) as status_previous\\n4  from public.card_transaction_snapshot\\n5),\\n6card_transaction_cte as (\\n7  select rank() over (order by created asc) as id,\\n8         *,\\n9         rank() over (partition by card_transaction_id order by created asc) as rank,\\n10         case when created = first_value(created) over (partition by card_transaction_id order by created desc r\\n11               else 0 end as current_ind\\n12  from transaction_1\\n13  where ((status <> status_previous) or (status_previous is null))\\n14)\\n15select a.card_account_id as card_account_id,\\n16       a.deserve_id as deserve_transaction_id,\\n17       a.card_transaction_id as perpay_transaction_id,\\n18       a.created as perpay_created_ts,\\n19       a.amount as perpay_amount,\\n20       a.tip_amount as perpay_tip_amount,\\n21       a.status as perpay_status,',\n",
       "  'segment_perpay_web_production a l l All front-end events in web (like profile \\nclicks).Schema Table Description\\nperpay_marketing_datamart short_io_insights Short IO click data. Note caveats above.\\nperpay_marketing_datamart google_insights Google spend and click data.\\nperpay_marketing_datamart google_conversions_insights Google conversions data, based on the \\nPerpay action.\\nperpay_marketing_datamart google_insights_flattened Google summary data, flattened to the ad \\nand day level, adding columns for each \\nconversion type.Schema Table Description',\n",
       "  '536\\nTable specific accessaccount_reconciliation_i\\nnt \\n   \\n  \\nperpay_fpa_datamart_int  \\n   \\n  \\nperpay_risk_datamart_e\\nxt \\n   \\n  \\ndomo_ext  \\n   \\n  \\n \\nsegment_logs  \\n  \\n   \\n \\nsegment_perpay_core_p\\nroduction \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_core_production \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production \\n  \\n   \\n \\nsegment_perpay_web_p\\nroduction \\n  \\n   \\n \\nsegment_aircall  \\n  \\n   \\n \\nsegment_intercom  \\n  \\n   \\n \\nsegment_kickoff_labs  \\n  \\n   \\n \\nsegment_mandrill  \\n  \\n   \\n \\nsegment_perpay_core_p\\nroduction_int \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production_int \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production_tra\\ncks \\n  \\n   \\n \\nsegment_perpay_web_p\\nroduction_int \\n  \\n  \\n  \\nsegment_perpay_web_p\\nroduction_tracks \\n  \\n  \\n  \\nfivetran_branch  \\n  \\n   \\n \\nfivetran_tiktok_ads  \\n  \\n  \\n  \\nfivetran_iterable  \\n  \\n  \\n  \\nperpay_marketing_data\\nmart_ext',\n",
       "  'each Census synch) before it can be transported to Iterable. Rather than creating one large table, it may be more efficient to create five \\nsmaller tables, one for each JSON object that needs to be created. This would allow for splitting up the tasks that contribute to each \\nindividual synch. Presumably there will be tasks that are common to all or most of the synchs that will need to run before the DAG splits up (see below which sections are complete)\\ncompleted\\ncompleted',\n",
       "  'engineering.\\nBusiness Context:\\nAn application for a card account is created in Deserve’s system when a user reaches the card terms page in Perpay’s application flow.\\nRollforward Table Logic:\\nThis field is not used elsewhere in the query. That said, it is useful to have in this table because when working with the production table, \\nit is often required to join in other Deserve reporting tables.\\nSource Tables:\\npublic.card_account\\nDefinition:\\nThe email associated with the Perpay user.\\nBusiness Context:\\nAll users who have signed up on the Perpay platform have also provided an email.\\nRollforward Table Logic:\\nThis field is not used elsewhere in the query. That said, it is useful to have in this table because when working with the production table, \\nan email associated with an account will often need to be provided to external teams (for example Product, Accounting, etc.) for \\ninquiries about specific accounts.\\nSource Tables:\\npublic.user\\nDefinition:',\n",
       "  \"597\\nDerya's Transition\\nDumping documentation links here for easy access:\\nAirflow Setup-Teardown Staging Framework \\nPlease see all subsection of this folder.\\nRelated Looms are under the Code Walkthroughs section.\\nWithdrawal NACHA File Generator \\nFlattened Merged Users \\nExperian Account Review Ingestion (Loom)\\nDeserve SFTP Ingestion DAG (Loom)\\nDeserve Clarity Underwriting Ingestion (Loom)\",\n",
       "  'object was created for $151.88, the account balance was incremented by $151.88 from -$2.00 to $149.88, and the loan status was updated \\nto partially_refunded. However, the last step of removing funds from the account balance and reducing the loan\\'s principal balance \\nfailed a verification check because the account balance was less than expected ($149.88 instead of $151.88).This meant that the refunded \\nmoney stayed in the user\\'s account balance and was later used in payment towards other items as Talha mentioned.How would you like the \\ndata to look given this history?\\nDE Follow-Up: is there a field of some sort that you guys have to indicate the refund went towards the account balance instead of loan \\nbalance?\\nEng Answer: From my current understanding of the code, at the creation of all refunds, the user\\'s account balance is incremented by the \\nrefund amount. Subsequently, the account balance is decremented when the loan is \"paid off\" by the refund, which is what failed here.I\\'m',\n",
       "  '263\\nDeserve SFTP Ingestion\\nIn an ideal scenario, Deserve will send us a new set of files each day, comprising\\ndeserve_cleared_transactions_eod_report\\ndeserve_daily_settled_transactions_report\\ndeserve_net_settlement_eod_report\\ndeserve_payments_eod_report\\ndeserve_rewards_daily_summary_report\\ndeserve_applications_eod_report\\ndeserve_disputes_eod_report\\ndeserve_accounts_eod_report\\ndeserve_cards_daily_report\\nTypical Cadence\\nCurrently, our Deserve SFTP DAG structure executes once daily at 9:30 AM EST to ingest these new files. This takes about 7 minutes.\\nThere is also an option to manually trigger the DAG with a configuration to initiate a full rerun ({\"full_rerun\": true}). However, this \\nprocess consumes nearly 4 and a half hours. During this time, tables are dropped, and files are gradually appended by sftp_dt. This means \\nthat the full dataset becomes temporarily inaccessible for queries.\\nFluctuations in Deserve Report Data',\n",
       "  'too much history. This process involves deleting the data stored in the S3 buckets (which were created by the processes alluded to in the \\nabove section), then copying the prod data over to staging, and finally, running Glue crawlers over these S3 locations so Redshift knows \\nwhere the data is. Before the issue was fixed, we were using Glue Crawlers with their default settings which included the options “Update \\nthe table definition in the Data Catalog” and “Delete tables and partitions from the Data Catalog.“\\nWith these settings, when the Glue crawlers look at the parquet files that have just been copied over from prod, two things happen. First, \\noften the file names are completely different since prod and staging environments generated the original datasets at different times with \\ndifferent identifiers in the file name. Second, the crawler no longer knows the data types of the files. So it will scan the new files, and will try',\n",
       "  'query = f\"\"\"select\\n                     *\\n                 from {self.test_schema}.card_balance\\n                 where id is null or\\n                     card_account_id is null or\\n                     user_id is null or\\n                     borrower_id is null or\\n                     created_ts is null or\\n                     current_balance is null or\\n                     rank is null or\\n                     current_ind is null;\"\"\"\\n     data = self.spectrum.query_to_df(query)\\n     error_msg = f\\'FAIL: Null values present!!! {data.notnull().any()}\\'\\n     self.assertTrue(data.empty,error_msg)',\n",
       "  'you can be sure you have the necessary \\ntime to address the underlying issues. \\nIntegration A simple health check on an integration \\nservice will indicate whether that service \\nis running. For more detailed monitoring, \\na metric monitor can be used to gauge \\nspecific information about an integration. \\nLive Process A process monitor is a health check which \\nsimply returns the status of matching \\nprocesses. This check is highly useful for monitoring \\nmany services running on multiple hosts. \\nWe don’t use this now, as we’re only \\nmonitoring a single host.\\nLogs Log monitors alert when a specified type \\nof log exceeds a user-defined threshold \\nover a given period of time. Common use cases for log monitors \\ninclude code exception errors or build job \\nnotifications.\\nNetwork A network monitor checks whether a \\ngiven endpoint is active. Monitors also \\ncan alert over a percentage on a cluster \\nbased on custom network tags defined \\nin the Agent and host tags.',\n",
       "  '474\\nvarianceWe take the starting_balance and add any transactions, fees, charges, etc within the day that would increment the card account \\nbalance from an accounting standpoint and subtract any refunds, returns, disputes, adjustments, etc that would decrement the card \\naccount balance from an accounting standpoint.  \\nRight now, we add(+) to card starting balance…\\ntransactions\\naccount_opening_fee\\nmonthly_fee\\nlate_payment_fee\\nshipping_fee\\ninterest_charge\\ncash_back_adjustment\\ncredit_balance_refund\\nreturned_payments\\nWe subtract(-) from card starting balance…\\naccount_opening_fee_adjustment\\nmonthly_fee_adjustment\\nlate_payment_fee_adjustment\\ndebit_card_processing_fee_adjustment\\ninterest_charge_adjustment\\ncash_back\\nrefund\\ndispute_won\\ndispute_lost_withdrawn\\ndispute_provisional_credit\\ndispute_write_off\\npayments\\nSource Tables:\\nstarting_balance comes from atomic_int.card_balance, which pulls current_balance from webhooks first through the',\n",
       "  '502\\nCommerce Report Variances\\nTracking\\nThe following DOMO cards reflect the variance that occurs in perpay_accounting_datamart_ext.commerce_loan_daily_rollforward\\nCommerce Loan Variance Tracking by Day\\nCommerce Loan Cumulative Variances over Time\\nOverview\\nCOMPLETED variance issues are marked in green. \\nBLOCKED variance issues are marked in red and imply DE is waiting on a response from another team or an action from another team to \\nmove forward.\\nIN PROGRESS variance issues are marked yellow, and this means DE is actively investigating the variance or coding the solution.\\nIN DE REVIEW labeled variances are ones with a PR up for a fix. This means the update is ready, and it’s waiting to be reviewed before the \\nchanges are deployed.\\nNOT STARTED variance issues are in gray; these have been identified/DE may have a hunch as to what’s going on but has not picked up the \\nissue for investigation yet.\\n1 NOT STARTED  Null source dates (link): The source dates for these are null. I',\n",
       "  'Cloud\\nOther 3rd Party Tools\\nThese tools do not fit into any of the headings above but are worth noting\\nKickoff Labs\\nTool for credit card waitlist\\nData is pushed into Redshift through Segment\\nShortio\\nHistorically used as a link shortener for Iterable texts, but no longer\\nWe still fetch the data though\\nQuickbooks\\nUsed for overall company finanicals\\nThe DOMO api is used to load a quickbooks spreadsheet into a dataframe which is then pushed up to the db',\n",
       "  '59\\nCredit Bureau Reporting\\nOverview: \\nThis document outlines the reporting logic for data we will send to Experian and Equifax.\\nProcess Summary:\\nThe process of generating data to send to Experian consists of the following steps, further outlined below:\\n1. Determine borrower sample (DEPRECATED)\\n2. Determine Perpay parallel of field definitions for data required\\n3. Gather borrower history data from the datamarts, formatting each field to Experian’s precise standards\\n4. Convert data into Metro 2 format, a specifically formatted text file\\n5. Validate that the Experian data is consistent within itself, with rules outlined within the Metro 2 documentation\\n6. Setup a programmatic SFTP connection to the server where the file will be sent on a regular basis\\n7. On an ongoing basis, do manual audits of a handful of borrowers sent to Experian to verify the data is accurate.\\n8. On an ongoing basis, field requests to cancel Perpay Plus enrollments.',\n",
       "  '602\\nData Validation: Great Expectations\\nGreat Expectations is an open source tool to run data pipeline tests. These tests include validation column values to be between certain \\nvalues, null checks, and general column statistics like mean/median.\\nResources:\\nSummary of the problems Great Expectations tries to solve. This resonates with issues in our organization: some logic is broken in \\nproduction that should have been spotted by data being on the fritz far earlier.\\nhttps://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a\\nLeaders at DataBricks outline how to leverage Great Expectations. It can be used as intermediate steps in a pipeline to validate data.\\nhttps://databricks.com/session_na20/automated-testing-for-protecting-data-pipelines-from-undocumented-assumptions\\nExamples:\\nGreat Expectations automatically renders data documentation from validation tests. Example:\\n \\nAdditional features:',\n",
       "  \"As you can tell, Option 1 is far more preferable to the latter as it is a tedious task to have to read through code files to replace the Jinja \\nformatting, if not confusing for any team members who might not be familiar with Jinja. However, there is a issue with the first option which is \\nthe limitation of running only on .py files. Thus, new implementation was needed in order to be run on .sql files as well.\\n \\neasy_render.py (old version)\\n \\neasy_render .pyThis is not a required read in order to understand how combined_render.py functions, but gives helpful background context (and \\nmy thought process).\\n1import sys\\n2import argparse\\n3from jinja2 import Template\\n4\\n5class EasyRenderer (object):\\n6    def __init__ (self, global_file_name ):\\n7        self .global_file_name = global_file_name\\n8        # Finds the global path of the repo on anyone's environment\\n9        self .base_path =  __file__ .rpartition ('perpay-airflow' )[0] + 'perpay-airflow/'\",\n",
       "  '324\\nIt appears 4 “refunded” transactions are missing from the Perpay Deserve data. Unclear exactly what a “refunded” transaction is - a guess: \\ndeclined transaction. These transactions occurred in quick succession of each other, and is only one user, so I’d guess it was maybe too \\nmany webhooks / APIs for the system to handle.\\nImpact // Examples:\\nTransaction IDs (less the ones created by Deserve employees)\\n6bf38ad0-4ff7-529e-a694-280bda97c1f1\\nf7657edd-b4f4-570f-9666-ac26b1426cf0\\n8663ecec-f90a-544e-97a6-8a3502d89d2e\\ncc32c5db-df7e-542d-9428-717417b31aa1\\n \\nPerpay Deserve Data Issue # 4: Transaction Latency\\nThere is latency between when a transaction is reported settled and when it is marked as settled in our database. Choosing 2 hours as \\nour baseline latency:\\n \\nThere are 4,584 (out of 114,769, ~4%) transactions with this latency. Of transactions with latency, the average latency is 83 hours. Of all \\ntransactions, the average latency is 3 hours.\\n \\n22limit 100;\\n1with transaction_1 as (',\n",
       "  '138\\nsimilarly,\\nError explanation\\nMost often, the error is from one of the core (public) tables- we just need to figure out what that is specifically.\\nInvestigation\\nLet’s see where the nulls are originating. I suggest starting with the temp_int.card_balance table because its query is short to dissect.     AssertionError: False is not true : FAIL: Null values present!!! id                  True\\n     card_account_id     True\\n     borrower_id             False\\n     user_id                      False\\n     created_ts               True\\n     current_balance     True\\n     rank                          True\\n     current_ind             True\\n     dtype: bool\\nfor Test_CardTrans\\n    AssertionError: False is not true : FAIL: Null values present!!! id                       True\\n    card_transaction_id      True\\n    amount                           True\\n    status                              True\\n    transaction_type            True\\n    transaction_category    True',\n",
       "  'Returned payments will no longer be deleted and we’ll instead have VALID or RETURNED statuses.\\nWill be complemented with a status table, more on that later.\\npublic.withdrawal_request\\nCurrently, we are deleting a subset of the cancelled withdrawal requests and there is no programmatic way to differentiate the ones \\ngetting deleted. We will no longer be doing this. Instead, they will just be marked CANCELLED like the rest of the cancelled withdrawal \\nrequests.\\npublic.withdrawal_request_status\\nThe changes here are tied together with that of the previous table.\\nHere are the new tables that will be created to complement the above:\\npublic.deposit_status\\nThis one is already out.\\nWill keep the history of status changes for the public.deposit table.Under construction.',\n",
       "  'It happens every day so the errors are not noticed or addressed\\nThe user dm being pulled into DOMO is extremely wide and takes over an hour and a half to load load into DOMO\\nSometimes it can run into itself because the task is kicked off every 2 hours\\nThis can case mass locks and cascading issues - additionally errors are not noticed or addressed\\nMany tables/columns are created and pushed to DOMO which slows down I/O\\nAudit tables/columns for features that are unused\\nAny error causes an entire data model to fail any update\\nSometimes this is a good thing, however, there are many instances where there is only a minor issue that should not completely kill \\nthe job and issues should just be highlighted\\nMarketing glue crawlers can fail due to the large data size and process\\nCreates stale marketing audiences\\nAlso an issue that is just allowed to self correct over time\\nGlue creates duplicate tables/partitions, seemingly randomly with a random hash at the end\\nThis is a ton of duplicated data',\n",
       "  'file IDs starting from A going down the alphabet. Old school, but fine. This meant that we needed to track the number of NACHA files we \\ncreate everyday and label our NACHA files accordingly. To do this, I used the information available on the Airflow backend database to count \\nthe number of NACHA files we produced prior to the run. When you read through the DAG file, this is logic embedded as part of the wrapper \\naround NACHAFile().main() call.\\nThere are two configurations you can feed to the DAG: clear_sheet, which can be set to false during testing in staging, and file_id to \\noverride the file ID value, though I don’t recommend this approach, it should not be used if the problem can be fixed with the solution in the \\nwarning box at the top of this page.\\nScheduling\\nThe most quirky part of this job was its schedule. The reason for this is you cannot CRON your way out of the 10am, 1:30pm, 5pm situation.',\n",
       "  'have hit a canceled status on this day, \\nresulting in the payments made up to \\nthis point being added back to the \\nuser’s core account balancePayment \\nTracking\\npayment_perpay_plus_completed Numeric\\n(38,2)Feature payments that have been \\ncompleted, resulting in the amount \\nbeing deducted from the user’s core \\naccount balancePayment \\nTracking\\nending_balance Numeric\\n(38,2)The balance resulting from all balance \\nmovement being applied to the day’s \\nstarting balanceCalculating \\nDaily \\nBalances',\n",
       "  'this product in the last 30 days. Counts for \\nproducts with parents are rolled up to the \\nparent, and this field will be 0.\\ncount_repayment_last_365_days bigint Count loans that went into repayment for \\nthis product in the last 365 days. Counts for \\nproducts with parents are rolled up to the \\nparent, and this field will be 0.\\ncategory_name_1 varchar(94) The category name of the category.\\ncategory_name_2 varchar(94) The category name of the next level sub \\ncategory.\\ncategory_name_3 varchar(94) The category name of the next level sub \\ncategory.',\n",
       "  \"1,\\n0\\n0\\n0 \\nto \\nS\\nh\\no\\np \\nN\\no\\nw, \\nP\\na\\ny \\nL\\nat\\ner                                                                    H\\nT\\nM\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0.\\n0\\n0                                                     tr\\nu\\ne              fa\\nls\\ne       fa\\nls\\ne         fa\\nls\\ne               fa\\nls\\ne    E\\nN\\nG\\nLI\\nS\\nH                      2\\n0\\n2\\n4-\\n0\\n1-\\n0\\n2\\nT\\n1\\n4:\\n5\\n6:\\n5\\n9-\\n0\\n5:\\n0\\n0                              ['\\nP\\nu\\ns\\nh\\nn\\na\\nm\\ni']                  []                                          fa\\nls\\ne\",\n",
       "  'who started the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automated and runs \\non a schedule\\ncancelled_by varchar The username of the Domo user \\n(a perpay.com email address) \\nwho cancelled the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automatically \\ncancelled \\nstart_time timestamp The timestamp at which the \\nexecution began\\nend_time timestamp The timestamp at which the',\n",
       "  '363',\n",
       "  'Census612\\n                Fivetran613\\n                Google/Facebook614\\n           Tech Stack615\\n                Airflow Deployment616\\n                     Jump Boxes617\\n                     Docker Containers618\\n                AWS619\\n                     DMS620\\n                     Redshift621\\n                     IAM622\\n                     S3623\\n                     EC2624\\n                     Terraform625\\n                Datadog626\\n      How-to articles627',\n",
       "  '409\\nemail. For the latter, it is a one-to-one relationship with a from and to email. In either case, we extract the created timestamp from the \\nevent table as our main timestamp.\\nOther relevant data comes from the public.user table which has important information in the is_active and ssn_is_verified \\ncolumns. We will extract these columns for each email we come across in the events described above. Finally, we will also extract all user \\ninformation from the public.user table where the email ends with .deleted to be able to account for deleted accounts. These accounts, \\nas per Engineering, will retain all of their transaction history and balances, but no PII will be available for them.\\nSome of the above understanding was arrived at iteratively after some back and forth with Engineering and represents the final state of our \\nunderstanding of the raw data. The following is a high level description of the analysis we performed before settling on our ETL approach for \\nthis project.',\n",
       "  'Non-Compliance\\nThe logic for identifying customers with non-compliant data is contained in compliance_checker.py. The customers identified are held out \\nfrom the report and their consumer account numbers are sent in the perpay_plus_reporting slack channel. Here is an example of what \\nthat message might look like:\\nMetro2 Errors\\nThe 375 page Metro2 manual provides guidelines on how data ought to be reported to the credit bureaus. We have two \\ntest_relational_m2.py files (one for card and one for marketplace) that have checks in place to ensure that the data we are reporting for \\nCard Compliance Checker Output 2024-02-28',\n",
       "  '389\\nbe considered completed until a loan hits a status of repayment. This means that payments on loans that have never hit repayment are \\nnot considered and payments on loans before the loan hits repayment are consolidated to the repayment date.\\nCanceled Marketplace Payments: This column captures payments made to marketplace loans that were later canceled, and as a result \\nhad the payments returned to the user’s core balance. First, we capture the first instance of a loan_id that currently has a status of \\ncanceled hitting a status of canceled in the loanstatus table. Canceled marketplace payments are considered to be the sum of \\namount in the payment_detail table for the given loan_id after excluding rows in the payment_detail table that have a payment_id \\nthat also appears in the returned_payment table. Canceled borrower credits are also backed out of this amount using the same logic',\n",
       "  'both jobs must be ran before starting the process over, meaning you cannot run the first job twice in a row.\\nWhat’s going on when we run these two jobs?\\nJob 1- The invoice_matching DAG\\n \\nJob 1 resides in dags/analytics/invoice_matching/invoice_matching_dag.py. Be careful not to confuse this with the other \\ninvoice_matching_dag.py file which is responsible for the V2 process.\\nPSA:\\nIf you run Job 1 and then Preston/Accounting notices there was a mistake in the Non Parsed Partners sheet (which feeds into Job 1 \\nper above), you’ll need to remove the mistaken data from the Non Parsed Partners sheet and remove it from the DoInvoiceMatching \\nsub sheet of the Invoice Matching sheet. If the Non Parsed data is then fixed and newly added, and you want to rerun Job 1 again, \\ndrop the temp_int.invoicing_docparser_output table.\\n \\nIf you ran Job 1 and Job 2 and then they notice there was a mistake in the Non Parsed Partners sheet, you’ll need to remove that',\n",
       "  \"31        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status i ON a.borrower_id = i.borrower_id AND cast\\n32        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status j ON a.borrower_id = j.borrower_id AND cast\\n33        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status k ON a.borrower_id = k.borrower_id AND cast\\n34        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status l ON a.borrower_id = l.borrower_id AND cast\\n35        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status m ON a.borrower_id = m.borrower_id AND cast\\n36        -- !!!!!!!!!!!!!!!!!!! Here's the join. Not limiting population.\\n37        LEFT JOIN feature_enabled fe on a.borrower_id = fe.borrower_id\\n38    WHERE a.card_account_id not in (2983, 3377, 385, 752, 5556)\\n1...\\n2create table {{temp_schema}}.borrower_attribute_1a as\\n3  select distinct a.borrower_id,\\n4         cast(a.created as date) as feature_enrollment_created_dt,\\n5         a.status as feature_status,\",\n",
       "  \"When OPS does the rec process, there are many ACH deposits that never end up being applied to any user account. This is \\nbecause these deposits can't be safely tied to a user, so engineering can't have them trickle down their data pipelines. However, for \\naccounting purposes, this is exactly what needs to be done.\\n3. Merged accounts do not go through the rollforward, old or new account\\nPreston basically said none of these really come through, however, I know that in core_account_daily_rollforward there is a \\nwhole section that accounts for merged accounts. We need to see what the code is doing vs. what accounting is expecting.\\nDuring the period 2023-06-07 to 2023-06-16 which Preston analyzed, we noticed that there were only 6 users whose accounts \\nwere merged. These six users' deposit amounts during the period only account for ~3% of the total variances that Preston observed, \\nindicating that merged users are likely not a major source of the variances.\",\n",
       "  'So the duplicates proliferated.\\nThe Resolution\\nIn order to stop this from happening, we need the Glue crawlers to update the table definitions in-place. This requires it (1) ignore the file \\nname changes, and (2) update the table schema in-place. To force this behavior we had to switch from using S3 locations as the data \\nsource to using the Data Catalog as the data source. This necessitates the existence of tables in the Glue Data Catalog for any databases \\nyou want to update with new data and for these tables to be selected as the data sources. This eliminates the need to specify the “Table \\nlevel” configuration and “Table Grouping Policy” is set to \"CombineCompatibleSchemas\" (API and Terraform) or to “Create a single',\n",
       "  'them or update them incorrectly, causing the parser to capture incorrect information. This leads to the underpayment of bills and \\nwork on DE’s end to fix broken DAGs. Our invoice matching v2 DAG also checks that Docparser pulls in very specific fields, which \\ndoes not need to be the case for every vendor. Thus, the removal of this will be a lot less touchy. I will obtain an updated list of all \\nrequired fields from Preston, which we can check for downstream in the code before pushing the data to QB. \\nSimilar to CSVs, we can utilize Zapier for all attachments into S3, including the PDFs.\\nAmazon Textract uses ML to extract data from documents and forms. One of its specialties is analyzing expense documents like \\nInvoices (check out this invoice extraction example using their dummy invoice- you can also upload your own to see how it’ll parse). \\nI want to verify all our current invoices look good through their test parser, but if that’s the case and it captures everything we need, I',\n",
       "  '534\\nTable-specific exceptions\\nUser-specific exceptions\\nmonthly_performance_files_int - @Kevin Allison has access\\nQ: Starting in line 944 and below, are we giving airflow and each person select privileges on all schemas? \\nQ: Starting line 1871 is this just specific public tables or all\\n s_in\\nt\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\nTabl\\neAir\\nflo\\nwad\\nmi\\nnpe\\nrp\\nay\\n_a\\ndm\\ninDE \\nSu\\nper\\nuse\\nrsDE \\nUs\\nersDE \\nInt \\nInt\\ner\\nnsDE \\nEx\\nt \\nInt\\ner\\nnsDS \\nSup\\neru\\nser\\nsDS \\nUs\\nersDS \\nInt \\nInt\\ner\\nnsDS \\nEx\\nt \\nInt\\ner\\nnsDA \\nUs\\nersDA \\nInt \\nInt\\ner\\nnsDA \\nEx\\nt \\nInt\\ner\\nnsRi\\nsk \\nUs\\nersRi\\nsk \\nInt \\nInt\\ner\\nnsRi\\nsk \\nEx\\nt \\nInt\\ner\\nnsCr\\nedi\\nt \\nRe\\npor\\ntin\\ng \\nPr\\noje\\nctDE \\nInt\\nern\\ns \\nTe\\nsti\\nng \\nPr\\noje\\nctCard \\nInve\\nstiga\\ntions \\nProje\\nctDS \\nTrai\\nnin\\ngInt\\ner\\nn \\nPr\\noje\\nctsIden\\ntity \\nGra\\nph \\nInte\\nrns\\npubl\\nic.p\\naym\\nent_\\nrisk\\ndata\\npubl\\nic.a\\nchd\\netail\\npubl\\nic.u\\nser\\nper\\npay\\n_ge\\nnera\\nl_da\\ntam\\nart_i\\nnt.u',\n",
       "  '625\\nTerraform\\nVideo topic ideas:\\nwhy do we have terraform?\\nwhat is a terraform plan? what is terraform drift?\\ngeneral overview of writing infrastructure-as-code in terraform (making resources, attaching them to each other, etc.)',\n",
       "  'Creating a Step In Between\\nDBT’s Staging Layer\\nThis confluence page covers DBT’s implementation of a “source layer”.\\nIn this page, they describe its purpose:\\nThe purpose of staging models (in our convention) is just to clean up and standardize the raw data coming from the warehouse, so \\nwe have consistency when we use them in downstream models.\\nIn our dbt project, we’ll place them in a staging folder, and prefix filenames with stg_ for easy identification (so our Zendesk chat \\nlog would be stg_zendesk_chats, which is based on the raw zendesk.chats source table).\\n1. \\nThey’re typically a one-to-one reflection of each of our raw sources, and we do really light transformations at the staging layer. We \\nwill very rarely join data models at the staging layer, but instead will perform transformations like:\\nField type casting (from FLOAT to INT, STRING to INT, etc), to get columns into the proper type for downstream joins or \\nreporting\\nRenaming columns for readability',\n",
       "  '7. Go to the Experian SFTP website\\na. The credentials and security question is saved in 1passoword, Data Engineering General folder\\ni. Experian SFTP Server (Quest and Ascend Program)\\n8.  Move to the to_xpn folder and upload the CSV into that folder\\n9. Email Kristine.Lobo@experian.com and  Matthew.Simon@experian.com and cc Zach, saying “We have uploaded a new Quest file for \\nprocessing. Please process the same as last month and include the positive factor codes”. Feel free to add your flair, Kristine and Matt \\nare very nice.\\n10. For posterity, save the file here:\\na. Add a numerical increment to the file. We hold on to these just in case there is an issue with how they processes something.\\n11. Go take a victory lap because you are done!\\n \\n1subl docker exec -it perpay-airflow_worker_default bash\\n1python3 -m analytics.credit_reporting.lib.scripts.script_experian_pull_monthly_vantages -p -s\\n1dags/analytics/credit_reporting/lib/untracked',\n",
       "  '1. Private vs. public methods. Allow subsystems to expose only certain pieces of their functionality to other parts of the DAG. \\nAllow a given dbt project to, for example, expose five models but keep the 80 upstream models private. This allows these 80 \\nupstream models to change without breaking downstream code maintained by others; as long as the 5 public models remain \\nunchanged then the project maintainers can change the others as required. Importantly, this needs to be governed inside of dbt \\n(using the ref statement) and not in database permissions!\\n2. Allow multiple versions of a model to be consumed at the same time so that downstream models can implement an upgrade \\npath. Publishers of an API give consumers of that API a fairly long period for them to upgrade to the next version—sometimes \\nyears! There needs to be similar functionality for creators of dbt models. You can’t just change something and then expect all',\n",
       "  'You can control the following schema entities:\\ntables - contract is applied when a new table is created\\ncolumns - contract is applied when a new column is created on an existing table\\ndata_type - contract is applied when data cannot be coerced into a data type associate with existing column.\\nYou can use contract modes to tell dlt how to apply contract for a particular entity:\\nevolve: No constraints on schema changes.\\nfreeze: This will raise an exception if data is encountered that does not fit the existing schema, so no data will be loaded to the \\ndestination\\ndiscard_row: This will discard any extracted row if it does not adhere to the existing schema, and this row will not be loaded to \\nthe destination.\\ndiscard_value: This will discard data in an extracted row that does not adhere to the existing schema and the row will be \\nloaded without this data.\\nThis link discusses schema evolution with dlt:\\nSchema evolution with dlt',\n",
       "  'REDSHIFT_SPECTRUM_README.md.\\nMore Technical Notes\\nFor guides on testing on staging and other technical stuff, please reference https://github.com/Perpay/perpay-airflow/blob/feature/spectrum-cont-\\ne/REDSHIFT_SPECTRUM_README.md',\n",
       "  \"208\\n(2) check if already_parsed is true or false\\nif true- do nothing. Proceed to condition statement 3.\\nif false (default)- we enter the block\\n1: Logs '[InvoiceMatching] Fetching docparser and customer partner outputs...'\\n2: Creates a Redshift table for each vendor in the Non Parsed Partners Google Sheet. Saves these to the temp_int \\nschema with the table name invoicing_unparsed_<vendor> (i.e. temp_int.invoicing_unparsed_rymax). \\n- these three vendors are in our current sheet, so one table is created for each:\\n- BestBuy\\n- Rymax\\n- FragranceNet\\nAlso creates a Redshift table for the VendorKeyName sheet. This is saved to temp_int.invoicing_vendor_key_matching.\\n(if you want to look into the function that creates these tables, it’s all_invoicing_sheets_to_redshift in \\ndags/utils/lib/google_sheet_integration.py. It loops through the Non Parsed Partners sheet and calls the sheet_to_redshift\",\n",
       "  \"Looking Forward\\nGoing forward, we need to check out stateful ingestion. If it keeps our old UI state and only updates the tables, that's perfect. There is likely \\nsome middle ground where it won't update all table structure or the UI, and we can't pick and choose what we'd like. This should be tackled \\nsomehow with regards to Redshift ingestion overwriting the UI. A patched fix would be running a script right after ingestion that references a \\ntable that holds state between ingestion. This isn't too clean - it'd be better to have DataHub run as we'd like. \\nIn regards to the Airflow pushes leaking datasets to the DataHub UI/frontend, there's a new feature introduced in the latest DataHub version \\n(0.13.3) called materialize_iolets. This seeks to tackle our exact problem in a true/false config. If it's actually that simple, that part of the \\nequation would be fixed. We shall see.\",\n",
       "  '155\\nRather than ignoring parameters, we are using this workaround to resolve drift issues for the configuration. In the event the GitHub issue is \\ndeprecated, the workaround steps are summarized as follows:\\n1. Set the value of the wlm_json_configuration parameter using jsonencode\\na. value = jsonencode(jsondecode(var.wlm_json_configuration))\\n2. Apply those changes via terraform\\n3. retrieve the JSON string for ParameterValue by querying through the AWS CLI\\na. AWS_PROFILE=pp-stage-admin aws redshift describe-cluster-parameters --parameter-group-name ana-stage-redshift-\\nparameter-group\\nb. output example: { \"ParameterName\": \"wlm_json_configuration\", \"ParameterValue\": \"[{\\\\\"query_concurrency\\\\\":5}]\", \\n\"Description\": \"wlm json configuration\", \"Source\": \"user\", \"DataType\": \"string\", \"ApplyType\": \"static\", \\n\"IsModifiable\": true }\\n4. Set the value of the wlm_json_configuration parameter using the hard coded ParameterValue\\na. example: value = \"[{\\\\\"query_concurrency\\\\\":5}]\"',\n",
       "  '251\\nCredit Reporting Weekly\\nEvery week we report the current behavior by all of our borrowers who have opted-in to Perpay Plus back to the credit bureaus. This \\nprocess is largely automated, however there are manual checks involved especially if there are errors.\\nBelow are the emails for reaching out to Transunion, Equifax, and Experian.\\ntransunion - dasworkpool@transunion.com, Joseph.Scardullo@transunion.com \\nequifax - celestine.cooper@equifax.com\\nexperian - Matthew.Simon@experian.com\\n \\nNote: If any AUDs are needed to be submitted through E-Oscar, the below are the subscriber codes for each bureau…\\nExperian: 2919940\\nTransunion: 2HMG001\\nEquifax: 496FF00527\\nThe cleaned report MUST be sent BEFORE the weekend \\n1. The credit_reporting_data_raw dag runs automatically every Monday\\na. This creates the “raw” values and get deposited in XXX with a timestamp\\nb. The end task of this dag is a Compliance Check which makes sure that all borrowers being included in the credit reporting are',\n",
       "  'runs. \\nExtensions\\nWhile we have a very minimal Travis file, it is quite easy to build on top of by adding test files or scripting. Travis has many capabilities that \\nwe could choose to play around with as well, such as using Docker within it or notifying via Slack. We can add further Airflow testing, such \\nas DAG unit testing, and automate it. We could also choose to only test modified files from the current PR, which could decrease build time. \\nWith flake8, we can add custom rules within our code if we’d like to make our syntax errors Airflow specific. This would require adding a',\n",
       "  '266\\n5. Kick off the Card Object Census sync (this is colored so it is not skipped)\\n66                \\', \"ind\": \\' || case when approved_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as approved,\\n67            \\'{\"dt\":\\' || case when h.created_ts is not null then concat(concat(\\'\"\\', to_char(h.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n68                                              else \\'null\\' end ||\\n69                \\', \"ind\": \\' || case when activated_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as activated,\\n70            \\'{\"dt\":\\' || case when k.created_ts is not null then concat(concat(\\'\"\\', to_char(k.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n71                                              else \\'null\\' end ||\\n72                \\', \"ind\": \\' || case when provisioned_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as provisioned,\\n73            \\'{\"enabled\":\\' || enabled ||\\n74               \\', \"started_card_appliaction\":\\' || started_card_appliaction ||',\n",
       "  'refund amount. Subsequently, the account balance is decremented when the loan is \"paid off\" by the refund, which is what failed here.I\\'m \\nnot aware of any field that directly indicates that the refund did not pay down the loan. I\\'ve had to cobble that together from looking at the \\nloan principal balance history and the fact that the current loan principal balance does not equal the following notional equation loan \\nprincipal balance = loan amount - payments - refund paydown + returned payments\\nFrom my reading of the code, when the refund verification checks fail, it looks like we:\\n1. Send arik an email\\n2. Write to our std_out logs, which are ephemeral and are not saved in the database\\nSo there\\'s no sort of log storage or event table data that you can query. The best check I can think of is: looking at the \\ncore_accountbalancehistory table, at the timestamp when the refund was created, was the account balance negative?\\nuncounted_refunds.csv refund_details.csv',\n",
       "  \"= 6697). This is breaking some tests on our end. Can someone look into this?\\n22    coalesce (b.id, j.card_id) as card_id ,\\n23    f.id as card_account_application_id ,\\n24    case when g.user_id is not null then 1 else 0 end as staff_ind\\n25from public.borrower d\\n26         left join public.card_account_application f on f.borrower_id = d.id\\n27         left join public.card_account c on c.id = f.card_account_id\\n28         left join public.card_card b on b.card_account_id = c.id\\n29         left join staff g on d.user_id = g.user_id\\n30         left join enabled h on h.borrower_id = d.id\\n31         left join arik_hard_code j on j.borrower_id = d.id\\n32where h.borrower_id is not null\\n1-- let's dobuble check the other data is there and get borrower_id\\n2select * from public.card_account where id = 5078; --- exists as we'd expect... let's up grab borrower_id\\n3select * from public.card_account_application where borrower_id = 3857277; --- also exists as we'd expect\",\n",
       "  '8b.deserve_id,       \\n9c.account_opened_at,       \\n10datediff(\\'second\\', cast(account_opened_at as timestamp), perpay_created_ts) as difference_in_seconds\\n11from card_int.card_borrower_fact as a\\n12left join public.card_account as b on a.borrower_id = b.borrower_idleft join cte as c on b.deserve_id = c.accoun\\n13where difference_in_seconds > 60;\\n1select *\\n2from public.card_transaction as a\\n3left join perpay_accounting_datamart_ext .deserve_daily_settled_transactions_report as b on a.deserve_id = b.trans\\n4where b.transaction_id is null and a.modified < \\'2022-11-16\\'  and a.status = \\'SETTLED\\'  and a.type = \\'REGULAR\\'\\n5order by a.created desc;\\n1Event Types\\n2    CARD_TYPE = \"card\"\\n3        - create or update card from api data\\n4    ACCOUNT_TYPE = \"account\"\\n5        - ignore ACCOUNT_REQUESTED, ACCOUNT_REQUEST_FAILED event_names\\n6        - create or update card account from api data\\n7        - if ACCOUNT_CREATED create card from api data\\n8    APPLICATION_TYPE = \"application\"',\n",
       "  \"fivetran_quickbooks_sandbox schema. This may be important to consider depending on what you're trying to test.\\nI like to copy the table into the fivetran_quickbooks schema (the schema actually utilized by the DAG) with select invoices to mimic the \\nSandbox QB info.\\nReset\\nA lot of times on staging, you’ll run the job, modify something in the sheet to see that it’s working, then want to reset. This process varies \\nbased on what you’re trying to do, but the most common form of reset goes as follows:\\ndelete the invoice from QB staging\\nif you want to delete a single one, you can find it, click the dropdown arrow to its right and hit delete\\nif you want to bulk delete for a vendor, you can mark that vendor as inactive, which essentially does the same (puts a suffix like \\n_deleted after each invoice). this allows you to recreate the vendor with a balance of $0.00\\ndelete the invoice histories\",\n",
       "  'investment/debt provisioning needs for Perpay by providing a transparent and verifiable trail of financial activities. This enhances the \\ncredibility and reliability of Perpay’s financial statements and helps to align with auditing standards. Moreover, individual Marketplace order \\nbalances are what our debt partners allow us to borrow against, so this level of detail is required for us to maintain compliance on our \\nborrowing arrangements.\\nNew Approach\\nThe previous iteration of account reconciliation had been done with the balance ledger tables core_accountbalancehistory and \\npayment_loanprincipalbalancehistory. Starting and ending balances for a given day would be sourced from these tables and the total \\nbalance movement on a given day would be calculated from the financial data available in the public schemas. In this paradigm, data would \\ntie-out when the ending balance for the day was equal to the starting balance added to the daily total balance movement. One of the issues',\n",
       "  'is charged off and settled with \\nTA, report 0. Otherwise, the \\noutstanding balance as of last \\nweek end.borrower_hist_d\\naily_status.balan\\nce\\nAmount Past \\nDueamount_past_du\\neTotal amount that are 30 days or more \\npast due.0 if the account_status =11; 0 if \\nthe account charged off and the \\ndebt was settled with \\nTrueAccord. Otherwise, \\nminimum_payment_amount * \\nmissed_pmts if under the \\nbalance, otherwise the current \\nbalance.borrower_hist_d\\naily_status:\\nmissed_pmts, \\nminimum_paym\\nent_amount\\nOriginal Charge-\\noff Amountoriginal_charge_\\noff_amountFor status codes 64 and 97, report the \\noriginal amount charged to loss.When the account status is 97 or \\n64, report the balance at the end \\nof last week. Otherwise, 0. \\nDate of Account \\nInformationdate_of_account\\n_infoAll account information (such as account \\nstatus and current balance) must be \\nreported as of the date in this field. For \\nmonthly reporters, report the date within \\nthe current month that represents the',\n",
       "  'who started the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automated and runs \\non a schedule\\ncancelled_by varchar The username of the Domo user \\n(a perpay.com email address) \\nwho cancelled the execution, if it \\nexists, otherwise \"system\" if the \\nexecution is automatically \\ncancelled \\nstart_time timestamp The timestamp at which the \\nexecution began\\nend_time timestamp The timestamp at which the',\n",
       "  \"51            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n52            and b.status = 'pending'\\n53        ) as g on g.borrower_id = a.borrower_id\\n54    group by a.borrower_id;\\n1create table {{temp_schema}}.plus_funnel_fact_1 as\\n2    select coalesce(b.user_id, e.id) as user_id,\\n3           min(a.server_received_time) as first_engagement_ts,\\n4           d.first_plus_eligible_dt,\\n5           f.status\\n6    from {{marketing_schema}}.amplitude as a\\n7    left join {{marketing_schema}}.amplitude_user_mapping as b on b.amplitude_id = a.amplitude_id\",\n",
       "  'It’s unclear why this generated on this day. The user’s first statement ends on 08/10 EST, so it does not correlate with a statement \\nend date. \\nIt appears that in the SFTP files, the statement information is updated on the last day of each month, regardless of where in the \\nstatement period that falls (\\n)\\nThe user’s fees_accrued increases by $9.00, but their current_balance does not increase. (\\n)\\nThe user’s minimum_payment of $15.00 was calculated on this day as well. \\nOn 08/10, the user’s first statement period closes. At time of statement close, their minimum payment owed is $15.00, which is due on \\n09/05.\\nOn 09/05, the user’s $15.00 minimum payment is due. \\nOn 09/09, a user initiates an ACH payment. Perpay holds this payment until 09/13. \\nOn 09/10, the user’s 2nd statement period closes. At time of statement close, their minimum payment owed is $30.00, due on 10/05.',\n",
       "  \"the data from the yes table (new_qb_df) into the all_new_qb_df.\\n- Then sort the data in all_new_qb_df by Bill No. \\n- Then we put that df back into the QuickBooks Bills Google sheet in the NewEntries sub sheetPut simply, this job takes data in from the Invoice Matching Google sheet (DoInvoiceMatching sub sheet) and moves it to \\nthe correct spot. It appends all rows with an ‘N' flag to the Pending sheet. It appends all rows with a 'Y’ flag to the \\nQuickBooks Bills (NewEntries) Google Sheet. No data is removed from any Google sheet. More specific info is below:\",\n",
       "  'shape and flow of the merge network made sense, the lead user was not the same as the last merged to account.\\nThe following are examples of cases (1), (2), and (4):\\nA cluster with cyclic merges.',\n",
       "  'move the money to another account. \\nThis results in neither the original, nor the second email showing up in the rollforward table.\\nAt other times, this problem can be resolved by RTP.23 \"ach\" public.auto_pay_payment\\n46 \"direct_deposit\" public.achdetailsource_rsn_cd \\n(i.e.content_type_id)source Source Table from Public Schema',\n",
       "  \"values on a monthly basis. These are pretty much sourced directly from the magento_sales_flat_order_item and refund tables, then \\nincorporated into our refunds data model, and finally presented in DOMO.\\nIn  contrast, the Commerce Loan Dailly Rollforward table focuses exclusively on the portions of refunds that contributed to reducing the \\nremaining loan balance on a per-loan basis. Since this analysis is conducted at the loan level, specific to each loan's balance on a given \\nday, the monthly aggregated total will naturally be lower when compared to the previous dataset.\\nTo provide further clarity, I'll use loan_id = 5304464, charge_id = 5304311 as an example: \\ncharge_5304311_eng.csv is straight from engineering's table, we see the refund on 2023-09-29 was $812.97. And when we look at our \\ntable,loan_5304464_commerce.csv , we have a lower refund amount of $733.27. \\nin this case:\\n1: starting that day: the balance was $771.62\",\n",
       "  '475\\nWe do the calculation ending_balance - ending_balance_calc\\nSource Tables:\\nN/A directly since it’s a calculation. \\nThe ending_balance and ending_balance_calc columns come from the logic described above in their specific sections.',\n",
       "  'Also an issue that is just allowed to self correct over time\\nGlue creates duplicate tables/partitions, seemingly randomly with a random hash at the end\\nThis is a ton of duplicated data\\n All non-SQL work is performed on the workers - this creates stress on the Airflow workers which can cause issues with the instance\\nAirflow is for orchestrating not executing\\nAll compute jobs could be launched in the same way as DS does now where the tasks would be isolated on their own, ephemeral, \\nEC2 nodes.\\nWhen an external table \\nBest Practices\\nAdding distkeys and sortkeys would be an easy win to improve performance of the Redshift cluster.  Benefits include:\\nPotentially downsizing the Redshift cluster as load is decreased.\\nImprove the efficiency for analysts: less time waiting for queries!\\nLeverage AWS Managed Airflow: this would offload a lot of personnel time onto AWS. Benefits include\\nWe would no longer need to dedicate time to managing Docker deployments.',\n",
       "  \"143\\n(Error) Nulls in Card ACH Payment Tracking table\\nWhen there are auto pay payments that are in an initiated status, the deposit and card payment IDs aren't created until the associated \\nACHTransaction moves into a completed state.\\n- During this time, there will be rows in the public.auto_pay_payment table with null deposit_id's and card_payment_id's.\\nAs a result of this, the test in the metrics layer DAG which checks for null values in the Card ACH Payment Tracking metrics table will \\nfail. This is a known issue. \\nSpecifically, the test that fails is the following query:\\nThe relevant error message is:\\n[ P R O D ] ERROR has occured!  \\nD A G : metric_layer  \\nT a s k : Test_CardPaymentACHTracking  \\nE r r o r : Bash command failed. The command returned a non-zero exit code 1.\\nExample: see this Slack thread\\nHowever, once the relevant ACHTransactions complete and the metrics layer DAG passes, the test will pass again. This means the\",\n",
       "  \"98        left join atomic_int.card_activation_events h on c.borrower_id = h.borrower_id and h.event_name = 'activation_deserve' and h.first_event_type_ind = 1\\n99        left join atomic_int.card_activation_events k on c.borrower_id = k.borrower_id and k.event_name = 'provisioning_complete' and k.first_event_type_ind = 1\\n100        left join atomic_int.card_activation_events m on c.borrower_id = m.borrower_id and m.event_name = 'application_started' and m.first_event_type_ind = 1\\n101        left join atomic_int.card_activation_events n on c.borrower_id = n.borrower_id and n.event_name = 'credit_pull_consent' and n.first_event_type_ind = 1\\n102        left join card_int.card_account_fact p on c.borrower_id = p.borrower_id\\n103        left join card_int.card_account_attribute as f on p.card_account_id = f.card_account_id\\n104        left join card_int.card_account_performance j on p.card_account_id = j.card_account_id\",\n",
       "  'Completed Collections Deposits: A completed collections deposit is any row in the platform_deposits metric layer with a source of \\ncollections and a status of completed. The amount of the row is applied as a deposit to the associated borrower_id on the date of \\nthe created_ts timestamp. This differs from engineering’s definition which applies collections deposits to the core balance when the \\ndeposit hist a status of pending.\\nCompleted Credit Card Deposits: A completed credit card deposit is any row in the platform_deposits metric layer with a source of \\ncredit_card and a status of completed. The amount of the row is applied as a deposit to the associated borrower_id on the date of \\nthe created_ts timestamp. This is the same definition that engineering uses for how credit card deposits are applied to the core balance.\\nCompleted Direct Deposits: A completed direct deposit is any row in the platform_deposits metric layer with a source of',\n",
       "  'Leverage AWS Managed Airflow: this would offload a lot of personnel time onto AWS. Benefits include\\nWe would no longer need to dedicate time to managing Docker deployments.\\nThe current workflow requires manual steps for a new deployment\\nAWS would manage scalability which would resolve both issues mentioned above: i.e. scheduler failing and consequent fallout\\nWe would no longer need to maintain terraform code for custom Airflow deployments: here',\n",
       "  'We currently follow this flow for the metric layer, and we intend to keep it. However, we want to add a new element where we filter out any \\ndata that has failed to avoid hard stops in the metric layer DAG run and then log for asynchronous triaging.\\n \\nThe second layer of tests occurs in the logic where our data models reside. The flow is as follows: \\ntemp table creation (using data from metric later tables) → tests for business logic (code-based testing) → table committed to the production \\nschema\\nWe currently follow a different flow for our data models, which we want to change. That current flow is:\\ntemp table creation (using data from metric later tables) → table committed to the production schema → tests for business logic (code-\\nbased testing) \\nThe current organization allows for data mart tables (ex user_attribute) to be available to anyone who has access to the particular datamart.',\n",
       "  'Borrower credits and payments prior to the refund are then calculated by using the logic found in Payment Tracking and deducted from the \\nloan balance. Refund amounts prior to the refund being observed are summed and considered to be existing refunds for the loan that are \\nalso deducted from the loan balance.\\nOne added filter applied to the refund tracking is to adjust for engineering’s negative account balance test that refunds entirely to the user’s \\ncore account balance in the event that a user carries a negative account balance at the time of the refund. In these cases, the refund is \\nexcluded from the loan refund tracking.\\nCalculating Daily Balances\\nIn the accounting reporting table, payments, starting balances, and refunds are subject to repayment date consolidation described here.\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance.',\n",
       "  \"297\\n1\\n9\\n1\\n9\\n9\\n4\\n9 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne\\n_\\nE\\nm\\nai\\nl_\\nG\\nIF \\n1\\n_\\n1.\\n3\\n0.\\n2\\n4                                1\\n4\\n3\\n6\\n9      P\\ner\\np\\na\\ny       T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK ht\\ntp\\ns:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          []                                                                                                                                                      Y\\no\\nur \\n$\\n1,\\n0\\n0\\n0 \\ns\\np\\ne\\nn\\ndi\\nn\\ng \\nli\\nm\\nit \\nis \\nw\\nai\\nti\\nn\\ng \\nfo\\nr \\ny\\no\\nu, \\nN\\na\\nm\\ne*                                             H\\nT\\nM\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0.\\n0\\n0                                                     tr\\nu\\ne              fa\\nls\\ne       fa\\nls\\ne         fa\\nls\\ne               fa\\nls\\ne    E\\nN\\nG\\nLI\\nS\\nH                      2\\n0\\n2\\n3-\\n1\\n2-\\n2\\n9\\nT\\n1\\n3:\\n1\\n1:\\n3\\n9-\\n0\\n5:\\n0\\n0                              ['\\nC\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne'\\n]             []                                          fa\\nls\",\n",
       "  \"402\\nCard Account Balance Daily Ledger\\nData Dictionary\\naccount_id Integer The associated account ID for the card \\naccount \\nborrower_id Integer The associated borrower ID for the card \\naccount \\nuser_id Integer The associated user ID for the card \\naccount \\ncard_account_id Integer The engineering ID of the card account \\nbeing observed \\ncard_account_uuid VarcharThe Deserve ID of the card account being \\nobserved \\nemail VarcharThe associated email for the card account \\nsrc_dt Date The observation date in EST  \\nobservation_day Integer The total days of observation for the card \\naccount at the src_dt \\nstarting_balance Numeric(\\n38,2)The starting balance on the observation \\ndate. On the first day of observation, it is \\n0. On every other day it is derived from \\nthe previous day's ending balanceCalculating \\nDaily \\nBalances\\ntransactions Numeric(\\n38,2)Sum of transactions of type REGULAR for \\nthe card account on the observation dateTransaction \\nTracking\\naccount_opening_fee Numeric\",\n",
       "  \"508\\nthis aligns with our borrower_credits tracking for the examples above\\nResponse: If it is not a big lift, can you share the total number of occurrences and share the most recent ones (last 2-3 should work)? No \\nrush though, for tomorrow.\\nFollow-up: Total number of occurrences is 64, with the two most recent being\\nvar_5_recent_ex_all.csv \\nI also took out the ones that have additional factors (i.e. payments) in case you want to look at those, where this total comes to 52 and the \\ntwo most recent are\\nvar_5_recent_ex_credits_only.csv \\nResponse: n your list for the two most recent of 64 occurrences, I'm seeing that loan 4997554 shows an ending balance of 729.98 \\n(payment_loanprincipalbalancehistory row 18015222). Similarly, I see that Loan  4980990 has an ending principal balance of \\n325.40.I've looked a bit into loan 4860890 and don't yet understand the two principal balance reductions of $10.81 on 6/15. Will keep \\ndigging after lunch.\",\n",
       "  '614\\nGoogle/Facebook\\nwhat are audiences? how do we build them?',\n",
       "  \"449\\nFile status_history (plus borrower dm) ✅\\nPlus Funnel Fact\\nAlso used for pp+ purposes! Limits the population based on pp+ enrollments, but cards in the wild shouldn’t affect this.\\n11           min(e.created) as first_canceled_ts,\\n12           max(case when e.status is not null then 1 else 0 end) as first_canceled_ind\\n13    from {{report_schema}}.plus_borrower_fact as a\\n14    left join (\\n15            select b.*\\n16            from {{pub_schema}}.feature_history as b\\n17            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\\n18            left join {{pub_schema}}.feature as c on c.id = d.feature_id\\n19            where c.type in ('perpay_plus', 'perpay_plus_v2')\\n20            and b.status = 'enabled'\\n21        ) as c on c.borrower_id = a.borrower_id\\n22    left join (\\n23            select b.*\\n24            from {{pub_schema}}.feature_history as b\\n25            left join {{pub_schema}}.feature_enrollment as d on d.id = b.feature_enrollment_id\",\n",
       "  '105\\nDoing these types of base transformations at the staging layer (and the staging layer only) serves as a jumping-off point for our \\nheavier transformation layers downstream.\\nIf anything ever changes in the source data, we have a layer of defense, and can be confident that if we fix the staging layer, our \\nchanges will flow into downstream models without manual intervention.\\nCurrently, our metric layer organizes data based on business logic, and then we validate the data. With this approach, we can protect the \\nmetric layer from unexpected changes, and only use tests to validate for changes that may come up form the joins and aggregations from \\nthe metric layer. \\nThis approach also makes it easier to find the root cause of a fail. While DMS is constantly updates, a source layer would run on a schedule, \\nmaking it clear what run caused the fail. With errors occurring in the source layer, decreasing the risk of errors or disruptions in downstream',\n",
       "  \"359\\n \\nOne of the most frequently triggered Amplitude events is the `Loaded a Page` event. This event is currently not referenced anywhere \\nin our codebase. The reason why this event is triggered is thus:\\nAmplitude currently ingests Segment events in our setup, and in the Segment dashboard, the 'Track All Pages to Amplitude' \\noption is currently enabled (see picture below). As a result, whenever a user views a page, Segment sends a ‘page_viewed: \\n<url>’ event along with a ‘Loaded a page' event to Amplitude. Segment treats these two events the same, so these events are \\ndouble-counted in Amplitude. We can thus get rid of the `Loaded a Page` event. \\nRegarding the mechanics of removing an event from Amplitude, there are three ways to remove an event from Amplitude (“hiding”, \\n“blocking” and “deleting”). It is recommended that we block events that we don’t need. \\nThe ‘T rack All Pages to Amplitude’  option in the Segment dashboard\",\n",
       "  '388\\nThis differs from the engineering logic which instead decrements the balance at the first sign of a withdrawal, and then only later restores the \\nbalance if the withdrawal were to fail.\\nRefund Tracking\\nThere is no difference in how engineering and accounting define how a marketplace refund is returned as cash balance. Therefore, the \\nrefund tracking from the engineering tie-out process is reused for the accounting rollforward reporting. The construction of refund tracking for \\nthe engineering tie-out process is outlined here.\\nCard Balance Refund Tracking\\nThere is also no difference in how engineering and accounting define negative card balance being transferred to the user’s core balance, so \\nthe engineering tie-out card balance refund tracking is reused. The documentation for card balance refund tracking is located here.\\nDeposit Tracking\\nCompleted ACH Deposits: A completed ACH deposit is any row in the platform_deposits metric layer with a source of ach and a',\n",
       "  '1subl docker exec -it perpay-airflow_worker_default bash\\n1python3 -m analytics.credit_reporting.lib.scripts.script_experian_pull_monthly_vantages -p -s\\n1dags/analytics/credit_reporting/lib/untracked\\n1aws s3 cp experian_vantage_pull_borrowers.csv s3://ana-prod-third-party-data-rukoa',\n",
       "  'example, it is clear what int_payments_pivoted_to_orders does. \\nData models in this layer are built with specific purposes to be usable in a downstream layer. \\nRather than combining 10 tables in the next layer, bringing together about 4 or 5 concepts into 2 tables each in the intermediate layer \\nsimplifies the join in the next layer\\nThese tables are not user-facing\\nDBT’s Mart\\nThis is DBT’s docs on it. It is most similar to our reporting layer.\\nFolders are organized by department/concept\\nFiles are named to be intuitive to intended users\\nThe data models in this layer are built thoughtfully driven by business context',\n",
       "  '192\\nDomo Card Permissions (domo_gov_card_permissions):\\nDomo Certified Content (domo_gov_certified_content_hist):\\nDomo content (cards/datasets) can be marked as “certified” by members of Perpay. This dataset tracks which Domo assets have been \\nofficially certified.execution completed\\nnum_rows_updated integer The no. of rows updated/inserted \\nduring this execution\\nstatus varchar Usually \\'SUCCESS\\'\\nphase type varchar A string containing a comma-\\nseparated list of the phases of the \\nexecution that occurred \\neg. \"INIT,QUEUE,IMPORT\" \\nindicates the execution occurred \\nduring the import phase\\nThe full list of possible phases are:\\nINIT\\nQUEUE\\nIMPORT\\nDATASTORE\\nINDEX\\nFINAL\\n(note: not all phases may be \\npresent in a particular execution)\\ndomo_card_id integer ID of the Domo Card \\nobservation_ts timestamp     the timestamp at which the \\nsnapshot was taken\\nentity_type string “user” or “group”\\nentity_id integer user_id or group_id\\nread_ind integer 1 if the entity has a read \\npermission, 0 otherwise',\n",
       "  \"296\\n2\\n0\\n9\\n7\\n0 M\\ny\\nC\\nre\\ndi\\nt\\nN\\no\\nw\\n8\\n5\\n0\\n_\\nQ\\n1\\n2\\n0\\n2\\n4          3\\n6\\n9      p\\na\\ny       X\\nT\\n_\\nLI\\nN\\nK s:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          o\\np \\nn\\no\\nw, \\np\\na\\ny \\nla\\nte\\nr, \\nb\\nui\\nld \\ncr\\ne\\ndi\\nt                     M\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0                 e              e       e         e               e    G\\nLI\\nS\\nH                      2\\n4-\\n0\\n1-\\n0\\n2\\nT\\n1\\n7:\\n2\\n4:\\n2\\n5-\\n0\\n5:\\n0\\n0  B\\ne\\ngi\\nn\\nni\\nn\\ng\\ns, \\nL\\nL\\nC'\\n]          e             \\n1\\n9\\n2\\n0\\n8\\n1\\n7 P\\nu\\ns\\nh\\nn\\na\\nm\\ni_\\nQ\\n4\\n2\\n0\\n2\\n4                                                  1\\n4\\n3\\n6\\n9      P\\ner\\np\\na\\ny       T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK ht\\ntp\\ns:\\n//\\na\\np\\np.\\np\\ner\\np\\na\\ny.\\nc\\no\\nm\\n/s\\nig\\nn\\n_\\nu\\np          []                                                                                                                                                      $\\n1,\\n0\\n0\\n0 \\nto \\nS\\nh\\no\\np \\nN\\no\\nw, \\nP\\na\\ny \\nL\\nat\\ner                                                                    H\\nT\\nM\\nL\\n_\\nA\\nN\\nD\\n_I\\nF\\nR\\nA\\nM\\nE     0.\\n0\",\n",
       "  'the created timestamp. (NOTE: There are instances of user merges where a payment is attributed to a lead user, but the associated \\nborrower credit for the payment is attributed to a different user in the merge cluster. This is handled later on in the aggregation logic.)\\nCompleted Perpay Plus Payments: Completed Perpay Plus payments are sourced from rows in the feature_payment table with a \\nfeature_id that has a type of either perpay_plus or perpay_plus_v2. The amount is considered to be a Perpay Plus payment for the \\nassociated borrower_id on the date of the created timestamp.\\nCalculating Daily Balances\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance. \\nOn the first day of observation, there needs to be some sort of starting point with which to calculate from. These initial starting balances are',\n",
       "  '565\\nInterviews- Questions\\nFull-time\\nRemote challenge \\nproblem set\\nOn-site interview\\nSlides can be found here\\nWhiteboard questions\\nBuild the Reddit/twitter/Facebook/ect data base architecture\\nCreate a function that reverses the key value pair\\nData Model questions\\nFamiliarity with data model structures?\\nStar/snowflake schemas? When to use?\\nHow wide should a table be?\\nAt Perpay, we track users/borrowers/marketplace/core - what kind of datamodels would be appropriate?\\nHow do you ensure that the data is accurate?\\nValidation techniques\\nWhat is better, known incorrect values in tables or stale data?\\nWhat levels of proficiency do you require from analysts as they add new tables\\nHave you worked with 3rd party marketing platforms before?\\nHave you dealt with rate limits?\\nHow would you make data stored in S3 available to analytics who mainly work in SQL editors\\nWhat are some mythologies to writing clean and efficient SQL code?\\nWhat are some considerations for data governance and permissions?',\n",
       "  \"13                    left join {{pub_schema}}.feature as b on b.id = a.feature_id\\n14        where b.type = 'perpay_plus') as f on f.borrower_id = c.borrower_id\\n15    left join {{pub_schema}}.user as e on e.uuid = a.user_id\\n16    where (a.event_type like 'Perpay_Plus.%' or a.event_type like 'Perpay_Plus_Top_Of_Funnel.%')\\n17    and coalesce(b.user_id, e.id) is not null\\n18    group by coalesce(b.user_id, e.id), d.first_plus_eligible_dt, f.status;\\n19...\\n1...\\n2create table {{temp_schema}}.user_attribute_3 as\\n3  select distinct\\n4    a.*,\\n5    case\\n6      when a.borrower_status in ('B1','B2','B3','B4','ChargedOff') then 1\\n7      when a.blacklisted_ind in (1) then 1\\n8      when a.bankrupt_ind in (1) then 1\\n9      when b.payment_plan in ('Y') then 1\\n10      when b.risk_based_denial_ind = 1 then 1\\n11      when c.is_active = 'false' then 1\\n12      else 0 end as standard_exclusion_ind,\\n13    f.status as plus_status,\\n14    e.enabled_ts as plus_enabled_ts\\n15from {{temp_schema}}.user_attribute a\",\n",
       "  'required resources even when there is no connection using them. This consumes storage because neither required WAL nor \\nrequired rows from the system catalogs can be removed by VACUUM as long as they are required by a replication slot. In extreme \\ncases this could cause the database to shut down to prevent transaction ID wraparound (see Section 24.1.5). So if a slot is no \\nlonger required it should be dropped.',\n",
       "  '300\\nW\\n8\\ny\\nT\\nN\\n3\\nR\\nc\\na\\nx\\ny\\nN\\nTI\\n6\\nX\\nF\\nE\\nX\\nR\\nZ\\nV\\nq\\nO\\nU\\nk\\nA\\nU\\nb\\nK\\nU\\nB\\nM\\nU\\ntn\\nw\\nE\\n0 1\\n4\\n3\\n6\\n9 P\\ner\\np\\na\\ny a\\ne\\n0\\n7\\nb\\nb\\n4\\n6-\\n9\\nc\\n3\\n7-\\n1\\n1\\ne\\nd-\\nb\\nb\\n3f\\n-\\ne\\n7\\n5\\n8\\ne\\n2\\n7\\n1\\nd\\n9\\ne\\na A\\nA\\nA\\nA\\nD\\nI\\nH\\nm\\nV\\nF\\n3\\nD\\nY\\nG\\nc\\nQ\\nV\\np\\ng\\nZ\\nG\\n0\\nm\\nG\\nK\\n7\\nT\\nz\\nb\\nP/\\nI\\nC\\nD\\na\\n8\\nJt\\ne\\nF\\nK\\nd\\ny\\nb\\nw\\n1\\n3\\nc\\n+\\ng\\nF\\nQ\\nV\\nq\\nu\\nW\\nD\\nv\\nH\\nD 2\\n0\\n2\\n3-\\n0\\n3-\\n0\\n7\\nT\\n0\\n2:\\n0\\n6:\\n4\\n6-\\n0\\n5\\n0\\n0 1\\n1\\n9\\n4\\n0\\n5\\n2 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne 1\\n5\\n9\\n1\\n1\\n4\\n3 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne\\n_\\nE\\nm\\nai\\nl_\\nG\\nIF \\n1\\n_\\n2.\\n1\\n8.\\n2\\n3 T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK     Af\\nfili\\nat\\ne/\\nC\\nP\\nA \\nN\\net\\nw\\nor\\nk     C\\na\\nnt\\no\\nn\\nm\\ne\\nnt Fl\\nor\\nid\\na U\\nS P\\nH\\nO\\nN\\nE iP\\nh\\no\\nn\\ne S\\naf\\nar\\ni i\\nO\\nS Li\\nn\\nk ht\\ntp\\n://\\nw\\nw\\nw.\\ncr\\ne\\ndit\\nse\\nsa\\nm\\ne.\\nco\\nm  Li\\nn\\nk    tg\\na\\nk\\nqj\\ny\\nh\\n7\\ng\\nv\\nw\\n0\\nw\\nq\\nw\\n5j\\n7\\n6      0.\\n0 T\\nu\\ne\\nW\\nX\\nY\\nx\\nw\\np\\nT\\nef\\nx\\ny\\nN\\nR\\nM\\nq\\ny\\no\\n9\\nU\\nV\\n31\\n4\\n3\\n6\\n9 P\\ner\\np\\na\\ny a\\n3\\nb\\n4\\n1\\n1\\n2\\n1-\\nb\\nc\\nb\\n6-\\n1\\n1\\ne\\nd-\\n9\\na\\n2\\n7-A\\nA\\nA\\nA\\nD\\nE\\nO\\nC\\nr\\nM\\nR\\nW\\nb\\nK\\nF\\nR\\n9\\nY\\nw\\na2\\n0\\n2\\n3-\\n0\\n3-\\n0\\n7\\nT\\n0\\n2:\\n0\\n6:\\n5\\n3-\\n0\\n5\\n0\\n0 1\\n1\\n9\\n4\\n0\\n5\\n2 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne 1\\n5\\n9\\n1\\n1\\n4\\n3 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne\\n_\\nE\\nm\\nai\\nl_\\nG\\nIF \\n1\\n_\\n2.T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK     Af\\nfili\\nat\\ne/\\nC',\n",
       "  'reported with monthly values, which will include data from the last completed month.\\nData General Specifications:\\n1. The Perpay portfolio will be regarded as a “revolving” portfolio. This portfolio type is similar to a credit card, in that customers who have \\nborrowed with us and have a spending limit but are not actively using the platform are being reported as using 0% of their credit limit; \\ntherefore, even inactive borrowers are benefiting week over week. Experian defines this portfolio type as:\\n2. Once a borrower is reported to Experian, they must be continuously reported week over week; we cannot stop sending their data.\\n3. Every week, borrower data is sent as of end of the last week. One of the fields (payment history profile) includes borrower history for the \\npast 24 months. So, any single record for a borrower sent to Experian will include data over the past 24 months.\\nMetro 2 Formatting and Compliance Detail:',\n",
       "  '404\\nTransaction Tracking\\nData Sources\\n1. All transactions in the card_transaction metric layer with a status of SETTLED and a current_per_status_ind of 1.\\n2. All transactions in the deserve_daily_posted_transactions_report built from the Deserve SFTP data that either do not exist or are \\nnot settled in the card_transaction metric layer.\\n3. All transactions in the deserve_daily_settled_transactions_report built from the Deserve SFTP data that either that are pending \\nwith a non-null expiry date in the webhook data that were transacted before 2023-02-28. This accounts for a known issue that previously \\nexisted with pre-auth transactions settling and not being reported as settled.\\nData Nuances\\naccount_opening_fee: There is a $9 fee assigned to new accounts that are opened, with the exception of staff card accounts.38,2) certain merchants Tracking\\nrefund Numeric(\\n38,2)Sum of transactions of type REFUND for \\nthe card account on the observation dateTransaction \\nTracking',\n",
       "  \"Step 8: There is lots of room for improvement here. \\n1create external schema segment_logs\\n2from data catalog\\n3database 'segment-logs'\\n4iam_role 'arn:aws:iam::606618693306:role/ana-stage-redshift-role'; -- insert prod role for production environment\",\n",
       "  'The issue we experienced was this: We would set the version to 3.4.7 and the terraform plan would look as expected. But after terraform \\napply, we kept getting instances that were at version 3.5.1.\\nIn the background, we had left a hardcoded true value for the auto_minor_version_upgrade. It took us a little while to notice this as \\nour attention was stuck on the need to add the allow_major_version_upgrade parameter to our Terraform repo.\\nGeneral Notes\\nDMS does not allow downgrading versions. This means that if there is ever a need to do this, we have to first destroy all DMS resources \\nand build them up from scratch. Modifying in place does not work.Replication slots persist across crashes and know nothing about the state of their consumer(s). They will prevent removal of \\nrequired resources even when there is no connection using them. This consumes storage because neither required WAL nor',\n",
       "  '426\\nPinwheel Investigation\\nBackground\\nPinwheel is a third party integration that allows users to setup payroll direct deposit. The Pinwheel experience is intended to increase new \\nborrower conversion by easing the friction of setting up a payroll deduction. \\nA company is defined as being eligible for Pinwheel if either the company or the payroll provider of the company is on Pinwheel’s company \\nand payroll provider’s eligibility list. In addition, we began directing users from all unverified companies to Pinwheel on 2021-03-27. \\nData Held in Public Tables\\nPinwheel eligibility data is held in two public tables, waffle_flag and waffle_flag_history. The waffle_flag table is intended to hold \\nthe complete current status of the Pinwheel eligibility of both companies and payroll providers. The waffle_flag_history table is intended \\nto hold the complete transactional history of the Pinwheel eligibility status of companies and payroll providers changing over time. Each time',\n",
       "  \"the bottom here, more on this later.\\nb. Sqlfluff: Executable Path : Check this to see if it's the same as the result of your which sqlfluff  command.\\n6. Now, you're ready to test things out. Open up one of Andrew's new .sql files in the metric layer and open the Command Palette and \\nsearch for Format Document. See what that does.\\n7. If you are a vim fan, then you can use the command line directly. Give this a try: sqlfluff format <file name>.sql --dialect \\nredshift --ignore templating .\\nConfig File\\nPlace the following in your .sqlfluff file as mentioned above:\\n1[sqlfluff]\\n2large_file_skip_byte_limit = 400000\\n3ignore = templating\\n4exclude_rules = structure.column_order\\n5\\n6[sqlfluff:indentation]\\n7# See https://docs.sqlfluff.com/en/stable/layout.html#configuring-indent-locations\\n8indent_unit = space\\n9tab_space_size = 4\\n10indented_joins = False\\n11indented_ctes = False\\n12indented_using_on = True\\n13indented_on_contents = True\\n14indented_then = True\\n15indented_then_contents = True\",\n",
       "  '399\\nRepayment Date Consolidation\\nA loan is considered to be an active receivable only on the day that the loan enters repayment. For this reason loan tracking starts in this \\naccounting version of the rollforward table on the first day that a loan hits a status of repayment in the loanstatus table. Any balance \\nmovement prior to this day is consolidated to the date that the loan first enters repayment.\\nIn these cases, the calculated starting balance on the new first day of observation will still use the observation date originally used in the \\nloan tracking for engineering tie-out described here.',\n",
       "  \"49\\nDataX Reporting Service\\nOverview: \\nThis document outlines the daily reporting logic for uploading to the third-party provider DataX\\nPurpose:\\n1. We use the DataX third party data to give us another channel to glean insight into our (first time) borrowers\\n2. DataX requires that reports are sent back detailing the payment performance of these users we pulled data\\n3. Currently we send only two tables:\\nupdates table\\nWhen a user becomes a FTB, a row must be added showing they have entered repayment so that they can be tracked within \\nDataX's system.\\nWhen a user completes/pays off their loan, we report a paid-off status so they know that the account is now closed and will no \\nlonger track\\npayments table\\nWhen a user makes a payment to their loan, it gets reported here (as well as with the method, ie direct deposit/card)\\n4. They request that these tables have no index or column headers and while columns can be blank, they strictly require the correct\",\n",
       "  '165\\npayment_id)\\n WHERE\\nunique_coun\\nt <> \\nid_count;`\\n2024-\\n04-12http\\ns://gith\\nub.co\\nm/Perp\\nay/per\\npay-air\\nflow/pu\\nll/2553\\n 2024-04-\\n11Withholding \\nusers with two \\nauth groups for \\ncard enablementAllow card reporting \\nto updateUniqueness https://githu\\nb.com/Perpay/\\nperpay-airflow/\\npull/2550\\n  Engin\\neering\\nRESTRICT\\nRESTRICTED CONTE',\n",
       "  'Analysts are required to leverage docker for contributing to the datamart\\nLarge lift for testing can cause testing to be circumvented\\nDifficult to manage more than one contributor in staging at a time\\nThe development environment is actually production?!?\\nIt’s really cumbersome/annoying to edit/drop/create external tables, and also requires admin level permissions. The utils_spectrum dag \\ncan be improved to make this much easier.\\nBoth staging and production are set to track the master branch. This does not allow a feature to run in staging before merging to \\nproduction without a workaround.\\nCurrent Airflow permissions flow requires team members to have full access or none. This makes it difficult for other teams to contribute \\nto the main Airflow repository and maintain ownership of those processes.\\nMake rendering easier for users\\nSoft errors, ex: duplicate user ids, not necessarily end the whole dag organizer job, just not update the users with issues',\n",
       "  'dags/analytics/data_model_payment_plans/lib/performance.py\\ndags/analytics/data_model_hist/company_hist_daily_data_model/attribute.py\\n1dags/analytics/data_model_borrower_hist_daily/lib/risk_tier.py\\n265:    left join {{pub_schema }}.payment b on a.borrower_id = b.borrower_id and\\n3\\n4dags/analytics/data_model_full/borrower_data_model/attribute.py\\n5160:left join {{pub_schema }}.payment b on a.borrower_id = b.borrower_id ;\\n6\\n7dags/analytics/data_model_hist/borrower_hist_monthly_data_model/attribute.py\\n8116:left join {{pub_schema }}.payment b on a.borrower_id = b.borrower_id\\n9\\n10dags/analytics/data_model_accounting/sales_tax/sales_tax_daily.py\\n11380:  from {{pub_schema }}.payment a\\n12457:  from {{pub_schema }}.payment a\\n13\\n14dags/analytics/data_model_accounting/account_reconciliation/commerce_loan_daily_rollforward/commerce_loan_daily_\\n15427:            LEFT JOIN {{pub_schema }}.payment b on a.borrower_id = b.borrower_id and a.payment_id = http://b.\\n16',\n",
       "  \"ensures that stakeholders know that when they are looking at a data element, the parent assets have also been documented and are ready \\nfor use.\\nDocumentation Review\\nA beneficial feature of Datahub is the ability to enable and disable data assets' visibility in the UI through search. This allows the Data \\nEngineering team to pull in new data sources without making them searchable for viewing by Datahub users, while still being able to access \\nthe page with the exact URL. Consequently, all new documentation will be created and reviewed in a state that is not accessible to outside \\nstakeholders. Once the new documentation has been reviewed and approved, the new data assets will be enabled for viewing through \\nsearch, and a release message will be created and distributed via the Datahub front page “posts.” Note, documentation review will also \\ninvolve non-DE team members so that SMEs can validate that the documentation is accurate and feature complete.\\nDatahub Tooling\",\n",
       "  \"temp_int.invoicing_level_match, to clean the data more and add a flag to denote whether or not it’s in quickbooks or \\nnot. It does this by creating a column called in_qb, which can have a value of 'Y' or 'N'\",\n",
       "  '20311:                LEFT JOIN {{pub_schema }}.withdrawal_request_status b on a.withdrawal_id = b.withdrawalreques\\n21\\n22dags/analytics/data_model_accounting/account_reconciliation/account_balance.py\\n23357:  from {{pub_schema }}.withdrawal_request wr\\n24358:  left join {{pub_schema }}.withdrawal_request_status wrs on http://wr.id  = wrs.withdrawalrequest_id',\n",
       "  \"479\\nlimit are denied, but this is a pre-authorized type of transaction (which most commonly occurs when the customer is buying gas)It may \\nbe worth looking into if transactions that are posted to balance prior to settlement are typically pre-auth types.\\nFollow-up Q: So while it does not look like every case of the post date and settlement date differing is a pre-auth transaction, in every \\ncase I have looked at it occurred with a transaction that took the borrower over or exactly to their credit limit.So I think you're right \\nConor, Deserve must have different rules for how they post and settle these sorts of transactions. Unless, we can get an actual data \\nsource for the post dates, we will probably have to wait to hear back what these rules are, and then implement them in our logic. \\nFollow-up R: cool, we’ve asked Deserve for the specific logic of when a transaction posts to the balance prior to settlement. I’ll lyk what \\nthey respond.\",\n",
       "  '27\\nCard credit reporting\\nAccount rec\\nAccounts Payable',\n",
       "  'Without config change, ingesting a new schema and table will unhide it from the UI.   Each ingestion is tagged with a \"runId\" which uniquely \\nidentifies each ingestion run. While this hasn\\'t been utilized yet, it\\'s important for enabling stateful ingestion, which is an idea that was kicked \\naround for trying to keep UI state between ingestion runs. The thought here (that needs testing) is that after ingestion, we can do a one-time \\nmanual hiding of all datasets, keeping those we want. From then on, using the \"ignore_new_state\" tag in our ingesting config, it will update \\nsome data without changing the DataHub state of each asset. This would allow us incremental changes by manually unhiding data as it\\'s \\nupdated. We’re unsure which data changes from run to run here, but it should be tested.\\nWhile ingesting datasets, DataHub creates these concrete \"data units\" - called entities - that are identified by Uniform Resource Names',\n",
       "  '269\\nIf this happens while you are on rotation, it would be good to get back to them before the end of the day. If one of these questions comes in \\nwhile you’re not on rotation feel free to direct the question to the DE that is.\\nDig Into Metrics Report\\nTo pull the monthly Metrics Report:\\n1. Log into Experian SFTP using edma credentials\\n2. Navigate to the from_xpn folder\\n3. Download the most recent file (file is produced monthly on the 14th\\n4. Navigate to the “Fatal Errors Details” section\\n5. In a new tab of the Metrics  Report Tracker, add the new consumer account numbers and the fatal error reasons\\nDE can ignore any row containing the  Reported Status Code does not match with the e-OSCAR®/ACDV correction reason. For \\nthese we will need to submit an AUD. Any other row should be investigated just as how any other holdout would be.',\n",
       "  '595\\nFocusing on the Deserve SFTP information for this user (the first query):\\nUser was provisioned on 07/11\\nNormally, the first entry in the deserve accounts eod report for this user would be the day that the user was provisioned. However, the \\naccounts eod report was only generated after 7/14. As a result, this user’s first entry in the accounts eod report is 7/14.\\nThe user already has an outstanding balance of $9.00, which is the account opening fee.\\nThis does not show up under the “fees_accrued” field, because this shows up as a transaction. You can run the following query to \\nsee it:\\nOn 11/17, Conor and Deserve discussed this discrepancy (that the account opening fee is not being considered a “fee”, but rather a \\n“transaction”. \\nConor said he would keep DE in the loop about the outcome of this conversation.\\nIn card borrower hist daily, we use logic to consider this transaction a fee, not a transaction.',\n",
       "  'commensurately.\\n2. borrower_id = 4251822 / card_account_id = 14777 / card_payment_id = 250680\\ndeserve_payment_250680.csv \\ndeserve_account_balance_14777.csv \\nVariance 10: Missing International Transaction Fees\\nAlready resolved- won’t attach examples',\n",
       "  'based testing) \\nThe current organization allows for data mart tables (ex user_attribute) to be available to anyone who has access to the particular datamart. \\nThese users are outside of the data engineering group and therefore can mistakenly use incorrect data which could force unwanted \\noutcomes. The updated format would pull our testing layer back and therefore anything that would be added to the datamarts would also be \\nabsolutely validated against our testing suite. Additionally, as with the metric layer testing, could be logged and removed so the entire \\nprocess continue and the issues could be triaged asynchronouslyRobot test cases are easily \\nreadable\\nrich built in libraries make it \\neasy to useinadequate in parallel \\ntesting (no built in feature) It is suggested to use a different \\noption if you want to develop a \\ncomplex automation framework \\nbecause other options involve \\nPython code\\nPytest open source, easy to learn\\ntesting framework in Python\\nwrite test suites in a',\n",
       "  'repo.\\nTo see the changes to the modules repo, compare the following:\\n1. Old Glue crawler module for datamarts and old Glue crawler module for datamarts and for individual tables,\\n2. New Glue module.\\nThe modules above are called from the main perpay-terraform repo, see following update:\\n1. Old Glue crawler setup,\\n2. New Glue crawler setup.\\nRecommended Reading\\n1. Customizing crawler behavior - AWS Glue \\n2. Prevent AWS Glue crawler from creating multiple tables',\n",
       "  '(4) Calls upload_bills which calls upload_bills from dags/analytics/accounts_payable/lib/docparser_api.py.\\n1: Calls get_bills_to_pay which gets the data from temp_int.bills_to_pay created in \\ndags/analytics/accounts_payable/queries/bills_to_pay.py and returns it as a df. These are the invoices that have not already been \\npaid and are also not in the variance sheet yet. \\n2: Calls format_bills_to_batches from the Quickbooks API (dags/analytics/accounts_payable/lib/quickbooks_api/quickbooks_api.py) \\nto format those invoices that haven’t been paid to work with the API\\n3: Upload them to Quickbooks via add_bill_batches_to_qb from \\ndags/analytics/accounts_payable/lib/quickbooks_api/quickbooks_api.py',\n",
       "  '403\\non the observation date\\nlate_payment_fee_adjustment Numeric(\\n38,2)Sum of transactions of type \\nLATE_PAYMENT_FEE_ADJUSTMENT for the \\ncard account on the observation dateTransaction \\nTracking\\nshipping_fee Numeric(\\n38,2)Sum of transactions of type \\nOVERNIGHT_SHIPPING_FEE for the card \\naccount on the observation dateTransaction \\nTracking\\ndebit_card_processing_fee_adju\\nstmentNumeric(\\n38,2)Sum of transactions of type \\nDEBIT_CARD_PAYMENT_PROCESSING_FEE_A\\nDJUSTMENT for the card account on the \\nobservation dateTransaction \\nTracking\\ninternational_transaction_fee Numeric(\\n38,2)Sum of transactions of type \\nINTERNATIONAL_TRANSACTION_FEE for the \\ncard account on the observation dateTransaction \\nTracking\\ninternational_transaction_fee_\\nadjustmentNumeric(\\n38,2)Sum of transactions of type \\nINTERNATIONAL_TRANSACTION_FEE_ADJUS\\nTMENT for the card account on the \\nobservation dateTransaction \\nTracking\\nreturned_payment_fee Numeric(\\n38,2)Sum of transactions of type \\nRETURNED_PAYMENT_FEE for the card',\n",
       "  'status_history (plus borrower dm) ✅\\nfact (plus funnel dm) ✅\\nattribute (user dm) ✅\\nuser_fact (risk dm) ❌\\nadhoc_card_data_update_query ✅\\ngenerate_data (iterable send) ❌\\ncard_activation_events ❌\\ncard_key_relations ❌\\ncard_required_direct_deposit_config ❌\\nmonthly_vantage_scores ✅ *\\nnew_test_monthly_vantage_scores ✅\\ntest_monthly_vantage_scores ✅\\nplatform_payments ✅\\ntest_core_errors ✅\\nCredit Reporting\\nMore context can be found here, but as a summary: the checks and Xs in this list are linked to their corresponding section to help the reader navigate.',\n",
       "  '282\\nAccount Reconciliation & Flow of Funds\\nTable of Contents\\nTable of Contents\\nSummary\\nVisual Representation\\nDOMO Cards\\nCard Report Definitions & Nuances\\nDaily Equation\\nNuances\\nStarting Balance\\nTransactions\\nAccount Opening Fee\\nMonthly Fee\\nLate Payment Fee\\nInternational Transaction Fees\\nDisputes Lost or Withdrawn\\nReturned Payments\\nPayments Completed Before 2023-05-08\\nInitiated Payments After 2023-05-08\\nEnding Balance\\nCommerce Report Definitions & Nuances\\nDaily Equation\\nNuances\\nStarting Balance\\nEnding Balance\\nPayments\\nReturned Payments\\nPartially Returned Payments\\nPayments Records over Multiple Days\\nRefunds\\nCore Report Definitions & Nuances\\nDaily Equation\\nNuances\\nAdditional Resources\\nSummary\\nThe account reconciliation report is produced by the DE department in an attempt to track the flow of customer funds from the source (as \\ninitiated by the customer) all the way through to application against various products and receivables balances. This report seeks to track',\n",
       "  '418',\n",
       "  \"82       original_charge_off_amount,\\n83       date_last_payment,\\n84       scheduled_monthly_payment_amount,\\n85       special_comment\\n86from perpay_risk_datamart.equifax_raw_data as a\\n87left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n88left join (\\n89    select * from public.forgiveness_period\\n90    where type != 'disaster_relief'\\n91    and cast(created as date) >= '2020-12-01'\\n92    ) as c on c.borrower_id = a.consumer_account_number\\n93where c.borrower_id is not null\\n94and enter_date = max_enter_date;\",\n",
       "  '\"true\"}. This should be done every so often, say, every ~30ish days.\\nThis turns the accounts_payable_int.qb_history table into a dataframe. This is called the historical_bills_df.\\nIt submits a GET request via the Quickbooks API (quickbooks_api_class.py) on Bills and turns the response into a dataframe. This is \\ncalled the qb_bills_df.\\nThe contents of qb_bills_df are “subtracted“ out of the historical_bills_df using Python joins. The remaining contents of \\nhistorical_bills_df after “subtraction“ has occurred are the bills that exist in historical_bills_df but not in qb_bills_df.\\nWe’re hoping that the number of historical_bills_df that exist at the end is 0, because that means that every bill in qb_history \\nhas been uploaded to Quickbooks at some point. If this isn’t true, this means bills aren’t being uploaded completely/some were \\nmissed in the upload process. This should be double checked by doing a search in QB and manually uploaded if it’s true (you can',\n",
       "  '378\\nbusiness logic on top of it to match the definitions of each of the balance moving fields to accounting’s standards. The accuracy of this table \\ndata is guaranteed by the tie-out process that occurs with the engineering tie-out table. These are the output tables that are used by \\naccounting and for any reporting relating to account reconciliation. The documentation for the accounting versions of the rollforward tables is \\ndesigned to contain the same technical detail that the documentation for the engineering table has, but it is simplified to use abstractions \\nthat will be more familiar to DRAAFT team members (references to metric layers rather than public tables). In addition, the documentation \\ncontains some detail about the engineering’s definitions of fields, accounting’s definitions of fields, and how they differ.',\n",
       "  \"red.\\nmarked as Awaiting Decision: it will be kept in the sheet with this status on the next job run. This means that you're discussing w \\nvendors. Row turns yellow.\\nmarked as Not Addressed: default status for new records. If the next job runs and there were old records under this status, they will be \\nkept on the sheet. Row is uncolored.\\nAnother field that may be changed by accounting is the chosen_payment_option field. This gives them liberty to decide whether the \\ninvoice prices or PO (ecommerce) prices should be reflected in QB. Again, this is a drop town switch to prevent as many manual input errors \\nas possible when picking an option. When the status of a row changes, the sheet will maintain its update the following run.\\nIf chosen_payment_option is…\\nmarked as Invoice Price: default status for new records. The prices sent to QB (and in the accounts_payable_int.po_matches table) \\nwill be the invoice prices.\",\n",
       "  '2. Integration with AWS\\nDatadog integrates with AWS to provide monitoring and observability for our AWS resources. We aim to observe resources such as EC2 \\ninstances, IAM roles, Lambda functions, and CloudWatch alarms.\\nThere are several different integration options:\\nAutomatic: CloudFormation, Terraform, Control Tower\\nManual: Role Delegation, Access keys (private clouds- doesn’t apply to us)\\nCloudFormation\\nCloudFormation is recommended by Datadog as the “best for getting started“ because it allows for an effective set up of the necessary AWS \\nconfigurations required for integration. We used it to set up Datadog to our AWS staging account. The following CloudFormation attributes \\ncontribute to an efficient integration process: \\n1. It provides an infrastructure-as-code approach: It allows us to define our integration in a declarative template using YAML. By',\n",
       "  '17    card_transaction_id ,\\n18    deserve_id ,\\n19    created_ts ,\\n20    transaction_status ,\\n21    rank () over (partition  by card_transaction_id order by created_ts asc) as rank,\\n22    case\\n23        when created_ts = first_value (created_ts ) over ( partition  by card_transaction_id order by created_ts d\\n24        else 0 end as current_ind\\n25from status_1\\n26where ((transaction_status <> transaction_status_previous ) OR (transaction_status_previous IS NULL));\\n27\\n28\\n29select\\n30    count(*) as cnt,\\n31    card_transaction_id\\n32from users_frank .card_transaction_status\\n33group by 2 having cnt > 2 order by cnt desc;\\n1select *\\n2from public.card_transaction_snapshot\\n3where card_transaction_id = 129165\\n4order by created desc;',\n",
       "  'current production launch schedule. However, it is possible to support more low priority initiatives than many high priority initiatives. DE will \\ntend to reduce multitasking as more high priority tasks are given due to their extreme dependence on time to release. On average, each DE \\nhas around 3 to 4 different tasks in fight at one time. Generally each DE will specifically take on one or two large projects, though \\ncollaboration is often. It is important though that small, lower priority projects/and task are given time. These can take the form of new \\ninitiatives or POCs for advancing capabilities and help drive innovation.',\n",
       "  '269\\nIf this happens while you are on rotation, it would be good to get back to them before the end of the day. If one of these questions comes in \\nwhile you’re not on rotation feel free to direct the question to the DE that is.\\nDig Into Metrics Report\\nTo pull the monthly Metrics Report:\\n1. Log into Experian SFTP using edma credentials\\n2. Navigate to the from_xpn folder\\n3. Download the most recent file (file is produced monthly on the 14th\\n4. Navigate to the “Fatal Errors Details” section\\n5. In a new tab of the Metrics  Report Tracker, add the new consumer account numbers and the fatal error reasons\\nDE can ignore any row containing the  Reported Status Code does not match with the e-OSCAR®/ACDV correction reason. For \\nthese we will need to submit an AUD. Any other row should be investigated just as how any other holdout would be.',\n",
       "  '270\\nDatahub Deployment\\nPage under construction…\\nPre-Terraform Steps\\nDocker Images and Environment Files\\nThe main repo you will need for this step is here. The README file of the repo goes through each step thoroughly, so do read before a new \\ndeployment and ensure you’ve gone over all requirements before using the command make all. This will be especially important if you are \\nattempting to add new features or bumping the Datahub version.\\nWhy do we use non-standard Docker images in ECR in this way? Two reasons: First, they are non-standard, the images need to be built to \\naccount for the requirements of ECS and our particular deployment approach. Second, in rapid cycles of deployment, if we use standard \\nimages on Dockerhub, we will run into request limits and get throttled. Using ECR avoids this.\\nA great resource you can reference is the Github repository of Datahub, especially their docker-compose files (the linked page is for v0.13.1',\n",
       "  \"98        left join atomic_int.card_activation_events h on c.borrower_id = h.borrower_id and h.event_name = 'activation_deserve' and h.first_event_type_ind = 1\\n99        left join atomic_int.card_activation_events k on c.borrower_id = k.borrower_id and k.event_name = 'provisioning_complete' and k.first_event_type_ind = 1\\n100        left join atomic_int.card_activation_events m on c.borrower_id = m.borrower_id and m.event_name = 'application_started' and m.first_event_type_ind = 1\\n101        left join atomic_int.card_activation_events n on c.borrower_id = n.borrower_id and n.event_name = 'credit_pull_consent' and n.first_event_type_ind = 1\\n102        left join card_int.card_account_fact p on c.borrower_id = p.borrower_id\\n103        left join card_int.card_account_attribute as f on p.card_account_id = f.card_account_id\\n104        left join card_int.card_account_performance j on p.card_account_id = j.card_account_id\",\n",
       "  '606\\nV2 Proposal:',\n",
       "  '133\\n(Error) Company Switch Mid-Run\\nIn the airflow_prod_alerts channel:\\nError explanation\\nUsually this error is an indication that there was a company switch mid run (which occurred from a timing issue). \\nInvestigation\\nUsually look into loan_fact or user_fact first for duplicates; most tables rely on these. \\nIf there is a duplicate, see where it’s data differs across row- e.g. if there was a company switch mid-run the company values will differ \\nacross rows.\\nSolution\\nBecause it was due to timing, it should self resolve if re-ran. Thus, we clear the tasks that rely on public.company in DM full. Be careful when \\nyou clear tasks- you want to clear the correct one (verify the name when you clear it) and clear the actual table build, not just the test that \\nfailed. \\n** something to keep in mind: now that dm full tasks are more independent as of [November 2, 2022], you need to check other data',\n",
       "  'balance from an accounting standpoint and subtract any refunds, returns, disputes, adjustments, etc that would decrement the card \\naccount balance from an accounting standpoint.  \\nRight now, we add(+) to card starting balance…\\nfiller\\nWe subtract(-) from card starting balance…\\nfiller\\nSource Tables:\\n \\nDefinition:\\nThe difference between the system generated ending balance (ending_balance) and the ending balance calculated by the entry \\nabove (ending_balance_calc)\\nBusiness Context:\\nThis is used to determine whether the card account balances tie out to a valid rollforward. If it’s 0, the balances tie out. If it’s <> 0, there \\nis a variance, meaning one of the following:\\n1. DE missed a field/a business case or miscalculated a field. Either the calculation is missing something or is being performed \\nincorrectly\\n2. The logic for a field that goes into this calculation is buggy on Engineering’s side and we’re looking at the downstream effects.',\n",
       "  '301\\ny\\nh\\nP\\nU\\nk\\nA\\nU\\nb\\nK\\nU\\nd\\nM\\nU\\ntn\\nw\\nE\\n0 7\\n9\\n3\\nd\\n8\\n3\\n4\\n6\\n7\\n4\\n6\\ne R\\n7\\nU\\ne\\nm\\n/v\\n3\\nx\\nm\\n3\\nq\\nc\\nm\\nQ\\nm\\n3t\\nP\\nU\\nC\\nx\\n5\\na\\nN\\nq\\nb\\nc\\nqI\\nU\\np\\nA\\nP\\ns\\nP\\nT\\nC\\nPi 1\\n8.\\n2\\n3 \\nW\\n8\\ny\\nT\\nN\\n3\\nR\\nc\\na\\nx\\ny\\nN\\nTI\\n6\\nX\\nF\\nE\\nX\\nR\\nZ\\nV\\nq\\nO\\nU\\nk\\nA\\nU\\nb\\nK\\nU\\nF\\nM\\nU\\ntn\\nw\\nE\\n0 1\\n4\\n3\\n6\\n9 P\\ner\\np\\na\\ny a\\ne\\n0\\n7\\nb\\nb\\n4\\n6-\\n9\\nc\\n3\\n7-\\n1\\n1\\ne\\nd-\\nb\\nb\\n3f\\n-\\ne\\n7\\n5\\n8\\ne\\n2\\n7\\n1\\nd\\n9\\ne\\na A\\nA\\nA\\nA\\nD\\nK\\nu\\n9I\\np\\nJ\\n0j\\n8\\np\\nL\\n5\\nn\\nq\\n5\\nn\\ne\\nz\\nP\\ng\\nAI\\nX\\nc\\nQ\\nT\\nn\\ny\\ny\\nw\\nV\\nd\\ng\\nb\\n4\\n6\\nH\\ndI\\nU\\nw2\\n0\\n2\\n3-\\n0\\n3-\\n0\\n7\\nT\\n0\\n2:\\n0\\n6:\\n4\\n7-\\n0\\n5\\n0\\n0 1\\n1\\n9\\n4\\n0\\n5\\n2 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne 1\\n5\\n9\\n1\\n1\\n4\\n3 C\\nre\\ndi\\nt \\nS\\ne\\ns\\na\\nm\\ne\\n_\\nE\\nm\\nai\\nl_\\nG\\nIF \\n1\\n_\\n2.\\n1\\n8.\\n2\\n3 T\\nE\\nX\\nT\\n_\\nLI\\nN\\nK     Af\\nfili\\nat\\ne/\\nC\\nP\\nA \\nN\\net\\nw\\nor\\nk     C\\na\\nnt\\no\\nn\\nm\\ne\\nnt Fl\\nor\\nid\\na U\\nS P\\nH\\nO\\nN\\nE iP\\nh\\no\\nn\\ne S\\naf\\nar\\ni i\\nO\\nS Li\\nn\\nk ht\\ntp\\n://\\nw\\nw\\nw.\\ncr\\ne\\ndit\\nse\\nsa\\nm\\ne.\\nco\\nm  Li\\nn\\nk    tg\\na\\nk\\nqj\\ny\\nh\\n7\\ng\\nv\\nw\\n0\\nw\\nq\\nw\\n5j\\n7\\n6      0.\\n0 F\\na\\ns\\ne',\n",
       "  'query.\\nb. Helpful article about the super type & jsons in Redshift Spectrum\\n8. Create a metric layer based on the event that you would like. (Airflow PR). \\na. Note: it is incredibly efficient/fast to query the Segment data, as long as the partition key is specified. Without that being specified, it is \\nslow/costly. The partition key is the day the segment event was sent to us, in Unix time. As a result, instead of rebuilding the data each \\ntime, I queried only the most recent completed day and appended this day to the metric layer table, so that we are being as efficient as \\npossible. \\nNotes for improvements:\\n(I did not do these things to keep the POC simple & the same between production and staging).\\nThis bucket name should be programmatically defined (as an output from creating the bucket). However, you can’t do that, because the \\nbucket is defined in a different module. Option for improvement: create the bucket in the main.tf file in 3-segment, so that you can',\n",
       "  \"Adjusting for Cards in the Wild437\\n                Clarity Score Drift from FTBs459\\n                VARIANCES - Accounts Receivables Reconciliation461\\n                     Card Report Definitions462\\n                     Card Report Variances476\\n                     Core Report Context483\\n                     Core Report Definitions489\\n                     Core Report Variances492\\n                     Commerce Report Definitions496\\n                     Commerce Report Variances502\\n                     Tying out Engineering's Account Balance Flow514\\n                Deposit, Payment, and WithdrawalRequest Updates519\\n                Iterable DAG Revamp523\",\n",
       "  'Engineering’s public tables. \\nRollforward Table Logic:\\n \\nSource Tables:\\n \\nDefinition:\\nA unique id associated with each Perpay user.\\nBusiness Context:\\nAn id for anyone who has signed up for the Perpay platform. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables.\\nRollforward Table Logic:\\n \\nSource Tables:',\n",
       "  'to be paid, and then outputs the bills (invoices) to be entered into Quickbooks and be paid in the Quickbook Bills sheet.\\nThis sheet, after some manual manipulation, is uploaded to Saasant, a third party tool that uploads the bills to Quickbooks. \\n \\nAdditional Context:',\n",
       "  'one testing methodology should be used)\\npriority weight (optional): a numeric weight to further prioritize the task group in execution order\\nNon-Standard Task Groups\\nFor tasks that are not going to fit cleanly into the TempTestCommit paradigm, there are three SQLOperator classes \\n(DropSQLExecuteQueryOperator, RenameSQLExecuteQueryOperator, GenerateSQLExecuteQueryOperator) that can be used to initialize \\ncustom task groups. The DropSQLExecuteQueryOperator drops a given table_name in a given schema_name if it exists, and the \\nRenameSQLExecuteQueryOperator will rename the given {{schema_name}}.{{current_table_name}} to the given {{schema_name}}.\\n{{new_table_name}}. The GenerateSQLExecuteQueryOperator is a task that will execute some sql statement that is passed through the \\nsql param.\\nHere is one example of a custom task group that is used in the metric layer. It rebuilds the card activation event descriptions table if a rerun \\nparameter is passed in the DAG config.\\nFile Structure',\n",
       "  '30\\n2023 Q2 OKRs\\nTimeline\\nRelated pagesTeam\\nOwner:\\nEnd-of-quarter objective \\nscore:0.0-1.01.0 Month 1\\nMonth 2\\nMonth 3\\n     \\n     \\n      \\n     \\n     \\n      \\n     \\n     Objectives Key results Owner Partner \\nwithExpected \\nEoQ key \\nresult \\nscoreCurrent status\\nFor OKR pro tips from Atlassian teams visit:https://www.atlassian.com/team-playbook/plays/okrs',\n",
       "  'Transaction 1 is a refresh on a MV, and transaction 2 is a select * statement on the MV that was just refreshed\\nThis should lead to this error: SQL Error [<5700]: Underlying table with oid (example-oid) of view (example-materialized-\\nview-name) does not exist\\nThe suggested solution is to have locks on the tables that the MV sources from, since a refresh statement would have to wait until \\nthe locks are released. Then, this solution suggests running MVs during non-business hours to make sure it doesn’t interfere with \\nother transactions.\\nThis implies that locks are applied on materialized views when they are refreshing and are not viewable by downstream select \\nstatements\\nTakeaway: If we implement MVs, in order to assure upstream tables do not run into an error if they query a MV when it is refreshing, \\nwe can make sure MVs are not being refreshed during that time by first applying locks to its upstream source tables. Once the locks \\nare released, they can refresh.\\nConclusion',\n",
       "  '77    return config.params\\n78\\n79def main():\\n80    # Set up argument parser\\n81    parser = argparse .ArgumentParser (description =\"Render SQL query from template\" )\\n82    parser .add_argument (\\'file_name\\' , type=str, help=\\'The path to the file to be rendered\\' )',\n",
       "  '3: Update the list of vendors and their invoice types\\nPreston and Abby will create a running list of vendors and invoice types (PDFs or CSVs). \\n[link here] \\nUpdate the sheet with the new vendor and its type!\\n4: Adding the vendor to Quickbooks\\nAdd the new vendor to Quickbooks! Write down the name you put in to give to DE in step 6 (example: if you put in Golden Nugget as GN, let \\nDE know GN was used).\\nImportant note: If a vendor name has been changed in Quickbooks, please let DE know the new name for the vendor!\\n5: Setting up the code\\nOnce all that’s done, reach out to DE! Tell them which invoice type the new vendor is from step #4, along with the name you put in step #5. If \\nyou can, also provide them with an example invoice. They’ll need to make some code changes/ test the code to accommodate the new \\nvendor. They’ll reach back out when they’re done!\\n6: Finally, DE will turn the correct zap on!',\n",
       "  '166\\nProd Errors 2023-07-10 Onwards\\nThis page will allow us to keep track of the types of errors that fire and the frequency that they do for each week starting on 2023-07-10.\\nThis allows us to see how often an error or warning is sent each week and, at a glance, if it is firing across multiple weeks. Weeks start on a \\nMonday and end on a Sunday to align with our prod error fielding schedule. The name of the team members fielding the errors are also \\nunder each week title so that others may go back and ask the fielders questions if needed!\\nWeek of   -  \\nDerya & Ernest\\nWeek of   -  \\nAbby & Andrew\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nDerya & Hongkai\\nWeek of   -  \\nAbby & Andrew\\n \\nWeek of   -  \\nDerya & Gabe',\n",
       "  '583\\nClick Next: Tags\\nClick Next: Review\\nReview what you have done, and click “Create User”\\nSave the Access Key ID and Secret Access Key provided on the screen once the user is created. You will enter these as credentials\\nCreate the zapier integration, following this as an example\\nStep 3: Create a database in Glue where this data will live\\nNavigate to the AWS Glue service in the AWS console\\nUnder Data catalog, click databases → add a database\\nGive the database a relevant name (csvs_statements_glue_database)\\nStep 4: Create a table in the database you just created\\nNavigate to the AWS Glue service in the AWS console → Databases → Tables\\nSearch for any tables in your database (it should be empty)\\nClick “Add Tables using a crawler”',\n",
       "  '325\\nThere are 92 (out of 35,550, ~ 0.25%) transactions with this latency since 10/17. Of all transactions with latency after 10/17 (the supposed \\nfix), the average latency is 33 hours. Of all transactions after 10/17, the average latency is 0.12 hours.\\nPerpay Deserve Data Issue # 5:\\nSometimes transaction show up as 2 separate transactions in the Deserve settled SFTP report, but show up as 2 transactions in the \\nDeserve cleared SFTP report and Deserve Perpay data.\\nTo find:\\nExample:\\nTransaction ID: 0c5df619-62c7-584e-978a-a229cb0bbd4d // 126915\\n \\nSub example (this is difficult)!\\nSFTP: 2 transactions on 8/25 (amount $12.10) and 8/26 (amount $105.58), same transaction ID. total: $117.68\\nDeserve Perpay Core: 1 transaction created 8/24, modified 8/26, amount $117.68. Only 08/26 row in card transaciton snapshot.\\nUser’s statement: 3 transactions, total: $117.68\\n1select count(*) as cnt, transaction_id\\n2from perpay_accounting_datamart_ext.deserve_daily_settled_transactions_report',\n",
       "  \"2select a.*, b.email\\n3from perpay_risk_datamart_ext .equifax_raw_data as a\\n4left join perpay_general_datamart_int .user_attribute as b on b.borrower_id = a.consumer_account_number\\n5where lower(b.email) like '[USER_EMAIL_ALL_LOWER_CASE]%'\",\n",
       "  'detected by Travis. Yet, this solution is inadequate because we do not want to ignore other parameter changes; the wlm_json_configuration \\nparameter is the only that causes drift. Unfortunately, because the parameter block is a set instead of a list or a map, it is unable to be \\nindexed, making it difficult (maybe even impossible) to ignore the wlm_json_configuration parameter on its own. \\nSolution\\n[October 26, 2022]',\n",
       "  '371\\nNew Process\\nPart 1: Invoice Ingestion\\nInvoices are received in PDF or Excel format to the email address invoices@perpay.com. \\n^ I don’t want to change this part of the process because it would require our vendors to switch their current setup, and there has \\nbeen reluctancy to change in the past. Furthermore, receiving invoices as attachments through the Gmail is manageable on our \\nside.\\nThe excel sheets are plopped into an S3 bucket through Zapier and loaded into Redshift automatically (via Glue). This will require an \\nupdate to the invoice_matching DAG. The Zaps are designed to not do any data cleaning, which is instead saved for later in the process \\nafter the invoices have been loaded into Redshift. One exception is Milor Group because the CSVs they send cannot be handled by the glue \\njob until they are cleaned.\\n^ There is no reason to copy and paste these into a Google sheet prior to pulling them into our db as we currently do. Plopping',\n",
       "  '2          then 1 else 0 end as deserve_employee_ind,\\n3       a.id as perpay_id,\\n4       a.created,\\n5       a.amount as perpay_amount,',\n",
       "  'Engineering’s public tables. \\nRollforward Table Logic:\\n \\nSource Tables:\\n \\nDefinition:\\nA unique id associated with each Perpay user.\\nBusiness Context:\\nAn id for anyone who has signed up for the Perpay platform. It is generated by engineering and can be used as a FK to a lot of \\nEngineering’s public tables.\\nRollforward Table Logic:\\n \\nSource Tables:',\n",
       "  '376\\nextract the key/value pairs and create the CSV name for S3\\nextracts the line items and create CSV name for S3\\nmake sure to add the layer for the pandas package if used. → this is the photo in the above section\\n2: Probably a good idea to increase the lambda function timeout from 3s to 2m',\n",
       "  'Completed Direct Deposits: A completed direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of completed_accounting. The amount of the row is applied as a deposit to the associated \\nborrower_id on the date of the created_ts timestamp. The completed_accounting status comes before the completed timestamp \\nthat is used in engineering tie-out. The completed_accounting timestamp more closely represents when Perpay actually receives the \\nmoney, whereas the completed timestamp more closely represents when Perpay can attribute the deposit to a user.\\nReturned Direct Deposits: A rejected direct deposit is any row in the platform_deposits metric layer with a source of \\ndirect_deposit and a status of returned_accounting. The amount of the row is deducted from the core balance of the associated \\nborrower_id on the date of the created_ts timestamp.\\nPayment Tracking',\n",
       "  '63\\nActual Payment \\nAmountactual_payment\\n_amountThe monthly payment actually received.Summed deposit amounts for \\nthe last completed month. \\nAccount Status account_status The status code the identifies the current \\ncondition of the account. More \\ninformation in Documentation.See further clarification below. \\nThis includes logic for new \\ncharge-off policy. In addition, the \\naccount status is DA for \\naccounts merged that no longer \\nhave loans associated with \\nthem. The account then stops \\nbeing reported after the DA \\nstatus is sent.borrower_hist_d\\naily_status:\\ndays_past_due,\\nbalance\\nperpay_risk_dat\\namart.trueaccor\\nd_upload_log, \\ntrueaccrord_deb\\nts, \\ntrueaccord_pay\\nments,perpay_a\\nccounting_data\\nmart.account_m\\nerged_users\\nPayment Ratingpayment_rating This must only be reported when the \\naccount status is 05, 13, 65, 88, 89, 94 \\nor 95. Contains a code that identifies \\nwhether the account was current, past \\ndue, in collections or charged off. \\n0 = 0–29 days past due\\n1 = 30-59 days past due',\n",
       "  \"82       original_charge_off_amount,\\n83       date_last_payment,\\n84       scheduled_monthly_payment_amount,\\n85       special_comment\\n86from perpay_risk_datamart.equifax_raw_data as a\\n87left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n88left join (\\n89    select * from public.forgiveness_period\\n90    where type != 'disaster_relief'\\n91    and cast(created as date) >= '2020-12-01'\\n92    ) as c on c.borrower_id = a.consumer_account_number\\n93where c.borrower_id is not null\\n94and enter_date = max_enter_date;\",\n",
       "  '520\\npublic.payment_status\\nWill keep the history of status changes for the public.payment table and will make up for the removal of the returned_payments table.\\nThere will no longer be any partial payment returns.This table will include a reason column with values: NEW_PAYMENT, \\nUSER_REQUESTED, DEPOSIT_RETURNED.\\npublic.feature_payment_status\\nWill keep track of the status changes for the public.feature_payment table.\\nHere is the one table we are deprecating:\\npublic.returned_payment\\nThis table is currently responsible for tracking returns. But due to issues around partial returns, it does not do a great job at it. So this \\nwill now be removed and replaced by the public.payment_status table. We will also no longer do partial returns.\\nAdditionally, the public.fee and public.card_paymentstatus tables are mentioned, however, doesn’t look like there will be any changes there.\\nScope of perpay-airflow Changes\\nDeposits\\nDeposits related changes will be mostly related to the following:',\n",
       "  'List of existing historical tables and the fields that are being tracked on them:\\nwaffle_switch_history:\\nname\\nactive\\nwaffle_flag_history:\\nname\\npercent\\neveryone\\njob_history:\\npay_cycle_json\\nborrower\\ncompany\\nstatus\\nextra_info\\ncompany_history:\\nname\\nround_to_dollar\\nactive\\nverified\\nextra_info',\n",
       "  '22\\nBELOW IS FOR V1. ACTIVELY DEPRECATING.\\nProduction:',\n",
       "  '29\\n2023 Q1 OKRs\\n Timeline 1/16/2023 - 3/27/2023\\nRelated \\npagesRoadmap:\\nPerpay OKRs | Q1 2023 - RAAF Roadmap Team @JD Herr @Abigail Rehmet \\nComplete a refactoring of our \\naccount reconciliation process\\nOwner: @Abigail Rehmet 1.0 Month 1\\nMonth 2\\nMonth 3\\nCardholders are accurately \\nreported to all three major credit \\nbureaus\\nOwner: @Abigail Rehmet   @JD \\nHerr   \\n     \\n     \\nTarget: Hire 1+ individual with \\n4+ years experience\\nOwner: @JD Herr      \\n     \\n     Objectives Key results Owner Partner \\nwithExpected \\nEoQ key \\nresult \\nscoreCurrent status',\n",
       "  '277                    \"workable_text\": 1,\\n278                    \"workable_email\": 1,\\n279                    \"strategy_email\": \"Standard\",\\n280                    \"strategy_email_override\": null,\\n281                    \"strategy_phone\": \"Standard\",\\n282                    \"strategy_phone_override\": \"ABTest1\",\\n283                    \"strategy_text\": \"Standard\",\\n284                    \"strategy_text_override\": null,\\n285                    \"risk_tier_at_entry\": \"T3\"\\n286                    \"risk_tier_segment_at_entry\": \"T3 Segment 2\"\\n287                }\\n288    }\\n289}\\n290\\nproduct_id bigint The unique identifier for the product.\\nname varchar(219) The name of the product.\\nurl varchar(274) The URL of the product.\\nvisibility_search integer Whether the product is visible in search.\\nvisibility_catalog integer Whether the product is visible in the \\ncatalog.\\ndescription varchar(11415) The description of the product.\\nthumbnail_url varchar(169) The URL of the thumbnail of the product;',\n",
       "  \"514\\nTying out Engineering's Account Balance Flow\\nGoal\\nThis document tracks issues within the TEMP_core_account_daily_rollforward table. It enables us to record discrepancies in our \\naccount balance, calculation inaccuracies, or inadvertent additions. Additionally, it assists in identifying discrepancies resulting from manual \\nerrors. This table is built by the TEMP_data_model_accounting DAG, which can be initiated manually.\\nContext\\nWe are stepping back from the core process to understand how Engineering determines account balance at the end of the day. Our aim is \\nto monitor every fund transaction that occurs within the day before we establish the accounting department's version of the balance. Once \\nwe have a complete grasp of all the elements contributing to Engineering's balance, we can proceed by incorporating new calculations for \\nthe starting and ending balances to view variances on a higher level like these. When this happens, we can remove the\",\n",
       "  '77    return config.params\\n78\\n79def main():\\n80    # Set up argument parser\\n81    parser = argparse .ArgumentParser (description =\"Render SQL query from template\" )\\n82    parser .add_argument (\\'file_name\\' , type=str, help=\\'The path to the file to be rendered\\' )',\n",
       "  \"model/change the modeling to pull this information in? In terms of a workaround, we could match the variance dates with returned payment \\ndates for borrowers and flag them as (likely) partial refunds for the time being, but I don't think we can accurately track those funds in our \\ntables until we create a way to track the exact refund amounts\\nFollow-up R (4.1): The interim workaround makes sense to me. We would need to discuss the modeling with the team, I would say it might \\nbe challenging, but feasible.\\nFollow-up (4.1): Okay, I'll add it to the code and add it as a workaround in the google sheet so we can keep it in mind. Thanks!\\n \\nFollow-up (4.2): there are 1,791 of these now! I filtered where potential_partial_return_ind = 0, so they are all likely related to reversals v\\nariance_4.2.csv \\n \\nVariance 9: Initial Loan Amount\\nQuestions: \\n1. It appears how the initial loan amount is defined in this table has changed over time. For the start_bal_loans, it looks like the initial loan\",\n",
       "  'to hold the complete transactional history of the Pinwheel eligibility status of companies and payroll providers changing over time. Each time \\na company or payroll provider is added or dropped from the Pinwheel eligibility list, the change should be indicated in \\nwaffle_flag_history. \\nThe company_id and payroll_provider_id used in waffle_flag and waffle_flag_history are those assigned by Pinwheel, so the \\npayment_companypinwheelflag and payment_payrollproviderpinwheelflag are needed as reference tables for how the companies \\nand payroll providers are identified in our schemas.\\nThe waffle_flag tables contains a modified column that holds the timestamp of the most recent time a company or payroll provider’s \\neligibility status was changed. The waffle_flag table contains rows that have been updated as recently as Feb 2022. However, these \\nchanges were not noted in the waffle_flag_history table, which indicates that the most recent change in Pinwheel eligibility status was',\n",
       "  'exists. If no corresponding row exists in the auto_pay_payment table, then the created timestamp of the row in the deposit table is \\nused to determine the date that the ACH deposit should be applied to the user’s core account balance. The deposit is attributed to the \\naccount_id listed in the deposit table.\\nReturned ACH Deposits: Returned ACH deposits are sourced from rows in the deposit table with a content_type_id of 23 and a \\nstatus of returned. The date of the modified timestamp of the corresponding row in auto_pay_payment is used to determine the day \\nto observe the return of the deposit. In cases where there is no corresponding row in auto_pay_payment the date of the modified \\ntimestamp of the deposit row is used.\\nAn additional data source for returned ACH deposits is the Deserve SFTP data. When a row in the deposit table can be mapped to a row \\nin the auto_pay_payment table with a non-null card_payment_id, we check for cases where the Deserve SFTP data reports the payment',\n",
       "  'is aligned with the metric layer, timing issues in the reporting layer could also be prevented for tables whose sources are in both the \\nmetric layer and source layer. This would resolve timing issues and address the main shortcoming of MV’s ability to deal with unexpected \\nchanges\\nQuestions…\\nWhat is the expected cost difference with MVs and the source layer versus our current structure?\\nCompute: If they are implemented in the source layer as suggested above, it may increase compute cost. Despite this, they would \\nstill be efficient due to their ability to incrementally refresh. \\nMemory: MVs take up more memory than regular views and tables, and so memory usage would go up\\nWhat is the logic behind our current timing structure across our various data modeling layers? What is the latency for each MV (how \\noften does it need to be incrementally refreshed)?\\nDMS updates continuously, metric_layer runs every 3 hours, various reporting layer DAGs run at different times',\n",
       "  'payment_id in the returned_payment table, the least of the amount in the platform_payments metric layer and the amount in the \\nreturned_payment table is considered to be a returned payment applied to the loan_id in the platform_payments metric layer.\\nRefund Tracking\\nIn the accounting reporting table, payments, starting balances, and refunds are subject to repayment date consolidation described here.\\nThe refund amount applied to a loan balance is calculated as the lesser of the remaining loan balance at the time of the refund and the \\nrefund amount.\\nThe refund amount is sourced from the amount column in the refund table. The loan amount is sourced from the amount column in the \\nloan table for the loan that is associated with the given charge_id in the refund table.\\nBorrower credits and payments prior to the refund are then calculated by using the logic found in Payment Tracking and deducted from the',\n",
       "  \"30      sum(case when f.type in ('perpay_plus', 'perpay_plus_v2') then fp.amount else 0 end) as perpay_plus_revenu\\n31      sum(case when f.type not in ('perpay_plus', 'perpay_plus_v2') then fp.amount else 0 end) as other_feature_\\n32  from {{pub_schema}}.feature_payment fp\\n33  left join {{pub_schema}}.feature_enrollment fe on fp.feature_enrollment_id = fe.id\\n34  left join {{pub_schema}}.feature f on fe.feature_id = f.id\\n35  group by cast(fp.created as date), fp.borrower_id, f.type\\n36  )\\n37  select\\n38      dc1.borrower_id,\\n39      dc1.user_id,\\n40      dc1.account_id,\\n41      dc1.src_dt,\\n42      dc1.deposit_amt,\\n43      dc1.cash_refund,\\n44      dc1.refund_to_loan,\\n45      dc1.returned_payment_amt,\\n46      dc1.payments_to_loan,\\n47      dc1.credits_applied_to_loan,\\n48      dc1.payments_less_credit,\\n49      dc1.awaiting_payment_bal,\\n50      dc1.awaiting_payment_applied,\\n51      dc1.withdrawal_amount,\\n52      coalesce(dc2.perpay_plus_revenue, 0) as perpay_plus_revenue,\",\n",
       "  'last refresh, and there is a consistency across the data at that \\ntime snapshot.incrementally (manually or automatically) for the data to be up \\nto date. However, updating a MV too frequently can impact \\nperformance, so finding a balance that works for our \\narchitecture will require foresight.\\nRedshift makes MVs powerful\\nMVs in redshift can query from S3\\nMVs can be built on top of other MVsMemory\\nWhile compute costs decrease, MV’s take up a lot of storage \\nspace, which can result in increased costs\\n1[\\n2    DISTSTYLE { EVEN | ALL | KEY } \\n3    DISTKEY ( distkey_identifier )\\n4    SORTKEY ( column_name , ... )\\n5]',\n",
       "  'through some of the confluence documentation that has been put together (thank you Abby!) that goes over our common trouble \\nshooting.\\nOnboarding\\n(1) First day',\n",
       "  '1subl docker exec -it perpay-airflow_worker_default bash\\n1python3 -m analytics.credit_reporting.lib.scripts.script_experian_pull_monthly_vantages -p -s\\n1dags/analytics/credit_reporting/lib/untracked\\n1aws s3 cp experian_vantage_pull_borrowers.csv s3://ana-prod-third-party-data-rukoa',\n",
       "  'potential lead emails, any emails involved in cyclic merges, the old emails associated with the cluster, and the last merged to email. In order \\nto detect cyclic merges, we use the NetworkX method that lets us extract strongly connected components. To detect potential lead nodes, \\nwe will look for all nodes with out-degrees of zero, meaning nodes that are never merged into other nodes. This now gives us a dataset that \\ntells us whether a given cluster has more than one potential lead node or whether it has cyclic merges or whether it’s lead node is the last \\nmerged to email. In the next step, we need to separate the healthy clusters from the unhealthy clusters and categorize them according to \\ntheir issues.\\nWe first split the clusters by whether they have a single lead account or not. In the code, we refer to this as healthy vs. unhealthy clusters.',\n",
       "  'AWS resources. These permissions are granular and can be assigned at a very fine-grained level, allowing administrators to control \\naccess to specific resources and operations):\\ncloudformation:CreateStack\\ncloudformation:CreateUploadBucket\\ncloudformation:DeleteStack\\ncloudformation:DescribeStacks\\ncloudformation:DescribeStackEvents\\ncloudformation:GetStackPolicy\\ncloudformation:GetTemplateSummary\\ncloudformation:ListStacks\\ncloudformation:ListStackResources\\nec2:DescribeSecurityGroups\\nec2:DescribeSubnets\\nec2:DescribeVpcs\\niam:AttachRolePolicy\\niam:CreatePolicy\\niam:CreateRole\\niam:DeleteRole\\niam:DeleteRolePolicy\\niam:DetachRolePolicy\\niam:GetRole\\niam:GetRolePolicy\\niam:PassRole\\niam:PutRolePolicy\\niam:UpdateAssumeRolePolicy\\nkms:Decrypt\\nlambda:AddPermission\\nlambda:CreateFunction\\nlambda:DeleteFunction\\nlambda:GetCodeSigningConfig\\nlambda:GetFunction\\nlambda:GetFunctionCodeSigningConfig\\nlambda:GetLayerVersion\\nlambda:InvokeFunction',\n",
       "  'Good → reformatted\\nAdHoc Reports\\nAre we still looking at product_card_activation for anything?\\nZach M would know\\nCard data model hist\\nCard account statement history data model ✅  PR is up\\nPR 2023-02-03: Update to the Card Account Statement History DM- logic & format changes\\ndocumentation: Account Statement History \\nCard account statement history fact\\n Good → reformatted\\nCard account statement history balance\\nNeed RDD at time – portfolio view (“$100 for $500” etc) → done',\n",
       "  \"330\\nNonIssues that I verified:\\nPayment amounts are the same\\nPayments that are returned in the SFTP are returned in Perpay\\n6       a.card_account_id,\\n7       e.email,\\n8       d.deserve_id as deserve_account_id,\\n9       a.status as perpay_status,\\n10       b.payment_id,\\n11       b.account_id,\\n12       b.payment_status as deserve_payment_status,\\n13       b.payment_amount as deserve_payment_amount,\\n14       b.completed_at as completed_at,\\n15       b.sftp_dt,\\n16       c.card_payment_id\\n17from public.card_cardpayment as a\\n18left join (select * from users_frank.card_sftp_payment where current_ind = 1) as b on a.deserve_id = b.payment_i\\n19left join public.auto_pay_payment as c on a.id = c.card_payment_id\\n20left join public.card_account as d on a.card_account_id = d.id\\n21left join perpay_general_datamart_int.user_attribute as e on d.borrower_id = e.borrower_id\\n22where  b.payment_id is not null and\\n23       perpay_status = 'RETURNED' and\\n24       deserve_payment_status = 'COMPLETED'\",\n",
       "  \"Redshift\\n1-- Redshift\\n2-- Note: Concurrency scaling is when AWS has to pull in additional resources when\\n3--   the cluster is under heavy load (near 100% CPU)\\n4select * from (\\n5select\\n6concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\\n7cast(case\\n8 when usagetype = 'Node:ra3.4xlarge'  then 'Compute'\\n9 when usagetype = 'RMS:ra3.4xlarge'  then 'Storage'\\n10 when usagetype = 'CS:ra3.4xlarge'  then 'Conc. Scaling'\\n11 when usagetype = 'USE1-DataScanned'  then 'Spectrum Scan'\\n12 else ''\\n13 end as varchar(100))as usage_type ,\\n14 totalcost:: float\\n15from users_herr .aws_bill_total\\n16where productcode = 'AmazonRedshift'  and\\n17   linkedaccountname is not null and \\n18   linkedaccountname = 'Production Account'\\n19   --linkedaccountname = 'Staging Account'\\n20   and totalcost > 0\\n21   and invoiceid is not null\\n22)\\n23pivot (sum(totalcost ) for usage_type in ('Compute' , 'Storage' , 'Conc. Scaling' , 'Spectrum Scan' ))\",\n",
       "  'job until they are cleaned.\\n^ There is no reason to copy and paste these into a Google sheet prior to pulling them into our db as we currently do. Plopping \\nthem into S3 will cut out that unnecessary manual work. \\nZapier connects Gmail attachments with S3, and we already have an account. It would require minimal work to complete the Zapier \\nset up for all Excels (BestBuy, Rymax, and FragranceNet). Further thought is required to transform these Excels into CSVs once \\nthey’re in S3, but I believe we can use a lambda function to do so. I don’t think there’s a need to convert these to Parquet since that \\nUpdates to the process are color coded in teal and steps that have not been updated remain in black. Bold teal are there to remind \\nme which DAGs will change with these updates.\\nReasoning behind decisions are displayed through quote blocks.',\n",
       "  '34\\nwell as the overall I/O. Glue can do this and is designed to do this. Utilizing this tool would make the transformations faster, reduce code \\nand up keep on our side, and make the process less error prone overall.',\n",
       "  '238                        \"category_path_6\": NULL,\\n239                        \"visits\": 10\\n240                    }\\n241                ]\\n242            },\\n243            \"viewed_products\": [44444, 66654, 77789],\\n244            \"viewed_products_detail\": [{\\n245 \"product_id\": 44444,\\n246 \"viewed_dt\": \"2018-10-25\"\\n247 },\\n248 {\\n249 \"product_id\": 66654,\\n250 \"viewed_dt\": \"2019-10-20\"\\n251 },\\n252 {\\n253 \"product_id\": 77789,\\n254 \"viewed_dt\": \"2019-11-29\"\\n255 }',\n",
       "  'involve non-DE team members so that SMEs can validate that the documentation is accurate and feature complete.\\nDatahub Tooling\\nThe Datahub UI has limited capabilities for adjusting parameters and access. However, the CLI and SDK offer many more options and \\ncapabilities through easy-to-use integrations. To provide timely updates and maintenance to the Datahub UI, Data Engineering has \\nconstructed a Jupyter notebook (link will be provided once complete) containing distinct cell sets capable of both updating the UI in specific \\nways and extracting relevant diagnostic information. The notebook will require users to input their Datahub credentials for any interaction, \\nthereby not incurring any additional security or governance issues.',\n",
       "  '397\\nAccounting Commerce Loan Daily Rollforward\\nData Dictionary\\nPayment Tracking\\nIn the accounting reporting table, payments, starting balances, and refunds are subject to repayment date consolidation described here.user_id Integer The user id associated with the loan being \\nobserved \\naccount_id Integer The associated user’s account ID  \\nborrower_id Integer The associated user’s borrower ID  \\nemail Varchar The associated user’s email  \\nloan_id Integer The id of the loan being observed  \\nsrc_dt Date The observation date  \\nobservation_day Integer The total days of observation on the loan at \\nthe src_dt \\nloan_status Varchar The last accounting-relevant loan status \\n(awaiting_payment, repayment, \\ncharged_off, complete, refunded) \\nstarting_balance Numeric(\\n38,2)The starting balance on the observation date. \\nOn the first day of observation, it is sourced \\nfrom the balance ledger. On every other day it \\nis derived from the previous day’s ending \\nbalanceCalculating \\nDaily \\nBalances',\n",
       "  'uses StatsD to power their dashboards. They make sure to monitor queued and running tasks in addition to count runs: \\nhttps://www.datadoghq.com/blog/monitor-airflow-with-datadog/\\nQudoble, similar to Atronmer and DataDog and a SaaS platform that hosts Airflow for companies, uses Prometheus and Grafana to track \\nmetrics. They have one dashboard for Airflow as a whole tracking dag counts, dag duration, and dependency checks: \\nhttps://docs.qubole.com/en/latest/user-guide/data-engineering/airflow/monitor-airflow-cluster.html\\nLyft uses Airflow in their stack, and uses StatsD, Grafana, and DataDog to monitor Airflow. The article linked also provides a lot of tips for \\ngetting more granular insight into Airflow and improving its performance, a topic which should be revisted: https://eng.lyft.com/running-\\napache-airflow-at-lyft-6e53bb8fccff\\nPandora uses Airflow in their stack, and uses Prometheus & Grafana to monitor its state. The shown Grafana dashboard tracks at the',\n",
       "  \"One can view payment_achtransaction as the ACH “analog” of auto_pay_payment\\npublic.ach_transaction_history serves as the history table for public.ach_transaction. That is, there exists a 1-to-many \\nrelationship between ach_transaction and ach_transaction_history (the same transaction could have reached multiple \\nstatuses during its history). \\n \\nThe following diagram illustrates the information above:\\n \\nEngineering’s Deposit Process\\nauto_pay_payment and achdetail deposit_ids are mutually exclusive. These tables both trickle into the deposit table. The \\ndeposit table also receives data from scheduled_payment table. Occasionally we also get physical checks which don't have an \\nautomated process as they are such a small population of our deposits.\\nauto_pay_payment process:\\nUser creates a schedule through the UI.\\nWhen the scheduled time is up, engineering creates an entry in auto_pay_payment.\",\n",
       "  'Pages Events353\\n           Group Permissions355\\n      Projects356\\n           Deployed357\\n                Amplitude Event Reduction358\\n                Accounting DAG Delay365\\n                Marketplace Credit Reporting Review (Current Deployment)368\\n                Accounts Payable Revamp - Q3 OKR369\\n                     Setting Up AWS Lambda w AWS Textract375\\n                Account Reconciliation V2377\\n                     Core Account Balance Daily Ledger379\\n                          Engineering Core Account Daily Rollforward380\\n                          Accounting Core Account Daily Rollforward386\\n                          DE Notes on Initial Core Discrepancies390\\n                     Commerce Loan Balance Daily Ledger393\\n                          Engineering Commerce Loan Daily Rollforward394\\n                          Accounting Commerce Loan Daily Rollforward397\\n                          DE Notes on Initial Loan Discrepancies400',\n",
       "  'chooses the earlier of the 2 profiles but its not straightforward)\\ni. The borrower IDs are then updated for the merge users  \\ne. The perpay_risk_datamart_ext.experian_borrowers table is then updated with all new borrowers and opt-in dates for reporting\\n2. Now that the updated borrower population has been confirmed, we now have to get all the raw data required for each borrower by the \\n(eventual) Metro 2 process (here). The following subpoints overview the SQL code that builds the raw data\\na. The same merged user logic is used as in the borrower query above so that the correct ids are used to throughout the rest of the \\nlogic (here)\\nb. Get all bankruptcy statuses for each borrower - these statuses come from the user_message table data field (here) and then are \\ncleaned up to their core action - It looks like the 2 main types are chapter 7 and 13 bankruptcy\\ni. A table is also created for the borrowers who have already been processed (discharged) for bankruptcy should not be reported on',\n",
       "  '97\\nmaterialized view (a time and energy consuming task) and find and drop tables that are dependent on the changed table (drop cascade), \\nwhich is both arduous and inefficient.\\nThis is where the source layer comes in. The source layer is meant to lie between the public and metric layer. The source layer’s main \\nfunction is to “numerically curate” the raw data from the public layer. This layer of validated data is meant to handle unexpected changes \\nthat may arise from public layer sources. This can play a role in the implementation of the materialized views which can smoothly carry out \\ntheir function of resolving timing issues. The placement of the MVs is further discussed in the section The source layer and materialized \\nviews. A good example of a source layer implementation is how DBT implements this structure.\\nMaterialized Views\\nWhat are they? Why use MVs? How do they work?',\n",
       "  '166\\nProd Errors 2023-07-10 Onwards\\nThis page will allow us to keep track of the types of errors that fire and the frequency that they do for each week starting on 2023-07-10.\\nThis allows us to see how often an error or warning is sent each week and, at a glance, if it is firing across multiple weeks. Weeks start on a \\nMonday and end on a Sunday to align with our prod error fielding schedule. The name of the team members fielding the errors are also \\nunder each week title so that others may go back and ask the fielders questions if needed!\\nWeek of   -  \\nDerya & Ernest\\nWeek of   -  \\nAbby & Andrew\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nDerya & Hongkai\\nWeek of   -  \\nAbby & Andrew\\n \\nWeek of   -  \\nDerya & Gabe',\n",
       "  '161\\nwould \\npopula\\nte the \\nfollowi\\nng day \\nwith \\nthe \\ngroup \\nby \\nthat \\nwas \\nadded \\nin the \\ninitially \\ntemp \\npull \\nreques\\ntup some timing on \\ntheir end because \\nthe date of 2023-05-\\n09 shouldn’t be \\nincluded in the \\n2023-05-08 data we \\nreceive on the 9th \\nand at the time I’m \\nwriting this (1PM, \\nthe ts of 2023-05-\\n09T03:23:41Z \\nhasn’t happened \\nyet). It is also the \\nonly record with that \\ndate which seems \\nsuspicious._borrower_his\\nt_daily_fact\\n https://g\\nithub.co\\nm/Perp\\nay/perp\\nay-\\nairflow/\\nblob/39\\nc8b54b\\n3853a3\\n18d7e8\\na5638e\\n3922fe9\\nc12450\\nd/dags/\\nanalytic\\ns/data_\\nmodel_f\\null/user\\n_data_\\nmodel/b\\norrower\\n_attribut\\ne.py#L2\\n522023-07-\\n07incorrect manual \\nchange to SL \\n($0.35) which \\ncaused a \\nmassive \\nutilization failing \\na testcaused an test to \\nfail which caused \\niterable to not \\nupdatepotential_utiliza\\ntion > 1000 select * from \\npublic.event \\nwhere id = \\n252576188;Engr \\ncanne\\nl\\n  2023-08-\\n06 A card borrower \\nhas  a negative \\nbalance yet has \\nnon-zero days \\npast dueWe have a test that',\n",
       "  \"i. ex: git checkout -b feature/new_dm\\n3. Flip between branches by checking out\\na. git checkout <branch>\\nCommitting and Pushing\\n1. Stage files you want to commit\\na. git add . for all changed files\\nb. git add <filename> for specific files\\n2. Push all commits up to the remote branch with appropriate git commit messages\\na. git commit -m 'your message in here'\\ni. The messages should be short and sweet to reflect your changes\\nb. git push origin <branch name>\\nTesting\\n1. Once the branch is ready, test it in your local airflow environment, and then test on staging! \\na. Before successfully running on staging, a draft PR can be created. However, do not mark the PR ready for review until you have fully \\ntested and validated the code and any downstream dependencies\",\n",
       "  'd63c-467a-\\nabe1-\\nda532de83c0fBBY-\\nBB20840922PS-\\nMX3X2LLABest Buy Power Sales 30:15.0 05:32.6\\ne8de589f-\\n6408-4678-\\na576-\\n9b292138192\\n6fe2b1bcf-\\nd088-4be8-\\n8c5a-\\n2221fe742ca2HAR-CI2144 BBY-\\nBB20840922Harco Best Buy 55:17.7 30:15.0\\na051eb4b-\\nd7f1-48a5-\\na49b-\\ne388902db19\\n5fe2b1bcf-\\nd088-4be8-\\n8c5a-\\n2221fe742ca2BH-\\nBES3WMBBBY-\\nBB20840922B&H Best Buy 57:25.0 30:15.0child_id parent_id child_sku parent_sku child_vendor parent_vendo\\nrchild_created parent_create\\nd',\n",
       "  'These contracts would give the engineering department context to the data they change. Changes that cause no data loss are easier to \\nhandle from our end automatically, whereas changes that do require a manual fix to prevent breaking our DAGs. Providing this knowledge \\nto our data providers creates more transparency between both parties.\\nOur primary issue here is caused by a lack of visibility for data producers and data contracts aim to address that. Having these in place can \\ndecrease costs incurred by having to rerun data models every time there is preventable error caused by an upstream data change\\n If the above is unachievable, we can isolate our efforts to handling the raw data once we receive it and it is detected by error.\\nCreating a Step In Between\\nDBT’s Staging Layer\\nThis confluence page covers DBT’s implementation of a “source layer”.\\nIn this page, they describe its purpose:',\n",
       "  '614\\nGoogle/Facebook\\nwhat are audiences? how do we build them?',\n",
       "  '229\\ninformation to the account associated with the specified Vendor, appropriately tagged with its respective Category. This POST request differs \\nbased on whether the invoice number associated with the line item already exists in QB or not.\\nIf the invoice number exists in QB already, meaning it’s in existing_bills_for_qb_upload…\\nThe line item gets appended onto the bill that already exists.\\nIf the invoice number does not exist in QB yet, meaning it’s in new_bills_for_qb_upload…\\nThe bill gets uploaded as an entirely new invoice.\\nBill payments\\nOnce the line items are uploaded to QB, accounting uses an external tool to pay the bills. More info can be found in this doc [*Insert \\nAccounting/DE shared doc here*]. As a reminder, they cannot pay partial bills (e.g. a bill where it’s line items were split across).\\nPost-upload\\nOnce the line items are sent to QB, the accounts_payable_int.po_matches table contents are inserted into the',\n",
       "  '331\\nData Discrepancies v1 (11/21/22)\\nHigh Impact Issues\\nPerpay Core Issues:\\nPerpay Core Issues - Latency:\\nNote: for simplicty, we will consider anything longer than 2 hours “late”1 transaction ID: \\n206089Transactions that exist in \\npublic.card_transaction do not \\nexist in \\npublic.card_transaction_snapsho\\nt36,731 transactions (~19% \\nof transactions)This is fixed! PR has \\nbeen merged to make \\nsure this doesn’t \\nhappen with future \\ntransactions and older \\ntransactions have \\nbeen backfilled to \\nhave snapshots. The \\nfix was deployed on \\n12/29/22, snapshots \\nwere backfilled on \\n12/30/22.\\n2 transaction IDs: \\n5ae53d2c-b816-\\n5c84-92a6-\\n17c367cbe28b\\n120eba31-d983-\\n50bc-81e7-\\n88b3df9383e5\\n3b51342f-72e5-\\n5cd2-9138-\\n7828c9dbc727The total transaction is different in the \\nPerpay Core database (as compared \\nto Deserve Admin & SFTP files).3 transactions This is fixed! These \\nwere deserve issues \\nwhen they used to \\nsend us webhooks \\nbefore updating their \\nown database. I’ve \\nupdated these',\n",
       "  'price_special_from_date boolean Whether the product has a special price; \\ncurrently always False.\\nprice_special_to_date boolean Whether the product has a special price; \\ncurrently always False.\\nship_price double precision The shipping price of the product.Column Name Type Description',\n",
       "  '18--     freight           integer encode az64,\\n19    invoice_due_date  date encode az64,\\n20    created_ts        timestamp with time zone encode az64\\n21);\\n1drop table if exists temp_marketplace_int.variance_sheet_output_history;\\n2create table temp_marketplace_int.variance_sheet_output_history\\n3(\\n4    vendor_name       varchar(4000),\\n5    invoice_number    varchar(4000),\\n6    exists_in_qb_ind  varchar(4000),\\n7    invoice_total     varchar(4000),\\n8    ecomm_total       varchar(4000),\\n9    variance          varchar(4000),\\n10    ready_for_qb      varchar(4000),\\n11    chosen_payment_option varchar(4000),\\n12    invoice_qty       varchar(4000),\\n13    ecomm_qty         varchar(4000),\\n14    invoice_net_price varchar(4000),\\n15    ecomm_net_price   varchar(4000),\\n16    invoice_shipping  varchar(4000),\\n17    invoice_dropship  varchar(4000),\\n18    invoice_freight   varchar(4000),\\n19    invoice_po_number varchar(4000),\\n20    ecomm_po_number   varchar(4000),\\n21    invoice_sku       varchar(4000),',\n",
       "  \"567\\nWhy is it important to use a virtual environment?\\nWhat package do you use for it?\\nWhat excites you about Perpay?\\nFun\\nIf I gave you $100,000 to start your own business, what business would you start?\\nWhat would you do if you had to work but didn't need the money?\\nIf you woke up to 5,000 unread slack notifications and only had time to answer 100, how would you choose which ones to answer?\\nIf you had to pick something as a base for a time machine, what would you use? (Ex a washing machine, a DeLorean)\",\n",
       "  '6dagbag = models.DagBag()\\n7for ti in tis:\\n8  dag = dagbag.get_dag(ti.dag_id)\\n9  task = dag.get_task(ti.task_id)\\n10  ti.refresh_from_task(task)\\n11  executor = ExecutorLoader.get_default_executor()\\n12  executor.job_id = \"manual\"\\n13  executor.start()\\n14  executor.queue_task_instance(ti, ignore_all_deps=False, ignore_task_deps=False, ignore_ti_state=False)\\n15  executor.heartbeat()\\n16',\n",
       "  'expect the new member to remember everything.\\n5. Code Base and Repository Organization\\na. Overview the 4 main repos but only high level on the data eng (settings) and the 2 terraform repos. More importantly, walk through \\nthe layout of the perpay-airflow repo such as what is currently in the analytics directory, how we are re-organizing it (breaking things \\nout of the analytics directory), what utilities we have built, etc. This should come after the DAG overview so hopefully some \\nconnections can be made.\\n6. Terraform Repositories and Layout\\na. Overview of how the Terraform directory is laid out and how we utilize some of our own in house modules from the terraform-modules \\nrepo. Going over the differences and expectations of the 3 different environments as well has how the infrastructure is broken up in \\nstaging and prod would be good to go over as well (some things are not straightforward, such as the spectrum s3 bucket is created in',\n",
       "  'dispute_provisional_credit: When a customer initiates a dispute, this transaction is used to immediately decrement the balance. While \\nthe disputes are active, they’re noted on the statement under Amount in Dispute and fall off after the final decisioning. Since the balance \\nadjustment occurs immediately, if the dispute were to finalize as a win for the customer, the corresponding dispute_won transaction would \\nhave no affect on the balance.\\ndispute_lost_withdrawn: Reverses the effects of the provisional credit that is issued when a dispute is initiated. This is the only \\ntransaction for a dispute conclusion that affects the card balance, dispute_won and dispute_write_off both just indicate that the \\nprovisional credit initially issued will remain on the balance.\\nRewards Tracking\\nThe Perpay credit card is a part of the Mastercard World program and there are associated benefits along with that outlined here. For',\n",
       "  'Also an issue that is just allowed to self correct over time\\nGlue creates duplicate tables/partitions, seemingly randomly with a random hash at the end\\nThis is a ton of duplicated data\\n All non-SQL work is performed on the workers - this creates stress on the Airflow workers which can cause issues with the instance\\nAirflow is for orchestrating not executing\\nAll compute jobs could be launched in the same way as DS does now where the tasks would be isolated on their own, ephemeral, \\nEC2 nodes.\\nWhen an external table \\nBest Practices\\nAdding distkeys and sortkeys would be an easy win to improve performance of the Redshift cluster.  Benefits include:\\nPotentially downsizing the Redshift cluster as load is decreased.\\nImprove the efficiency for analysts: less time waiting for queries!\\nLeverage AWS Managed Airflow: this would offload a lot of personnel time onto AWS. Benefits include\\nWe would no longer need to dedicate time to managing Docker deployments.',\n",
       "  'Deserve SFTP Ingestion263\\n           Update Card Iterable Card Object without Data Model Full265\\n           Monthly Credit Reporting Rotation267\\n           Datahub Deployment270\\n           Rewards Validation (CBC)274\\n           Enable Team Members to Access Staging DB279\\n      Info280\\n           Repository Landscape281\\n           Account Reconciliation & Flow of Funds282\\n           Platform291\\n                Impact Naming Convention Investigation292\\n           Marketplace304\\n                Copy of Model History Tables305\\n           Card307\\n                Understanding the Card308\\n                     Card Data Modifications309\\n                     Card Glossary314\\n                     Card Table Glossary316\\n                Data Source Audit317\\n                     Card Transactions322\\n                     Card Payments328\\n                     Data Discrepancies v1 (11/21/22)331\\n                     Data Discrepancies v2 (11/29/22)335',\n",
       "  '205\\nIf they are not parsed at all, the invoice information will not be added to the docparser_invoices_master table.\\n4: .jpgs or .pngs cannot be processed in the first task in invoice_matching_v2 dag; only .pdfs may be processed (because that’s where data \\nis extracted in Docparser)\\nYou can set each parser to ingest specific extensions in Docparser. Just set it to ingest *.pdf only.',\n",
       "  'Immediate, catastrophic issues with the data platform and or infrastructure\\nSlack is the main channel where issues are boiled up\\nLarge product-driven initiatives (ex pinwheel integration)\\nThese are detailed within Jira as epic level projects\\nAd hoc requires from exterior stakeholders\\nThese come through the work request form\\nResource, platform, and optimization upgrades to the platform\\nMainly this is internal DE initiatives and development, however, sometimes this can take higher priority when upgrades are needed to \\nmeet a planned expansion of data volume\\nHow many projects we can support at one time?\\nHow this section is defined is almost out side of the scope of DE due to our work load being highly tied to the health of the platform and the \\ncurrent production launch schedule. However, it is possible to support more low priority initiatives than many high priority initiatives. DE will',\n",
       "  '60\\nOption 2: Once the initial test file is approved for continuous reporting with Experian, upon making sure the data is clean, rollout reporting of \\nother tiers.\\nWhich option is chosen depends on risk aversion.\\n--\\nReporting Period:\\nData will be reported on Tuesday, a midweek day to have time surrounding the reporting date to look into any outliers if necessary. The \\nreporting period is weekly, which is the most frequent pay-cycle Perpay supports. Any references to data from the “previous reporting period” \\nwill then be data from the second-to-last completed week (week-end on Saturday). The displacement of a week for reporting is also to give \\ntime for any outliers to be investigated. For example, data reported on Tuesday, Jan 21 2020 will be from Jan 5-11. Certain fields must be \\nreported with monthly values, which will include data from the last completed month.\\nData General Specifications:',\n",
       "  '110\\nSQL Queries\\nQuery readability is super important! It helps DEs understand code faster. \\nThese are some query conventions everyone should practice when planning to add code to the codebase. \\nReadability\\nReadability is SUPER important to us! If your PR does not follow these conventions, we will not merge it!\\n(1) Commenting conventions\\nEach table in our schema should have a general comment at the top of its creation!\\nAdditionally, each block of code should include a comment above it! \\nThe example above from platform_merged_users.py shows these well (lines 4-6 and each line above the CTE creations). \\n(2) SQL Capitalization\\nall SQL commands should be written in uppercase letters (i.e SELECT, FROM, WHERE, ORDER BY, GROUP BY, JOIN, MIN, MAX, SUM, FIRST, LAST, DESC, ASC, ON)\\nThis shouldn’t be too much of a burden, as it can be done once you’re done writing all your code:\\nIf you use VSCode:\\ninstall Upper Case SQL from the extensions panel',\n",
       "  'timestamp that the payment was initiated that are described above.\\nCalculating Daily Balances\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance. \\nOn the first day of observation, the starting balance is set to a value of 0. \\nTotal Movement: The amount that the starting balance moves on a given day is equal to the total transactions + account_opening_fee \\n- account_opening_fee_adjustment + monthly_fee + shipping_fee - debit_card_processing_fee_adjustment + \\ninternational_transaction_fee - international_transaction_fee_adjustment + returned_payment_fee + interest_charge - \\ninterest_charge_adjustment - cash_back + cash_back_adjustment - charge_off_settlement + principal_debit_adjustment - \\ncourtesty_credit - merchant_cashback - refund + credit_balance_refund + dispute_lost_withdrawn - \\ndispute_provisional_credit + returned_payments - completed_payments_pre_may_8_2023 -',\n",
       "  \"Interviews- Questions565\\n      Onboarding & Offboarding568\\n           DE Onboarding569\\n                M1 Chip Workarounds For Docker/Airflow571\\n           Offboarding572\\n                Sylvia's Transition573\\n                     Calculating AWS Costs574\\n                     Segment Data Ingestion (via S3)576\\n                     Accounts Payable Updates579\\n                          Roadmap for Completion580\\n                          Automated CSV --> Parquet in AWS Console581\\n                          Lambda function to convert excel --> csv file types587\\n                     Accounts that need new owners589\\n                     Random Resources / Knowledge Transfer590\\n                     Credit Card593\\n                          Lifetime walkthrough of a card user594\\n                Derya's Transition597\\n      Continuing Education598\\n           Analytics Stack Reporting599\\n           Data Modeling Pipelines & DBT600\\n           Airflow & Kubernetes601\",\n",
       "  'section). When the DAG runs normally, this test is not conducted.\\n4: Automated Testing\\nTests have been deployed to detect anomalies or irregularities in the process.There is a toggle in QB that warns when duplicate invoice numbers (document numbers) are added. This is ON because we do \\nNOT want duplicate invoice numbers. Based on everything above (checking what we want to push against what’s already in QB and \\nappending line items with existing invoices to them), we shouldn’t even be trying to push up duplicate invoice numbers. However, \\nthis toggle is an extra measure.\\nThe code to create accounts_payable_int.existing_bills_for_qb_upload lies in existing_bills_for_qb_upload.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/paying_bills/existing_bills_for_qb_upload.py).\\nThe code to create accounts_payable_int.new_bills_for_qb_upload lies in new_bills_for_qb_upload.py (PATH: \\ndags/analytics/accounts_payable_revamp/lib/paying_bills/new_bills_for_qb_upload.py).',\n",
       "  '521\\nRipgrep results for \"}}.payment_status\":\\nWithdrawal Requests\\nI don’t believe we need to make any withdrawal request related changes since most queries using this table are simply counting the various \\nstatuses or selecting rows based on statuses. Since the change here involves no longer deleting a subset of the cancelled withdrawal \\nrequests and including them in both the main and the status table, all that will happen is an increase in any amount showing cancelled \\nrequests which should not affect balances anyway. Nonetheless, here is the output of ripgrep for a search for \"}}.withdrawal_request\" in \\nour codebase:\\n20dags/analytics/data_model_accounting/account_reconciliation/account_balance.py\\n2170:  left join {{pub_schema }}.payment p on lbd.borrower_id = p.borrower_id\\n22\\n23dags/analytics/data_model_accounting/account_reconciliation/account_user_refunds.py\\n2449:  left join {{pub_schema }}.payment b on a.src_dt = cast(b.created as date) and a.borrower_id = b.borrower_id\\n25',\n",
       "  'Our largest cost will come from the on-demand infrastructure hosts. Currently, we have 16 active, so will be paying ~$288/month which is \\n~3456/year with the integration. \\nCurrently we pay ~$150/month, so gaining complete coverage of our metrics through this integration only increases this cost by ~$150.\\n5. Additional Resources\\nDatadog Integrations\\nAWS General Integration Guide & AWS CloudFormation Integration Guide\\nDatadog Documentation',\n",
       "  '3\\n4session = settings.Session()\\n5tis = session.query(models.TaskInstance).filter(models.TaskInstance.state==\\'queued\\')\\n6dagbag = models.DagBag()\\n7for ti in tis:\\n8  dag = dagbag.get_dag(ti.dag_id)\\n9  task = dag.get_task(ti.task_id)\\n10  ti.refresh_from_task(task)\\n11  executor = ExecutorLoader.get_default_executor()\\n12  executor.job_id = \"manual\"\\n13  executor.start()\\n14  executor.queue_task_instance(ti, ignore_all_deps=False, ignore_task_deps=False, ignore_ti_state=False)\\n15  executor.heartbeat()\\n1# Only allow one run of this DAG to be running at any given time\\n2dag = DAG(\\'my_dag_id\\', max_active_runs=1)\\n3\\n4# Allow a maximum of 10 tasks to be running across a max of 2 active DAG runs\\n5dag = DAG(\\'example2\\', concurrency=10, max_active_runs=2)\\n6',\n",
       "  'crucial that users have a clear understanding of the context behind each feature to avoid any ambiguity. For instance, knowing whether a \\ncolumn contains the date of the first occurrence of a status or represents the current status of an entity can significantly impact the quality of \\nbusiness decisions.\\nThere are two methods for adding descriptive data to columns, which can be used simultaneously but are not always required. The following \\nrecommendations highlight what should be included in column-level descriptions:\\n1. Description Section\\na. Description field - manually typed entry, no longer than 1 or 2 sentences providing context for correct use\\nb. Glossary Term (see below) 0 - if the field is completely summarized by the glossary term, no additional description is necessary. \\nHowever, if there is any deviaton from the glossary definition, it is highly encouraged to add a description to remove any ambiguity \\nfrom the stakeholder.\\n2. Tags',\n",
       "  'You can control the following schema entities:\\ntables - contract is applied when a new table is created\\ncolumns - contract is applied when a new column is created on an existing table\\ndata_type - contract is applied when data cannot be coerced into a data type associate with existing column.\\nYou can use contract modes to tell dlt how to apply contract for a particular entity:\\nevolve: No constraints on schema changes.\\nfreeze: This will raise an exception if data is encountered that does not fit the existing schema, so no data will be loaded to the \\ndestination\\ndiscard_row: This will discard any extracted row if it does not adhere to the existing schema, and this row will not be loaded to \\nthe destination.\\ndiscard_value: This will discard data in an extracted row that does not adhere to the existing schema and the row will be \\nloaded without this data.\\nThis link discusses schema evolution with dlt:\\nSchema evolution with dlt',\n",
       "  \"Using the Python API, we can investigate attributes of specific schemas and datasets and change what's visible to the users of DataHub. \\nWe've also used the Python API to access lineage to help with the project to move data off our staging environment. To get access to this, a \\ntoken will need to be generated from the UI and sent to a new user. Here you can manage permissions at a broad level - admin, editor, and \\nreader - and then to use the Python API or CLI, pass through this token with the server endpoint. This is managed by the utility through a \\nsecrets file that is created in your home directory and should be outlined in the utility README.md file. (note: PR not merged; not viewable). \\nFrom here, you can interact with the class underlying the utility notebook. \\nData Ingestion with DataHub\\nDataHub connects to our Redshift database and ingests metadata to get table/column profiling. These are set up in a job structure and can\",\n",
       "  'When DMS copies tables from one redshift db to another in a continuous mode, it does so in batches. Over a set time period, data is copied \\nfrom the source and stored in memory on the DMS instance. After a the copy time is concluded, it then sends the batched data to the target. \\nBoth processes incur i/o time. If the batched data gets too large, it will not only take more time and incur latency but also eat up more \\nmemory on the box. Once ram has been totally used, it moves to hard disk and eventually swap. At time all memory is maxed out, errors will \\nbegin to occur in the data and the instance will completely fail to update tables. \\nOn 2022-10-11, the ana-prod-core-to-redshift-taxing-tables-task fell over due to the exact issue above. By viewing the performance metrics \\nin the console for the task and the instance it was clear the latency and errors were being driven by the high memory usage. The immediate',\n",
       "  '7. Go to the Experian SFTP website\\na. The credentials and security question is saved in 1passoword, Data Engineering General folder\\ni. Experian SFTP Server (Quest and Ascend Program)\\n8.  Move to the to_xpn folder and upload the CSV into that folder\\n9. Email Kristine.Lobo@experian.com and  Matthew.Simon@experian.com and cc Zach, saying “We have uploaded a new Quest file for \\nprocessing. Please process the same as last month and include the positive factor codes”. Feel free to add your flair, Kristine and Matt \\nare very nice.\\n10. For posterity, save the file here:\\na. Add a numerical increment to the file. We hold on to these just in case there is an issue with how they processes something.\\n11. Go take a victory lap because you are done!\\n \\n1subl docker exec -it perpay-airflow_worker_default bash\\n1python3 -m analytics.credit_reporting.lib.scripts.script_experian_pull_monthly_vantages -p -s\\n1dags/analytics/credit_reporting/lib/untracked',\n",
       "  'changes were not noted in the waffle_flag_history table, which indicates that the most recent change in Pinwheel eligibility status was \\nin Sep 2021. It is clear that some of the more recent history of the waffle_flag table is missing from the waffle_flag_history table. It is \\nunknown how much of the history prior to Sep 2021 is also missing.\\nThere is a column history_type in waffle_flag_history that either contains the value ~ or -. We have logic in our current Pinwheel \\ndata model that checks this column and ignores the historical record if it contains -. Engineering could have more insight on what this \\ncolumn is meant to represent. \\nData Held in Archived Tables\\nThere are two archived tables, pinwheel_company_legacy and pinwheel_payroll_provider_legacy that also contain historical data for \\ncompanies and payroll providers moving on and off the pinwheel eligibility list. Our assumption is that these tables represent an effort to',\n",
       "  \"guys take a look?\\nFor a similar case:\\nHello! Seeing a card account application (application ID: 6512 , borrower ID: 61097) that has a null card_account_id  in the \\npublic.card_account_application table, but the card account application status is provisioned.I can see that they do have a \\ncard account (ID: 4588 ), and in the past, we see the card_account_id field populated in the card_account_application table \\nwhen the card account is created. This null field is causing some tests to break on our end.Would someone be able to help \\ninvestigate if this was expected behavior or not? If so, we'll update the tests on our end. Thanks!\\nAnd another case:\\nHi! We’re seeing a card_account with id = 6697 but no card tied to the account (nothing in card_card table where card_account_id \\n= 6697). This is breaking some tests on our end. Can someone look into this?\\n22    coalesce (b.id, j.card_id) as card_id ,\\n23    f.id as card_account_application_id ,\",\n",
       "  \"Variance 9: Initial Loan Amount\\nQuestions: \\n1. It appears how the initial loan amount is defined in this table has changed over time. For the start_bal_loans, it looks like the initial loan \\namount was the starting_balance of the loan's first record in the payment_loanprincipalbalancehistory table. However for the \\nend_bal_loans, it looks like initial loan amount was the ending balance of the loan's first record in the \\npayment_loanprincipalbalancehistory table. This is causing variances as we cannot tell exactly what starting balance to use for the \\nbeginning of the loan. Does anybody know the timeline of how the beginning of a loan has been represented in \\npayment_loanprincipalbalancehistory?\\n2. For the start_bal_loans, it also looks like the starting balance of the first row does not match the initial loan amount as reported in the \\nloan table. Does anybody know why this is? This is also causing some variances because we were expecting the start of the loan's\",\n",
       "  '341\\n \\n \\n \\nNotes on wrong ones:\\nActual Payment Amount - includes all deposits made, regardless of product.\\nDate Last Payment - includes all deposits made, regardless of product.\\n \\nOutstanding question: how should we be reporting users that are charged off, but not uploaded to TrueAccord?Indicator or confirmed, military, or other statuses. address\\nResidence \\nCode\\n residence_cod\\neO = Owns, R = Rents code. Blank  \\nSegment \\nIdentifier\\n segment_identi\\nfierReporting segment on employer info: \\nN1  \\nEmployer name\\n  employer_nam\\neName of employer company_name if not like \\nOther >user_attribute.c\\nompany_name\\nFirst Line of \\nEmployer \\nAddress\\n employer_addr\\ness_line1 Blank  \\nSecond Line of \\nEmployer \\nAddress\\n employer_addr\\ness_line2 Blank  \\nEmployer City\\n  employer_city  Blank  \\nEmployer State\\n  employer_state Blank  \\nEmployer \\nPostal / Zip \\nCode\\n employer_zip  Blank  \\nOccupation\\n  occupation  Blank  \\nReserved\\n  reserved3 Blank',\n",
       "  'transactions, the average latency is 3 hours. \\n \\n22limit 100;\\n1with transaction_1 as (\\n2  select *,\\n3         lag (status, 1) over (partition  by card_transaction_id order by created ) as status_previous\\n4  from public.card_transaction_snapshot\\n5),\\n6card_transaction_cte as (\\n7  select rank() over (order by created asc) as id,\\n8         *,\\n9         rank () over (partition  by card_transaction_id order by created asc) as rank,\\n10         case when created = first_value (created) over (partition  by card_transaction_id order by created desc r\\n11               else 0 end as current_ind\\n12  from transaction_1\\n13  where ((status <> status_previous ) or (status_previous is null))\\n14)\\n15select a.card_account_id as card_account_id ,\\n16       a .deserve_id as deserve_transaction_id ,\\n17       a .card_transaction_id as perpay_transaction_id ,\\n18       a .created as perpay_created_ts ,\\n19       a .amount as perpay_amount ,\\n20       a .tip_amount as perpay_tip_amount ,',\n",
       "  '100\\nTakeaways\\nComparing a full refresh that requires a recompute with a table that would perform a drop and create, it seems that a a full refresh is \\nmore expensive. It takes a few extra steps in modifying the view to use the new table that is created.\\nOverall, the paper is critical of MVs because of…\\nthe way refreshes interact with the VACUUM process\\nthe encoding choices that underly MVs\\nHis recommendation is to make precomputed tables manually, instead of using MVs.\\nHow does DBT implement the source layer?\\nDBT lays out a template, that we can adjust to our specific needs and bandwidth in our architecture. A foundational principle their approach \\nis built on is designing a trajectory where data goes from “source-conformed to business-conformed”. \\nDBT’s Staging Layer\\nAs described in this documentation page, their staging layer is designed to build the “atoms” from the raw sourced data that can be used to',\n",
       "  \"is null). The final table is saved to the temp_int schema, making temp_int.invoicing_docparser_output.\\n(3) check if build_matching_tables is true or false\\nif true (default)- we enter the block\\n1: Logs '[InvoiceMatching] Executing matching transformation queries, populating google sheet...'\\n2: Calls _execute_matching_queries. This creates an empty external table for the new entries in quickbooks. It’s called \\nperpay_accounting_datamart_ext.invoicing_new_quickbooks_entries\\n- Heads up, the table will throw a message every time explaining it won’t be created because it already exists. \\nThis function then creates the temp_int.invoicing_master_invoice and temp_int.invoicing_level_match tables in \\nredshift via dags/analytics/invoice_matching/lib/queries/generate_invoice_matching_query.py.  \\n- The invoicing_master_invoice table does string manipulation of the vendor data using the \\ntemp_int.invoicing_vendor_key_matching table (created in step 2.2) joined onto the\",\n",
       "  '108\\nHow do you handle upstream schema changes in your pipelines? \\nWhat is a Data Contract? Monte Carlo\\nWhat is a Data Contract? IBM\\nSchema Evolution on the Data Lakehouse Onehouse’s approach of handling upstream data changes - emphasis on backward-\\ncompatible queries\\nCrux Makes Dealing With Schema Changes Easy - are both proactive and reactive\\nInterfaces and Breaking Stuff - dbt’s founder shedding light on the importance of the conversation around the lack of contracts\\nSchema Evolution and Compatibility for Schema Registry on Confluent Platform | Confluent Documentation - offer a “Schema \\nRegistry” with the purpose of versioning schemas\\ndlthub \\n⭐ Schema Evolution | dlt Docs a well written out approach to schema evolution, and the tasks of numerical curation and business-\\ninfused structuring that data engineers have\\nGoogle Colab - DLT demo',\n",
       "  'The Unit Tests job runs tests on business logic to ensure nothing breaks during development.\\nAirflow Tests calls a shell script in the same location and executes like Unit Tests above, but parses for the prefix /airflowtest_. The \\ncurrent import error test collects the DAGs from our repo and to check their imports. A specific error is held out from the error \\nmessage: psycopg2.OperationalError: could not translate host name \"postgres\" to address: Name or service not known - this is thought \\nto just be a problem with Travis not accessing our database, so it is suppressed. \\nSyntax Tests calls flake8 directly from the terminal and passes in which specific flake8 tests we’d like to run. It does not run all flake8 \\nlinting since this would be awful. Below is a list of those that are used and are relevant for simple syntactical testing. They can be \\nexpanded upon in the future.\\n # F822: undefined name `name` in `__all__` \\n# F823: local variable name referenced before assignment',\n",
       "  '29       a.consumer_account_number,\\n30       date_of_account_info,\\n31       enter_date,\\n32       payment_rating,\\n33       payment_history_profile,\\n34       account_status,\\n35       credit_limit,\\n36       highest_credit_or_original,\\n37       terms_frequency,\\n38       actual_payment_amount,\\n39       current_balance,\\n40       amount_past_due,\\n41       original_charge_off_amount,\\n42       date_last_payment,\\n43       c.type,\\n44       d.min_enter_date as first_report_dt,\\n45       e.opt_in_dt as feature_opt_in_dt\\n46from perpay_risk_datamart.equifax_raw_data as a\\n47left join (select max(enter_date) as max_enter_date from perpay_risk_datamart.equifax_raw_data) as b on 1=1\\n48left join (select consumer_account_number, min(enter_date) as min_enter_date from perpay_risk_datamart.experian_\\n49left join (\\n50    select distinct borrower_id, b.type from public.feature_enrollment as a left join public.feature as b on b.i\\n51    ) as c on c.borrower_id = a.consumer_account_number\\n52left join (',\n",
       "  'price_special_from_date boolean Whether the product has a special price; \\ncurrently always False.\\nprice_special_to_date boolean Whether the product has a special price; \\ncurrently always False.\\nship_price double precision The shipping price of the product.Column Name Type Description',\n",
       "  '65\\nDate Closed date_closed Date the account was closed to further \\npurchases, paid in full, transferred or \\nsold. There may be a balance due.Date of TrueAccord debt \\nretraction if customer is \\ndeceased. If the account has \\ncharged off and has a 0 balance, \\nthe date the balance went to 0. \\nOtherwise, null (open account). \\nDate of Last \\nPaymentdate_last_paym\\nentMost recent date a payment was \\nreceived, full or partial.If the account is charged off and \\nthe borrower most recently paid \\nTrue Accord (after paying us), \\nthen the last payment date to TA. \\nOtherwise, the maximum not-null \\nlast_pmt_dt for the borrower.borrower_hist_d\\naily_status.last_\\npmt_dt\\nInterest Type \\nIndicatorinterest_type_in\\ndicatorF = Fixed, V = Variable/Adjustable \\ninterest.Blank  \\nReserved reserved2 Blank fill   \\nSurname surname Last name, cleaned for generation codes \\nand suffixes. user_attribute.la\\nst_name\\nFirst Name first_name First name, cleaned for titles and \\ngeneration codes. user_attribute.fir\\nst_name',\n",
       "  '267\\nMonthly Credit Reporting Rotation\\nBackground\\nData engineering supports Perpay’s credit reporting procedure by generating and sending Metro2 files to the three major credit bureaus and \\nassisting in resolving customer credit disputes related to data issues. Because Metro2 guidelines are ever-changing and new edge cases \\nare always popping up this DE responsibility is by nature never-ending.\\nCredit reporting also presents to Perpay “one of the the greatest existential risk centers“ out of all of Perpay’s procedures. Incorrect reporting \\ncan lead to endless disputes, impossible and expensive litigation, and being barred for being able to report credit. This would cripple a \\nvaluable aspect of Perpay. For that reason, it’s important that these DE responsibilities don’t get buried under everything else coming our \\nway. The monthly credit reporting rotation was set up to ensure this never happens. One DE member a month is responsible for credit',\n",
       "  '135\\nError explanation\\nThis is usually a timing issue! We run this job right after rec, where all the card payments are in the process of being created.\\nInvestigation\\nThe fact that payment_status and created_ts failed the test makes us think that this is a timing issue because we know both those fields \\ncomes from public.card_cardpaymentstatus in the creation of atomic_int.card_payment_status here.\\nNow that we have our hunch, let’s investigate further to confirm that’s the issue.\\nIn the card_payment_status.py file, we see that the CTE that captures the fields payment_status and created_ts from the public tables \\nis the following, so we run it in datagrip to see if a rerun will clear the issue (meaning it was indeed a timing problem):\\nIf nothing is returned, that means we were correct and it was timing!\\nIf this was not the case (which has never happened before), we need to investigate further by looking into the null fields to figure out what \\nwent wrong.',\n",
       "  '3. Type a lower case o then enter to add a line underneath and go into command mode.\\n4. Now you can copy and paste the key in there with more c/p keyboard commands\\n5. Exit and save via first entering command mode again with esc key\\na. Followed by :wq then hit enter\\n \\n1ssh-rsa AAAAB3NzaC1yc2EAAAADA <bunch of stuff> asADFks29= blakelaw@BlakesPerpayMBP.local.meter',\n",
       "  '205\\nIf they are not parsed at all, the invoice information will not be added to the docparser_invoices_master table.\\n4: .jpgs or .pngs cannot be processed in the first task in invoice_matching_v2 dag; only .pdfs may be processed (because that’s where data \\nis extracted in Docparser)\\nYou can set each parser to ingest specific extensions in Docparser. Just set it to ingest *.pdf only.',\n",
       "  '3rd Party Platforms44\\n           Third-Party API and Reporting Integrations46\\n                Automated Audience Management47\\n                DataX Reporting Service49\\n                Amplitude Marketing GET API53\\n                Facebook Marketing GET API54\\n                Google Marketing GET API55\\n                Accounts Payable Management56\\n                Credit Bureau Reporting59\\n                Iterable User and Catalog Batch Updates69\\n                QuickBooks GET API77\\n                Perpay Front-End Tracking and Marketing Stack78\\n                Domo Data Governace81\\n           AWS Glue, Redshift, and Our Data Processes83\\n           Versioning and Sunset Date85\\n           Monitoring our Infrastructure86\\n                DataDog Capabilities & Usages90\\n           AWS Data Migration Service (DMS)95\\n           The Source Layer96\\n                Handling Upstream Schema Changes - Considerations104\\n      Coding standards109\\n           SQL Queries110',\n",
       "  'schema with the table name invoicing_unparsed_<vendor> (i.e. temp_int.invoicing_unparsed_rymax). \\n- these three vendors are in our current sheet, so one table is created for each:\\n- BestBuy\\n- Rymax\\n- FragranceNet\\nAlso creates a Redshift table for the VendorKeyName sheet. This is saved to temp_int.invoicing_vendor_key_matching.\\n(if you want to look into the function that creates these tables, it’s all_invoicing_sheets_to_redshift in \\ndags/utils/lib/google_sheet_integration.py. It loops through the Non Parsed Partners sheet and calls the sheet_to_redshift \\nfunction to write its contents to the schema. The argument False is set to False so that the Non Parsed Partners google sheet \\ndoesn’t clear. If set to true, the sheets will be cleared; we never want this to happen, so we leave it as false)\\n3: We create empty external tables for the vendors, using the temp_int tables from step 2. These tables follow the same',\n",
       "  '398\\nPayments: Payments are the sum of amount in the platform_payments metric layer for the loan_id with a destination of \\nmarketplace and a status of completed on the day of the created_ts timestamp. Borrower credits are backed out of this amount and \\nstored in the separate borrower_credits column. The calculation for borrower credits is described below.\\nBorrower Credits: Borrower credits are sourced from the offers_borrowercreditamounthistory table. For any row in the table with a \\nstatus of used or partially_used and a starting_balance - ending_balance greater than zero, the amount equal to the \\nstarting_balance - ending_balance is considered to be a dollar amount of borrower credits applied to that loan_id on the date of the \\ncreated timestamp.\\nReturned Payments: Returned payments cannot be accurately calculated at the loan level because there is no way to reconcile historical',\n",
       "  \"116\\nLogic layout\\nBroader logic conventions\\n(1) Don’t compare Boolean values to True or False using the equivalence operator\\nIn python, an if will be entered if the condition is true. This is inferred; there is no need to check for the boolean value. \\n(2) Use the fact that empty sequences are false in if statements\\nThere’s no need to check the length of sequences in Python.\\nReferences\\nHow to Write Beautiful Python Code With PEP 8 – Real Python \\n \\n53                df = pd.read_csv (io.StringIO (response_data .content.decode('utf-8')))\\n54            else:\\n55                df = pd.DataFrame ()\\n56            if df.empty:\\n57                # If the CSV didn't return anything, log the error, raise an error.\\n58                logging .info('[Impact API] No data for actions. Please investigate!' )\\n59                raise Exception (f'[ImpactApi] Issue with response! No data returned for actions: {response_data .text}. URL: {url}')\\n60\\n61            # Rename columns\",\n",
       "  '530\\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  seg\\nmen\\nt_str\\nipe\\ncore\\n_sp\\nendi\\nng_l\\nimit\\ns_in\\nt\\nperp\\nay_a\\ncco\\nunti\\nng_\\ndata\\nmart\\n_ext\\nperp\\nay_\\nmar\\nketi\\nng_\\ndata\\nmart\\n_ext\\ndom\\no_e\\nxt\\nperp\\nay_r\\nisk_\\ndata\\nmart\\n_ext\\nacc\\nount\\ning_\\nwor\\nksp\\nace\\nperp\\nay_f\\npa_\\ndata\\nmart\\n_int\\nperp\\nay_e\\ncom\\nm_d\\nata\\nmart\\n_ext\\nperp\\nay_\\ndata\\n_go\\nvern\\nanc\\ne_ex\\nt\\nseg\\nmen\\nt_lo\\ngs',\n",
       "  \"16    # property 2 matches\\n17    {'restaurant_id' : 4, 'state': 'NJ', 'latitude' : 2, 'longitude' : 0},\\n18    {'restaurant_id' : 5, 'state': 'NJ', 'latitude' : 0, 'longitude' : 3},\\n19    # test state exclusion logic, this should not match any user even though\\n20    # within radius of properties 1 and 2\\n21    {'restaurant_id' : 6, 'state': 'WY', 'latitude' : 0, 'longitude' : 0},\\n22\",\n",
       "  'There was no use of the initiated_at fieldThis Confluence page tracks when the change over to the newer configuration occured:\\nACH Holdout Logic  \\nVariance 8: Processing Payment Timing\\n1: borrower_id = 23817 / card_account_id = 6123 / deserve_card_payment_id = d847f677-1d00-4fb4-84e2-23a5fd612da5\\nQuestion: (for Conor) According to deserve_payments_eod_report, this $750 payment was PROCESSING on 2023-08-17. Based on \\nour prior conversations, considering this happened post-May (in our current Deserve paradigm), the card account balance should have \\ndecremented on the 17th. However when we look at the account rollforward, it decrements on the 18th (along with another pending',\n",
       "  \"22left join perpay_general_datamart_int.user_attribute as e on d.borrower_id = e.borrower_id\\n23where  b.payment_id is null and -- indicates that the payment is missing from deserve sftp files\\n24       c.card_payment_id is not null and -- indicates that the payment is an ACH payment\\n25       a.created < current_date and a.created > '2022-11-03'-- when 1/1 relationship was created\\n26order by a.created desc\\n27limit 100;\\n1select case when account_id in ('149eaf20-8288-4ce3-87da-425b0ed06934', 'bf085378-a8ad-4244-b921-9f926d1f7416', \\n2          then 1 else 0 end as deserve_employee_ind,\\n3       a.id as perpay_id,\\n4       a.created,\\n5       a.amount as perpay_amount,\\n6       a.card_account_id,\\n7       e.email,\\n8       d.deserve_id as deserve_account_id,\\n9       a.status as perpay_status,\\n10       b.payment_id,\\n11       b.account_id,\\n12       b.payment_status as deserve_payment_status,\\n13       b.payment_amount as deserve_payment_amount,\\n14       b.completed_at as completed_at,\",\n",
       "  \"3select * from public.card_account_application where borrower_id = 3857277; --- also exists as we'd expect\\n4select * from public.card_card where card_account_id = 5078;\\n5\\n6-- looking at the enabled CTE\\n7select * from public.feature_enrollment where borrower_id = 3857277; -- DOES NOT EXIST! makes sense because borro\",\n",
       "  '461\\nVARIANCES - Accounts Receivables Reconciliation\\nGoal\\nThere are three daily tables to track account balances on our platform. There’s card_daily_rollforward, \\ncommerce_loan_daily_rollforward, and core_account_daily_rollforward, which track card account balances, marketplace account \\nbalances, and platform account balances, respectively. The goal of these tables is to tie out the account balances pulled from \\nengineering’s table with the balances that we calculate from an accounting standpoint. To keep track of balances that aren’t tied out, \\nwe have a variance column at the end of each table. We aim to work with engineering/product/accounting to get these as close to 0 \\nas we can.\\nThese balances are modified based on the flow of customer funds from the source (as initiated by the customer) all the way through the \\napplication. Thus, in a broader sense, each report seeks to track every day of an account’s active history and provide details on all',\n",
       "  'Engineering Users:  perpay_engineering\\n---\\nMarketing\\nMarketing Users: @Nikos Petrides \\n = Current, \\n = No Access\\nproduct_workspace\\n   \\n  \\n  \\ndeserve_data_int\\n   \\n  \\n  \\nshared_resources_int\\n   \\n  \\n  \\naccounting_workspace  \\n   \\n  \\nshared_resources_int  \\n   \\n  \\nSchema Product Users Accounting Users Engineering Users Marketing Users',\n",
       "  \"12\\nRollout Plan\\nDatahub is a new platform for Perpay, and Data Engineering has designed a rollout plan to enable a continuous release of feature-complete \\ncomponents. Due to Datahub's complexity and flexibility, it is imperative that only fully completed assets are made available to stakeholders. \\nThe goal of the rollout is to ensure that the UI remains as clean and feature-complete as possible at all times.\\nData Availability/Release in UI\\nData Engineering will enable only fully documented data assets. Therefore, the only data assets available in the UI will be those that have \\nbeen documented in their entirety, following the required procedures [here]. Due to the nested structure of relational data (schema > table > \\ncolumn), whenever a new table is ready to be incorporated into the UI, the schema-level documentation must also be completed. This \\nensures that stakeholders know that when they are looking at a data element, the parent assets have also been documented and are ready\",\n",
       "  '68\\nFor each month, the account status is calculated and the code for that month is calculated as follows:\\nWhen to Stop Credit Reporting:\\nThere are a few instances were we will stop reporting tradelines to the credit bureau. Below are the following instances:\\nMerged Accounts: Business Operations identify that two Perpay accounts are related and proceed to merge these accounts. The \\nmerged from account should be deleted and no further information reported\\nBlock Notifications: When a credit bureau verifies that a case of identity fraud, they issue a block notification which indicates that \\nblocked should not be re-reported. Steps should be taken to ensure that we no longer report blocked tradelines\\nBankruptcy Notices: When a bankruptcy notice is received, steps should be taken to stop reporting tradelines until the bankruptcy is \\ndischarged, dismissed or withdrawn. Reporting should resume upon a terminal state with the required amendments for compliance \\npurposes',\n",
       "  '166\\nProd Errors 2023-07-10 Onwards\\nThis page will allow us to keep track of the types of errors that fire and the frequency that they do for each week starting on 2023-07-10.\\nThis allows us to see how often an error or warning is sent each week and, at a glance, if it is firing across multiple weeks. Weeks start on a \\nMonday and end on a Sunday to align with our prod error fielding schedule. The name of the team members fielding the errors are also \\nunder each week title so that others may go back and ask the fielders questions if needed!\\nWeek of   -  \\nDerya & Ernest\\nWeek of   -  \\nAbby & Andrew\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nAndrew & Derya\\nWeek of   -  \\nAbby & Gabe\\nWeek of   -  \\nDerya & Hongkai\\nWeek of   -  \\nAbby & Andrew\\n \\nWeek of   -  \\nDerya & Gabe',\n",
       "  \"15  -- !!!!!!!!!!!!!! Unions with other events here\\n16  select user_id, created_ts, event from card_enabled\\n17    UNION\\n18  select user_id, created_ts, event from deserve_events\\n19    UNION\\n20  select user_id, created_ts, event from card_activation\\n21    UNION\\n22  select user_id, created_ts, event from first_card_payment\\n23    UNION\\n24  select user_id, created_ts, event from segment_events_userid\\n25    UNION\\n26  select user_id, created_ts, event from message_userid\\n27    UNION\\n28  select user_id, status_ts as created_ts, event from first_purchase\\n29),\\n30...\\n1...\\n2-- !!!!!!!!!!!!!!!!!!!!!!!!! This CTE captures enabled only\\n3enabled as (\\n4    select\\n5        distinct borrower_id     \\n6    from {{pub_schema}}.feature_enrollment\\n7    where feature_id = 67 and status = 'enabled' \\n8)\\n9....\\n10select\\n11    distinct\\n12    d.id as borrower_id,\\n13    d.user_id,\\n14    d.account_id,\\n15    coalesce(c.id, j.card_account_id) as card_account_id,\\n16    c.deserve_id as deserve_card_account_id,\",\n",
       "  '162                        \"visits\": 3\\n163                    },\\n164                    {\\n165                        \"category_path_deep\": \"fashion-beauty/personal-care\",\\n166                        \"category_path_1\": \"fashion-beauty\",\\n167                        \"category_path_2\": \"personal-care\",\\n168                        \"category_path_3\": NULL,\\n169                        \"category_path_4\": NULL,\\n170                        \"category_path_5\": NULL,\\n171                        \"category_path_6\": NULL,\\n172                        \"visits\": 2',\n",
       "  'wait actually, can you elaborate on\\nRows below are the history table for the principle balance of that loan. We should’ve cleaned up the last 4 rows when we deleted the \\npayment.\\nare there other relevant tables that are not adjusted when payments are undone?\\nNo, there shouldn’t be.\\nQuestion (4.2): ah okay, thanks for the clarity. Going to include accounting in this to understand if we need to track those payments in this \\ntable from their perspective ----- (For Accounting) From an accounting standpoint, is it necessary to track these ACH reversal cases in this \\ntable to maintain an accurate ledger of account balance between the date of deposit and the date of reversal? Or, can we act as if the \\ndeposit never occurred which aligns with what engineering currently does?\\nResponse (4.2): For accounting I think we are fine tracking as if it never occurred, as long as it clears within a few days.',\n",
       "  '320\\nThere are 92 (out of 35,550, ~ 0.25%) transactions with this latency since 10/17. Of all transactions with latency after 10/17 (the supposed \\nfix), the average latency is 33 hours. Of all transactions after 10/17, the average latency is 0.12 hours.\\nPerpay Deserve Data Issue # 5:\\nSometimes transaction show up as 2 separate transactions in the Deserve settled SFTP report, but show up as 2 transactions in the \\nDeserve cleared SFTP report and Deserve Perpay data.\\nTo find:\\nExample:\\nTransaction ID: 0c5df619-62c7-584e-978a-a229cb0bbd4d // 126915\\n \\nSub example (this is difficult)!\\nSFTP: 2 transactions on 8/25 (amount $12.10) and 8/26 (amount $105.58), same transaction ID. total: $117.68\\nDeserve Perpay Core: 1 transaction created 8/24, modified 8/26, amount $117.68. Only 08/26 row in card transaciton snapshot. \\nUser’s statement: 3 transactions, total: $117.68\\nSFTP Data Issue # 6:',\n",
       "  \"226\\nThe data in accounts_payable_int.po_matches can be sent to Quickbooks, but the data in accounts_payable_int.po_variances gets \\nplopped into the Google Sheet outlined in the next section.\\nManual\\nAs mentioned above, any records in the accounts_payable_int.po_variances table that failed the PO Matching checks, meaning there \\nwas a variance between the e-commerce and invoice data or there were mismatched SKUs or PO Numbers, or there’s a tax, gets put into a \\nGoogle Sheet. Here’s a link to the sheet: Manual PO Matching  . Reach out to Abby if you need access.\\nHere’s how it works:\\nEverything in the sheet needs to be manually addressed by accounting to determine whether or not we should pay the variance. The most \\nimportant information (vendor, totals, and variance amounts) are locked to the left side, with further information to the right, allowing anyone \\non accounting to scroll across to see where that variance may have occurred. The ready_for_qb column is a dropdown (so there's no\",\n",
       "  \"5cast(case\\n6 when usagetype like '%InstanceUsage%'  then 'Compute'\\n7 when usagetype like '%GP2%' then 'Storage'\\n8 else ''\\n9 end as varchar(100))as usage_type ,\\n10 totalcost:: float\\n11from users_herr .aws_bill_total\\n12where productcode = 'AmazonRDS'  and\\n13   linkedaccountname is not null and \\n14   linkedaccountname = 'Segment Account'\\n15   and totalcost > 0\\n16   and invoiceid is not null\\n17)\\n18pivot (sum(totalcost ) for usage_type in ('Compute' , 'Storage' )) \\n19order by vintage asc;\\n20\\n1-- EC2\\n2select * from (\\n3select\\n4concat(datepart (year, billingperiodenddate:: date), RIGHT('0' + CAST(datepart (month, billingperiodenddate:: da\\n5cast(case\\n6 when usagetype like '%BoxUsage%'  then 'Instance Provision'\\n7 else ''\\n8 end as varchar(100))as usage_type ,\\n9totalcost:: float\\n10from users_herr .aws_bill_total\\n11where productcode = 'AmazonEC2'  and\\n12   linkedaccountname is not null and \\n13   linkedaccountname = 'Production Account'\",\n",
       "  '126\\n(Error) Marketing Daily Row Count\\nIn the airflow_prod_alerts channel:\\nWhen you look into the DAG (Airflow) message:\\nError explanation\\nThis could be a…\\n(the latter being more likely)\\n \\nAssuming it is a result of a workflow change, this is what happened:\\nIf an email send is NOT a part of a workflow (a workflow being a series of emails sent to the same customers), then it is just a one-send \\nemail. Its workflow_id will be set to null (because, again, not a part of a workflow).\\nBUT sometimes marketing will want to add a one-send into a workflow. Thus, they will change the workflow_id to match the workflow id \\nof the other emails in that workflow.\\nnow we have an issue where the historically null workflow ids are still null (there was no backfilling done up to this point), but the future \\nones are fine. \\nHere’s where definitional drift comes into play and is good to be aware of',\n",
       "  'may not be worth the extra resources since it will pause the pipeline regardless, and why not just deal with it at the later point at which we \\ncurrently do? Unless it is critical for an issue to not pass the metric layer, it might be worth saving resources on adding more to the pipeline  \\nand hone in on building better commincation with our data providers.\\n \\nlinks (⭐  indicate they are cooler than the rest)\\nData Contracts And 4 Other Ways To Overcome Schema Changes Monte Carlo\\nHow to Think About Schema Changes | Blog | Fivetran \\n⭐ Implementing Data Contracts: 7 Key Learnings - this article outlines how a senior engineer at GoCardless went about \\nimplementing data contracts\\ndbt users how do you do handle upstream schema changes?',\n",
       "  '626\\nDatadog\\nhow we monitor using datadog and how to set up new monitoring?\\nmaking dashboards\\nhow to not spend all of DE’s budget on datadog',\n",
       "  'this time (account not opened)\\nD = No payment history available this \\nmonth.\\nE = Zero balance and current account \\n(only applied to credit cards and lines of \\ncredit)\\nH = Foreclosure completed\\nJ = Voluntary surrender\\nK = ReposessionBased on current definitions, \\nnever gets to 5 or 6. Based on \\nthe account status field at point \\nin time. If the account is not \\nactive on a particular month, the \\npayment rating for the month is \\nbased on the last known \\nborrower status. See further \\nclarification below.',\n",
       "  'monthly_fee_adjustment field uses its join condition to only bring in the rows in this card_transactions CTE where \\ntransaction_type is MONTHLY_FEE_ADJUSTMENT.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:\\nThe sum of all LATE_PAYMENT_FEE transactions associated with a given card account on a given src_dt. The transactions are \\nassociated with the date in which the transaction reaches a SETTLED status.\\nBusiness Context:\\nA transaction in a SETTLED state is as defined by Deserve. \\nRollforward Table Logic:\\nAll transactions that have reached a SETTLED status are first pulled into the card_transactions CTE here. These transactions are \\nlater filtered down in their own respective join conditions by transaction_category for the construction of the final table. The',\n",
       "  'transaction_type is ACCOUNT_OPENING_FEE_ADJUSTMENT.\\nSource Tables:\\nThis data is sourced from atomic_int.card_transaction which pulls data from both the public.card_transaction and the \\ndeserve_data_int.deserve_daily_settled_transactions_report tables.\\nDefinition:',\n",
       "  '9\\n10     filtered_vantage_scores as (\\n11        select \\n12            vs.borrower_id,\\n13            vs.monthly_score_change,\\n14            vs.cumulative_score_change\\n15        from {{metrics_schema}}.platform_vantage_scores vs\\n16        left join {{pub_schema}}.feature_enrollment fe on vs.borrower_id = fe.borrower_id\\n17        where vs.latest_score_ind = 1 and vs.enabled_ind = 1 and fe.feature_id in (35, 133))\\n18...',\n",
       "  'drop the temp_int.invoicing_docparser_output table.\\n \\nIf you ran Job 1 and Job 2 and then they notice there was a mistake in the Non Parsed Partners sheet, you’ll need to remove that \\ndata from both those sheets AND from the perpay_accounting_datamart_ext.invoicing_docparser_output table (because it \\ngets added to this external table in Job 2)\\nif you remove data from the external table, ALWAYS copy it to your users schema first\\nit’s also good to copy the Google sheets data over to a different file for version control\\nPut simply, this job takes new data in from the Non Parsed Partners Google sheet (BestBuy, Rymax, FragranceNet), \\nappends it to our new V2 vendor data, reformats it, adds a ready_for_qb column to it, and then plops it into the Invoice \\nMatching Google sheet (in the DoInvoiceMatching sub sheet). More specific info is below:',\n",
       "  'Card Borrower Performance\\nLIke card user fact, card user detail and card borrower history daily fact, this affects card borrower performance via the card enabled \\ntimestamp date. Here it’s used to compare against the marketplace date and determine if they were a marketplace borrower at the time of \\ncard enablement.\\n12        cdt.marketplace_borrower_at_enabled_ind,\\n13        di.src_dt,\\n14        round((di.src_dt - cast(cdt.provisioned_at AS date))/30,0) AS months_on_book,\\n15        di.src_dt - cast(cdt.provisioned_at AS date) AS days_on_book,\\n16        gcai.staff_ind\\n17    FROM \\n18        card_driver_table AS cdt\\n19        -- !!!!!!!!!!!!!!!!! Left joined- not limiting the population.\\n20        LEFT JOIN {{pub_schema}}.feature_enrollment AS fe ON cdt.borrower_id = fe.borrower_id AND fe.feature_id \\n21        LEFT JOIN {{report_schema}}.date_identity AS di ON di.src_dt >= cast(cdt.provisioned_at AS date) AND di.',\n",
       "  '505\\nCurrent Variances - Specific Cases\\nThese are examples tied to variances that have not been resolved. At max, we’ll attach two examples per variance, unless asked for more. \\nEach variance is numbered and matches the number in the overview table above. \\nVariance 1: Null Source Dates\\nThese borrowers have no src_dt in the rollforward table. Because all have variances, I believe this is because we can’t tie out the account \\nbalance correctly with null dates. Will dig further into the code before asking any specific question to another team, but logging these done to directly resolve it, other \\nthan engineering going in and \\nmanually updating the rows \\n(which is not preferred). \\n*Side: there are two cases that \\nmagically resolved that we need \\nto look into. These were cases in \\nthe list I gave Talha. This PR \\nhoped to resolve one, but when I \\ntested it, the initial variance was \\ngone (this list was from variance 5 \\nbut I’m adding the message here \\nas it aligns more with 11).',\n",
       "  \"27    e.enabled_ts as plus_enabled_ts\\n28from {{temp_schema}}.borrower_attribute a\\n29left join (select max(src_dt) as max_src_dt from {{report_schema}}.borrower_hist_daily_status) as d on 1=1\\n30left join {{report_schema}}.borrower_hist_daily_status b on a.borrower_id = b.borrower_id and d.max_src_dt = b.s\\n31left join {{temp_schema}}.borrower_attribute_1 c on a.borrower_id = c.borrower_id\\n32left join (\\n33    select a.borrower_id, min(a.created) as enabled_ts\\n34    from {{pub_schema}}.feature_history as a\\n35    --- !!!!!!!!!!!!!!! Uses feature_enrollment and feautre tables\\n36    left join {{pub_schema}}.feature_enrollment as c on c.id = a.feature_enrollment_id\\n37    left join {{pub_schema}}.feature as b on c.feature_id = b.id\\n38    where a.status = 'enabled'\\n39    and b.type in ('perpay_plus', 'perpay_plus_v2')\\n40    group by 1\\n41  ) as e on e.borrower_id = a.borrower_id\\n42left join (\\n43    select a.*\\n44    from {{pub_schema}}.feature_enrollment as a\",\n",
       "  'inquiries about specific accounts.\\nSource Tables:\\npublic.user\\nDefinition: \\nThe observation date associated with the row. Not the same as observation_day.\\nBusiness Context:',\n",
       "  '135\\nError explanation\\nThis is usually a timing issue! We run this job right after rec, where all the card payments are in the process of being created.\\nInvestigation\\nThe fact that payment_status and created_ts failed the test makes us think that this is a timing issue because we know both those fields \\ncomes from public.card_cardpaymentstatus in the creation of atomic_int.card_payment_status here.\\nNow that we have our hunch, let’s investigate further to confirm that’s the issue.\\nIn the card_payment_status.py file, we see that the CTE that captures the fields payment_status and created_ts from the public tables \\nis the following, so we run it in datagrip to see if a rerun will clear the issue (meaning it was indeed a timing problem):\\nIf nothing is returned, that means we were correct and it was timing!\\nIf this was not the case (which has never happened before), we need to investigate further by looking into the null fields to figure out what \\nwent wrong.',\n",
       "  '248\\nRequest for Perpay Plus Feature Enrollment Cancellation\\nBefore updating the Perpay Plus status of a user we much check where they are in the reporting pipeline because that status dictates the \\navailable actions. The Perpay Plus statuses can be found here.\\nThere will be Trello cards created by Ops of users requesting to cancel Perpay Plus. Most of these will be users who opted in to Perpay Plus \\non accident; we want to keep the option to opt back into Perpay Plus open if the user has not been already reported. There are two courses \\nof action:\\n1. If a user is already reported, we must cancel the enrollment.\\n2. If a user has not been reported, we can delete the enrollment as if it never happened\\nEngineering will ask DE to check if the user is reported. The action here is to run the query bat the bottom of this page. If there is data \\nreturned, advise the following action…\\nIf Data is returned for the borrower\\nEnrollment cancellation\\nNo Data is returned for the borrower',\n",
       "  '566\\nHow do you ensure recovery of the platform?\\nWe have a technically proficient analyst group - how do you enable them to contribute but not open the platform to errors?\\nHow do you design for scalability?\\nWe also have a technical (or at least interested) leadership - how would you describe the data architectures and capabilities to them?\\nCapabilities questions\\nEventually we want to incorporate a streaming paradigm to our data warehouse, what are some considerations and tooling?\\nWhat should be done if the flow of data decreases or goes beyond platform capabilities?\\nIntern\\nInitial phone screen (30 minutes)\\nWhy Perpay?\\nExperience\\nWhy DE? What is exciting? What do they like working on? What aspect of DE do they like the most?\\nArchitecture/Infrastructure/Product/ect\\nAre they familiar with AWS cloud? Airflow/scheduling?\\nPython/SQL?\\nOn-site interview\\nTechnical\\nWhat do you think the most important skills are for a data engineer?\\nWhen should a task be handled by python verses SQL?',\n",
       "  '32\\n2023 Q4 OKRs\\n Timeline\\nRelated pagesTeam\\nOwner:\\nEnd-of-quarter objective \\nscore:0.0-1.01.0 Month 1\\nMonth 2\\nMonth 3\\n     \\n     \\n      \\n     \\n     \\n      \\n     \\n     Objectives Key results Owner Partner \\nwithExpected \\nEoQ key \\nresult \\nscoreCurrent status\\nFor OKR pro tips from Atlassian teams visit:https://www.atlassian.com/team-playbook/plays/okrs',\n",
       "  '461\\nVARIANCES - Accounts Receivables Reconciliation\\nGoal\\nThere are three daily tables to track account balances on our platform. There’s card_daily_rollforward, \\ncommerce_loan_daily_rollforward, and core_account_daily_rollforward, which track card account balances, marketplace account \\nbalances, and platform account balances, respectively. The goal of these tables is to tie out the account balances pulled from \\nengineering’s table with the balances that we calculate from an accounting standpoint. To keep track of balances that aren’t tied out, \\nwe have a variance column at the end of each table. We aim to work with engineering/product/accounting to get these as close to 0 \\nas we can.\\nThese balances are modified based on the flow of customer funds from the source (as initiated by the customer) all the way through the \\napplication. Thus, in a broader sense, each report seeks to track every day of an account’s active history and provide details on all',\n",
       "  \"276\\n \\n \\n \\nThis (less passing) query uses the deserve_payments_eod sftp for card payments:\\n22        SUM(a.amount) AS payments\\n23    FROM\\n24        atomic_int .platform_payments a\\n25        INNER JOIN random_sample b\\n26            ON a.borrower_id = b.borrower_id\\n27    WHERE\\n28        a .status IN ('completed' )\\n29        AND a.destination_rsn_cd = 0\\n30        AND convert_timezone ('UTC','EST',created_ts )::date between [Quarter start dt] and [Quarter end dt]\\n31    GROUP BY 1\\n32),\\n33-- Capture all rewards that were redeemed for the borrowers in Quarter\\n34rewards_redeemed_unsummed as (\\n35    SELECT\\n36        b .borrower_id ,\\n37        d .email,\\n38        a .account_id ,\\n39        a .sftp_dt,\\n40        a .rewards_redeemed_usd\\n41    FROM\\n42        deserve_data_int .deserve_rewards_daily_summary_report a\\n43        INNER JOIN random_sample b on a.account_id = b.deserve_card_account_id\\n44        LEFT JOIN public.borrower c on b.borrower_id = c.id\",\n",
       "  \"When a table is soft-deleted, you can't search for it in this search function - one way to manipulate the search space is to pass in a certain \\nRemovedStatusFilter, which is a Python class in DataHub used to filter search (source code). It can search for only hidden entities, only \\nshown entities, or all entities. This is configurable in some parts of the DataHub utility and sent through to the search function. By default, \\nonly shown assets are searched. However, in the frontend the data and landing pages are not lost when an asset is hidden. You can \\ngenerate a URL for any entity using the utility, and then send that to whoever needs to edit the documentation. Only then would we want to \\nrelease the asset and unhide it for all to see. \\nLooking Forward\\nGoing forward, we need to check out stateful ingestion. If it keeps our old UI state and only updates the tables, that's perfect. There is likely\",\n",
       "  '118 \"created\": \"2019-11-29\",\\n119 \"shipping\": 5.00,\\n120 \"tax\": 0.0,\\n121 \"items\": [{\\n122 \"product_id\": 77789,\\n123 \"price\": 49.99\\n124 }]\\n125 },\\n126                ],\\n127                \"benchmarks\": {\\n128                    \"app_start\": {\\n129                        \"count\": 5,\\n130                        \"ind\": 1,\\n131                        \"first_dt\": \"2019-01-05\"\\n132                    },\\n133                    \"app_complete\": {\\n134                        \"count\": 5,\\n135                        \"ind\": 1,\\n136                        \"first_dt\": \"2019-01-05\"\\n137                    },\\n138                    \"awaiting_pmt\": {\\n139                        \"count\": 3,\\n140                        \"ind\": 1,\\n141                        \"first_dt\": \"2019-01-05\"\\n142                    },\\n143                    \"repayment\": {\\n144                        \"count\": 3,\\n145                        \"ind\": 1,\\n146                        \"first_dt\": \"2019-02-01\"\\n147                    }\\n148                }',\n",
       "  '286\\nbefore their payment was due, but because the payment was initiated before the due date and not completed before the due date, the \\npayment could still be considered late. \\nPayments completed before 2023-05-08 are sourced from Deserve SFTP data. In most cases, completed payments can be identified by a \\nstatus of COMPLETED. Unfortunately, there is a good size chunk of exceptions to this rule that also have to be accounted for. Before 2023-\\n05-08, engineering would ping Deserve to create the payment after the payment was completed. In some cases, this would also be after the \\npayment was returned as well. In these cases, there is no Deserve SFTP data indicating that the payment was completed and only data \\nindicating that the payment was returned. With these payments, there will exist data with a RETURNED status, a non-null completed_at \\ntimestamp, and no corresponding data with a COMPLETED status.',\n",
       "  'connecting to Airflow. Removing this variable messes up our Airflow configuration within Travis - \\nairflow.exceptions.AirflowConfigException: error: sqlite C library version too old (< 3.15.0). \\nbefore_install\\nThis section serves to prepare the environment for testing. We have a couple of pip install statements here for tests as well as a \\nchmod +x to allow execution access to Travis for the airflow test shell script.\\njobs\\nEach of these jobs runs in parallel has a name: Unit Tests, Airflow Tests, Syntax Tests. Each job shows up in Travis separately, \\nmeaning they test individually, and they work by running a line of shell script. If any of these fail, the whole build fails, and they should \\nbe addressed before continuing.\\nUnit Tests calls a shell script within our repo under /script/. This files runs pytest on a group of files fitting the prefix /pytest_. \\nThe Unit Tests job runs tests on business logic to ensure nothing breaks during development.',\n",
       "  'withdrawals_initiated + refund_marketplace_as_cash + refund_card_balanace + deposit_ach_pending - \\ndeposit_ach_returned + deposit_check_completed + deposit_collections_pending + deposit_credit_card_completed + \\ndeposit_direct_deposit_completed - deposit_direct_deposit_returned - payment_credit_card_initiated + \\npayment_credit_card_returned + payment_marketplace_canceled - payment_marketplace_completed + \\npayment_marketpalce_returned - payment_perpay_plus_completed for the merge cluster.\\nEnding Balance: Calculated as the day’s starting balance added to the merge cluster’s total movement for the given day.',\n",
       "  'b. Card View: counted each time a user clicks into the Details view for a card\\nAd-Hoc Domo Governance DAG:\\n1. Internal governance data is generated by Domo’s “Governance Datasets” \\n2. This data is pulled into redshift from Domo when the DAG is run (by manual trigger), and then formatted to be useful\\n3. Resulting table contains a list of redshift columns in the Domo schema that are not used in Domo\\nTables Created:\\nDomo Governance Columns (domo_gov_columns_filter)\\n \\nDomo Governance Datasets (domo_gov_datasets_full):redshift_table_name character varying The name of the table in redshift \\nwhere the column is found\\nredshift_column_name character varying The name of the column in \\nredshift\\ndomo_dataset character varying The name of the dataset in Domo \\nwhere the column is used, if one \\nexists\\ndataset_in_domo_ind integer A flag indicating whether the \\nredshift dataset is used in DomoVariable Name: Data Type: Description:\\nVariable Name: Data Type: Description:',\n",
       "  \"'6756867', \\n'7010324', \\n'8389679', \\n'2301532', \\n'2759037', \\n'4140034', \\n'6664947', \\n'286724', \\n'1264429', \\n'1622835', \\n'7462547', \\n'102546', \\n'8487324', \\n'5592132', \\n'6267517', \\n'6719828', \\n'8535260'     \\nandI know about the \\nissue and will reach \\nout to engineering \\nabout it on monday. \\nSo, I don’t \\nwant/need the error \\nto fire every hour \\nuntil then. This \\nprevents the test \\nfrom firing for these \\ncurrently erroneous \\nborrowers.  pt 1: \\nhttps://github.co\\nm/Perpay/perpa\\ny-\\nairflow/commit/\\na6871d9b14f40\\n99184953617c\\n4879974d71e5f\\n7f \\npt 2: \\nhttps://github.co\\nm/Perpay/perpa\\ny-\\nairflow/commit/\\n733b9ba52cd9\\n35e1ba494fb80\\n7f23f19bb7cde\\n6f   \\n1'6515138',\\n2'6871272',\\n3'6389575',\\n4'2718898',\\n5'8155771',\\n6'6807485',\\n7'3425035',\",\n",
       "  \"values on a monthly basis. These are pretty much sourced directly from the magento_sales_flat_order_item and refund tables, then \\nincorporated into our refunds data model, and finally presented in DOMO.\\nIn  contrast, the Commerce Loan Dailly Rollforward table focuses exclusively on the portions of refunds that contributed to reducing the \\nremaining loan balance on a per-loan basis. Since this analysis is conducted at the loan level, specific to each loan's balance on a given \\nday, the monthly aggregated total will naturally be lower when compared to the previous dataset.\\nTo provide further clarity, I'll use loan_id = 5304464, charge_id = 5304311 as an example: \\ncharge_5304311_eng.csv is straight from engineering's table, we see the refund on 2023-09-29 was $812.97. And when we look at our \\ntable,loan_5304464_commerce.csv , we have a lower refund amount of $733.27. \\nin this case:\\n1: starting that day: the balance was $771.62\",\n",
       "  '254\\nCredit Dispute Process\\nOverview:\\nA user may feel that information that Perpay has reported to the credit bureaus is inaccurate and will choose to file a dispute with the \\nbureaus. This document outlines the process by which Perpay receives and responds to disputes.\\nAt a high level, the dispute resolution process works as follows:\\nData engineering checks for new credit disputes each week in the Metro 2 compliant, dispute resolution system e-Oscar. \\nData engineering queries the data warehouse for quality assurance, confirming that all information that was reported to the bureaus it \\nstill accurate. This information is then conveyed to our Operations team.\\nThe Ops Team then performs further due diligence by viewing conversation history with the user for additional context. Operations \\ncontacts the customer with the outcome of the dispute. \\nFinally, Data Engineering responds to the dispute within e-Oscar.',\n",
       "  'transactions, the average latency is 3 hours.\\n \\n22limit 100;\\n1with transaction_1 as (\\n2  select *,\\n3         lag(status, 1) over (partition by card_transaction_id order by created) as status_previous\\n4  from public.card_transaction_snapshot\\n5),\\n6card_transaction_cte as (\\n7  select rank() over (order by created asc) as id,\\n8         *,\\n9         rank() over (partition by card_transaction_id order by created asc) as rank,\\n10         case when created = first_value(created) over (partition by card_transaction_id order by created desc r\\n11               else 0 end as current_ind\\n12  from transaction_1\\n13  where ((status <> status_previous) or (status_previous is null))\\n14)\\n15select a.card_account_id as card_account_id,\\n16       a.deserve_id as deserve_transaction_id,\\n17       a.card_transaction_id as perpay_transaction_id,\\n18       a.created as perpay_created_ts,\\n19       a.amount as perpay_amount,\\n20       a.tip_amount as perpay_tip_amount,\\n21       a.status as perpay_status,',\n",
       "  \"27    e.enabled_ts as plus_enabled_ts\\n28from {{temp_schema}}.borrower_attribute a\\n29left join (select max(src_dt) as max_src_dt from {{report_schema}}.borrower_hist_daily_status) as d on 1=1\\n30left join {{report_schema}}.borrower_hist_daily_status b on a.borrower_id = b.borrower_id and d.max_src_dt = b.s\\n31left join {{temp_schema}}.borrower_attribute_1 c on a.borrower_id = c.borrower_id\\n32left join (\\n33    select a.borrower_id, min(a.created) as enabled_ts\\n34    from {{pub_schema}}.feature_history as a\\n35    --- !!!!!!!!!!!!!!! Uses feature_enrollment and feautre tables\\n36    left join {{pub_schema}}.feature_enrollment as c on c.id = a.feature_enrollment_id\\n37    left join {{pub_schema}}.feature as b on c.feature_id = b.id\\n38    where a.status = 'enabled'\\n39    and b.type in ('perpay_plus', 'perpay_plus_v2')\\n40    group by 1\\n41  ) as e on e.borrower_id = a.borrower_id\\n42left join (\\n43    select a.*\\n44    from {{pub_schema}}.feature_enrollment as a\",\n",
       "  'The infrastructure created in the past (glue.tf) is built under the assumption that there is a singular Redshift Spectrum bucket, and \\nrelies on the Redshift Spectrum bucket as an output. However, since the Segment bucket exists outside of the Redshift Spectrum \\nbucket (and outside of the Redshift Spectrum module), the Redshift Spectrum module can be modified to accommodate for a more \\nflexible infrastructure. We need to use the glue_crawler_role_arn  that is created in the Redshift Spectrum module - if we made \\nthis role more general, then we wouldn’t need to add this glue crawler in the Redshift Spectrum module. \\n Once that is separated, then you could put this glue crawler in the  3-segment module, so that you could programmatically reference \\nthe s3 bucket. \\nStep 6: We should ultimately have this happen via Terraform when we are adding new schemas via Glue Crawlers in terraform. \\nStep 8: There is lots of room for improvement here. \\n1create external schema segment_logs',\n",
       "  'aware. Virtually every company has a different data contract or connectors so each additional 3rd party we on-board typically requires a \\ncurated solution. However, recently we have adopted a few platforms to facilitate inputting and outputting data and these platforms should \\nbe leveraged at much as possible. This work is performed mainly in python.\\nData Models\\nThe core product that DE supplies is our data warehouse. This highly validated and tested data is served to all domains within the business; \\nfrom marketing to the leadership. In order to keep definitions updated, supply departments with required data, support new initiatives, \\nconstant work is required within the DAGs, tasks, and scheduler so that all SLAs and needs are met. These projects also require domain \\nknowledge (either innately or provided by the stakeholder) and an analytical eye in order to build and trouble shoot queries results. This \\nwork is performed mainly in SQL.\\nGeneral Maintenance',\n",
       "  \"Depending on which column the error was caught on (clicks, impressions, reach), that’s the table you look into.\\nfor instance, if it’s impressions:\\n35         left join perpay_marketing_datamart_ext .iterable_daily_insights as b on b.campaign_id = a.campaign_id a\\n36where a.platform = 'Iterable' ;\\n37\\n38select\\n39    a.join_key ,\\n40    a.src_dt,\\n41    e.join_key ,\\n42    e.src_dt,\\n43    e.campaign_id ,\\n44    e.adset_id ,\\n45    e.ad_id\\n46    count(*) as cnt\\n47from perpay_general_datamart_int .marketing_daily_fact a\\n48         inner join temp_dev_int .marketing_daily_source_metrics_iterable_2 e on a.platform = e.platform and a.jo\\n49group by 1,2,3,4,5,6\\n50having cnt > 1;\\n1select *\\n2from perpay_marketing_datamart_ext .iterable_daily_insights\\n3where ad_id = <the ad id > and adset_id = <the adset id > and action_date = <the action date>\\n1--- table 1\\n2select *\\n3from perpay_marketing_datamart_ext .iterable_daily_insights\\n4where ad_id = 7430126 and adset_id = 5496568 and action_date = '2022-12-05'\",\n",
       "  'Tracking\\ncharge_off_settlement Numeric(\\n38,2)Sum of transactions of type \\nCHARGE_OFF_SETTLEMENT for the card \\naccount on the observation dateTransaction \\nTracking\\ncourtesy_credit Numeric(\\n38,2)Sum of transactions of type \\nCOURTESY_CREDIT for the card account \\non the observation dateTransaction \\nTracking\\nprincipal_debit_adjustment Numeric(\\n38,2)Sum of transactions of type \\nPRINCIPAL_DEBIT_ADJUSTMENT for the \\ncard account on the observation dateTransaction \\nTracking\\nmerchant_cash_back Numeric(Mastercard rewards for using the card with Rewards',\n",
       "  '28    -- Card Activation\\n29    y.first_event as first_activation_submit_last_four_ts,\\n30    y.event_count as activation_submit_last_four_count,\\n31    z.first_event as first_activation_success_ts,\\n32    z.event_count as activation_success_count,\\n33    aa.first_event as first_activation_click_done_ts,\\n34    aa.event_count as activation_click_done_count\\n35-- !!!!!!!!!!!!!!!!!!!!!! The CTE above is a base table for the final report creation.\\n36from report_product_card_activation a\\n37left join (select user_id, min(timestamp) as first_event, count(user_id) as event_count from {{segment_schema}}.\\n38--left join (select user_id, min(timestamp) as first_event, count(user_id) as event_count from {{segment_schema}\\n39left join (select user_id, min(timestamp) as first_event, count(user_id) as event_count from {{segment_schema}}.\\n40left join (select user_id, min(timestamp) as first_event, count(user_id) as event_count from {{segment_schema}}.',\n",
       "  '68\\nFor each month, the account status is calculated and the code for that month is calculated as follows:\\nWhen to Stop Credit Reporting:\\nThere are a few instances were we will stop reporting tradelines to the credit bureau. Below are the following instances:\\nMerged Accounts: Business Operations identify that two Perpay accounts are related and proceed to merge these accounts. The \\nmerged from account should be deleted and no further information reported\\nBlock Notifications: When a credit bureau verifies that a case of identity fraud, they issue a block notification which indicates that \\nblocked should not be re-reported. Steps should be taken to ensure that we no longer report blocked tradelines\\nBankruptcy Notices: When a bankruptcy notice is received, steps should be taken to stop reporting tradelines until the bankruptcy is \\ndischarged, dismissed or withdrawn. Reporting should resume upon a terminal state with the required amendments for compliance \\npurposes',\n",
       "  \"Companies use Snowflake as a final desination for all data, serving as a “Data Lakehouse” solution. This could be positive for our data \\nanalysts to have access to all data in one place if we so wanted.\\nhttps://www.snowflake.com/blog/beyond-modern-data-architecture/\\nRedshift Spectrum handles concurrency scaling better than Redshift, one of our biggest bottlenecks:\\nhttps://aws.amazon.com/about-aws/whats-new/2020/04/announcing-cost-controls-for-amazon-redshift-spectrum-and-concurrency-\\nscaling/#:~:text=Amazon Redshift Spectrum enables you,with consistently fast query performance.\\nTechnical resources:\\nIntroduction of Databricks' Delta Lake:\\nhttps://databricks.com/blog/2020/11/12/announcing-the-launch-of-sql-analytics.html\\nExamples for how to connection S3 as a source or destination for DMS, and how to query S3 as a data lake:\\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.S3.html\",\n",
       "  '24        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status b ON a.borrower_id = b.borrower_id AND cast\\n25        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status c ON a.borrower_id = c.borrower_id AND cast\\n26        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status d ON a.borrower_id = d.borrower_id AND cast\\n27        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status e ON a.borrower_id = e.borrower_id AND cast\\n28        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status f ON a.borrower_id = f.borrower_id AND cast\\n29        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status g ON a.borrower_id = g.borrower_id AND cast\\n30        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status h ON a.borrower_id = h.borrower_id AND cast\\n31        LEFT JOIN {{card_schema}}.card_borrower_history_daily_status i ON a.borrower_id = i.borrower_id AND cast',\n",
       "  'Creating materialized views in Amazon Redshift\\nAdvantages of materialized views\\nWhy Use a Materialized View?\\nExploring Data Objects: Advantages and Disadvantages tables, materialized views and views\\nMaterialized Views, a memory-performance trade off\\nWhat you can(’t) do with Materialized Views in Amazon Redshift\\nREFRESH MATERIALIZED VIEW\\nRefreshing a Materialized View\\nRedshift MV vs Table\\nHow do I resolve the \"Underlying table with oid of view does not exist\" error that I receive when refreshing Redshift materialized views?\\nAWS docs page\\nThis AWS doc\\nRedshift Research Project by Max Ganz II - Materialized Views\\nDBT’s implementation\\nHow we structure our dbt projects\\nStaging: Preparing our atomic building blocks\\nDBT’s redshift codebase implementation of materialized views\\nOptimizing Materialized Views with dbt\\ndbt Models — Staging layer',\n",
       "  '229\\ninformation to the account associated with the specified Vendor, appropriately tagged with its respective Category. This POST request differs \\nbased on whether the invoice number associated with the line item already exists in QB or not.\\nIf the invoice number exists in QB already, meaning it’s in existing_bills_for_qb_upload…\\nThe line item gets appended onto the bill that already exists.\\nIf the invoice number does not exist in QB yet, meaning it’s in new_bills_for_qb_upload…\\nThe bill gets uploaded as an entirely new invoice.\\nBill payments\\nOnce the line items are uploaded to QB, accounting uses an external tool to pay the bills. More info can be found in this doc [*Insert \\nAccounting/DE shared doc here*]. As a reminder, they cannot pay partial bills (e.g. a bill where it’s line items were split across).\\nPost-upload\\nOnce the line items are sent to QB, the accounts_payable_int.po_matches table contents are inserted into the',\n",
       "  '139\\nFigure out the card_account_id where there are nulls in card_balance:\\nFor the case above, we see the nulls occurred where the card_account_id = 5078. \\nRunning easy_render on card_balance.py gets you the query to build the table. You can try to see which CTE renders the null based on the \\ncard_account_id and work backwards from there:\\nFor this case, the issue resided in id_xwalk, so we know there’s something missing in atomic_int.card_key_relations. Let’s dig into that table \\ncreation query (easy render on card_key_relations.py):\\n1select * from temp_int.card_balance where user_id is null;\\n1-- Check the first CTE\\n2with\\n3balance_1 as ( --get credit limits\\n4    select\\n5        card_account_id ,\\n6        created as created_ts ,\\n7        current_balance ,\\n8        lag (current_balance , 1) over (partition  by card_account_id order by created ) as current_balance_previous\\n9    from public.card_account_snapshot\\n10)\\n11select * from balance_1 where card_account_id = 5078;\\n12\\n13',\n",
       "  '374\\nWhile these last parts of the framework are the same, there’s a lot of additional testing I want to do in staging before we deploy into prod. I \\nwant to clean up the invoice matching sheet to align with Preston’s exact needs, and update the code so it guarantees it’s pulling in the \\ncorrect pieces of the sheet to Quickbooks when there are Ys.\\n \\nNew process (looks kinda dinky sorry, draw.io is difficult for me- feel free to ask me any Qs): \\nSome other tools to consider 👀 :\\nAccounts Payable Automation Platform \\nDocparser Alternatives: GPT Document Extraction Platform',\n",
       "  '16\\n \\nThe Airflow push problem outlined above stems from Airflow pushing table names to the platform when it parses our SQL queries. In the first \\ncase outlined above, we get existing Redshift tables that get unhidden from the UI. These tables are already properly stored in DataHub as \\na dataset entity, so they exist within the perpay database’s container and their schema’s container. In the second case outlined above, those \\ntemporary tables don’t exist and we don’t usually care much about them. They aren’t properly stored in DataHub and will have significantly \\nfewer attributes than those from the first scenario. They are still stored as entities as type dataset, so we can filter them out, but they are not \\nstored under the same ‘perpay’ database container as the others. In the UI, this manifests as two folders that say ‘perpay’ as if there’s two',\n",
       "  'ID). Below are the recommended elements to include in Glossary entries while the normal additional owner and tags values can be selected \\nas well.\\n1. Description (required)\\na. A short to medium form description of the term and how it pertains to Perpay. What are its rules? When does it change? What affects \\nit?\\n2. Project Links\\na. Please add links to any relevant projects that provides greater detail on the term. And example is the underwriting risk score where a \\nlink to the data science project documentation would be natural to add so that the user can have more background on the concept if',\n",
       "  '2WITH\\n3    feature_enabled as (\\n4        SELECT\\n5            borrower_id,\\n6            min(created) as first_card_enabled_ts\\n7        FROM {{pub_schema}}.feature_enrollment\\n8        WHERE feature_id = 67\\n9        GROUP BY borrower_id\\n10    )\\n11    SELECT',\n",
       "  'loan balance. Refund amounts prior to the refund being observed are summed and considered to be existing refunds for the loan that are \\nalso deducted from the loan balance.\\nOne added filter applied to the refund tracking is to adjust for engineering’s negative account balance test that refunds entirely to the user’s \\ncore account balance in the event that a user carries a negative account balance at the time of the refund. In these cases, the refund is \\nexcluded from the loan refund tracking.\\nCalculating Daily Balances\\nStarting Balances: For any day other than the first day of observation, the starting balance will be equal to the prior day’s ending balance. \\nOn the first day of observation, there needs to be some sort of starting point with which to calculate from. These initial starting balances are \\nsourced from the payment_loanprincipalbalancehistory table. We first try to set the starting balance to the ending_balance of the',\n",
       "  '59\\nCredit Bureau Reporting\\nOverview: \\nThis document outlines the reporting logic for data we will send to Experian and Equifax.\\nProcess Summary:\\nThe process of generating data to send to Experian consists of the following steps, further outlined below:\\n1. Determine borrower sample (DEPRECATED)\\n2. Determine Perpay parallel of field definitions for data required\\n3. Gather borrower history data from the datamarts, formatting each field to Experian’s precise standards\\n4. Convert data into Metro 2 format, a specifically formatted text file\\n5. Validate that the Experian data is consistent within itself, with rules outlined within the Metro 2 documentation\\n6. Setup a programmatic SFTP connection to the server where the file will be sent on a regular basis\\n7. On an ongoing basis, do manual audits of a handful of borrowers sent to Experian to verify the data is accurate.\\n8. On an ongoing basis, field requests to cancel Perpay Plus enrollments.',\n",
       "  '545\\nArchitecture\\nExcalidraw .md file if you ever need to change the above - use Obsidian with the Excalidraw plugin:\\nJump Box\\nFor accessing endpoints in the production environment from the staging Airflow EC2 instance, we needed to set up a jump box in a public \\nsubnet in the production environment. You can see the Terraform code for that here. We generated the SSH key directly on the staging \\nAirflow EC2 and placed the public key on the jump box via Terraform.\\nOnce that was set up, we needed to set up SSH tunneling on the staging box so traffic would be routed to the Redshift cluster in production. \\nSince SSH connections can drop, we needed a more persistent approach, enter autossh. On the staging Airflow EC2 instance, I ran the \\nfollowing to download the package:\\nAfter that, you can simply set up SSH tunneling with the following command:\\nDrawing 2024-0 …\\n19 Jun 2024, 08:41 PMdraw .md\\n1sudo amazon-linux-extras install epel -y\\n2sudo yum install autossh -y',\n",
       "  '395\\nPayment Tracking\\nPayments: Payments are sourced from the payment_detail table. For every row in the payment_detail table the amount is considered to \\nbe a completed marketplace payment on the date of the created timestamp for the given loan_id. Borrower credits are backed out of \\nthis amount and stored in the separate borrower_credits column. The calculation for borrower credits is described below.\\nBorrower Credits: Borrower credits are sourced from the offers_borrowercreditamounthistory table. For any row in the table with a \\nstatus of used or partially_used and a starting_balance - ending_balance greater than zero, the amount equal to the \\nstarting_balance - ending_balance is considered to be a dollar amount of borrower credits applied to that loan_id on the date of the \\ncreated timestamp.\\nReturned Payments: Returned payments cannot be accurately calculated at the loan level because there is no way to reconcile historical',\n",
       "  \"455\\nSo, new_test_monthly_vantage_scores is fine. ✅\\nTest Monthly Vantage Scores\\nThese have to do with pp+ v1 and v2. I don’t even know if this file is used anymore since the new_test_monthly_vantage_scores exists.\\nSo, test_monthly_vantage_scores is fine. ✅\\nOthers\\nFS Daily Perpay Plus Revenue\\nThis only pertains to pp+. \\n22            from public.feature_history a\\n23            left join public.feature_enrollment b on b.id = a.feature_enrollment_id\\n24            left join public.feature c on b.feature_id = c.id\\n25            where c.type in ('perpay_plus', 'perpay_plus_v2') and a.status = 'enabled'\\n26            group by a.borrower_id),\\n27...\\n1...\\n2                with plus_status_history as (\\n3                    select\\n4                        b.borrower_id,\\n5                        b.start_time as status_time,\\n6                        cast(b.start_time as date) as status_date,\\n7                        cast(first_value(b.start_time) over (\",\n",
       "  'balance changes occur on the post date. In most cases, the account balance will change on the date of settlement, however it could be \\nthat the settlement date is usually the same as the post date. Do you know anywhere that transaction post dates are reported other than \\non monthly customer account statements? The only timestamp information for transactions that I see in the Deserve reports are a \\ntransaction date, a clearing date, and a settlement date.\\nFollow-up R: yea I would think post date would typically = settlement date and then it might be in some cases they update balance \\nprior to settlement (I could see them doing this if there’s a high probability that the transaction will eventually settle).I don’t think post \\ndate is reported anywhere other than statements  and I suppose can be imputed using the account balance SFTP reporting.I’ll reach out',\n",
       "  '38                \"payroll_provider_name\": \"Gusto\",\\n39                \"payroll_provider_type\": \"Gusto\",\\n40                \"payroll_portal_url\": \"www.gusto.com\",\\n41                \"company_type\": \"Verified Pinwheel Eligible\"\\n42            },\\n43            \"referral\": {\\n44                \"referral_url\": \"john-smith-1\",\\n45                \"was_referred_ind\": 0,\\n46                \"referral_invitations_total\": 5,\\n47                \"referred_users\": 3,\\n48                \"referred_borrowers\": 1,\\n49            }\\n50        },\\n51        \"financial\": {\\n52            \"current\": {\\n53                \"status\": \"B1\", // Not In Repayment, Slow Payer, ChargedOff, Current, B1, B2, B3, B4, B5\\n54                \"lifecycle\": \"onboarding\", // Activation, ChargedOff, Conversion, Delinquent, Dormant, Onboarding, Repayment\\n55                \"risk_tier\": \"T3\", \\n56                \"balance_outstanding\": 2140,\\n57                \"balance_cash\": 300,\\n58                \"balance_perpay_credits\": 50,',\n",
       "  'in Jun 2021. The historical record prior to the introduction of the waffle flag tables is held in \\nrefrence_archive_int.pinwheel_payroll_provider_legacy and reference_archive_int.pinwheel_company_legacy. In addition, the \\npublic.waffle_flag table contains a column modified which indicates the timestamp of the last instance that a company or payroll \\nprovider’s eligibility status changed. Because there are some companies and payroll providers with a timestamp in modified that does not \\nhave a corresponding record in public.waffle_flag_history, this column was also used to help form a more complete historical record. \\nAssembling Critical Dates\\nPerpay understands a company to be eligible for Pinwheel if the company is unverified, if the company’s payroll providers is on Pinwheel’s \\npayroll provider eligibility list, or if the company is on Pinwheel’s company eligibility list. In addition, Perpay began directing unverified',\n",
       "  \"when the pay_cycle is monthly then this is calculated as days_since_last_rdd_success minus the (grace period (3 \\ndays) + 30 days) = 33 days (where grace period is hardcoded for some reason)\\ndays_since_card_account_suspended: \\nthe number of days since the status of the card account hit 'suspended'\\ndays_since_Account_frozen: \\nthe larger of the days_since_rdd_failure and the days_since_card_account_suspended → this makes sense to me\\nreformatted\",\n",
       "  'from using API calls as well.\\nDAG Not Running \\nOften something in data model full will break just because of how many tasks exist in the DAG. When this happens the DAG will fail and \\nIterable will not be updated again until the next time that the data model full DAG runs. When a single GE test fails, we do not necessarily \\nwant the entire DAG to fail. Implementing logic to allow the GE tests to warn without failing would allow the Iterable data to be updated much \\nmore frequently. \\nDomo Tasks\\nNot every task in data model full contributes to the data that eventually is transported to Iterable. Some of the tasks are only needed for the \\ncreation of the Domo reports that data model full is also responsible for. These tasks only exist in the data model full because they have \\ndependencies on other tasks within the DAG. This set of tasks does not need to run before the data is ready to be picked up by Census and',\n",
       "  '536\\nTable specific accessaccount_reconciliation_i\\nnt \\n   \\n  \\nperpay_fpa_datamart_int  \\n   \\n  \\nperpay_risk_datamart_e\\nxt \\n   \\n  \\ndomo_ext  \\n   \\n  \\n \\nsegment_logs  \\n  \\n   \\n \\nsegment_perpay_core_p\\nroduction \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_core_production \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production \\n  \\n   \\n \\nsegment_perpay_web_p\\nroduction \\n  \\n   \\n \\nsegment_aircall  \\n  \\n   \\n \\nsegment_intercom  \\n  \\n   \\n \\nsegment_kickoff_labs  \\n  \\n   \\n \\nsegment_mandrill  \\n  \\n   \\n \\nsegment_perpay_core_p\\nroduction_int \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production_int \\n  \\n   \\n \\nsegment_perpay_storefr\\nont_web_production_tra\\ncks \\n  \\n   \\n \\nsegment_perpay_web_p\\nroduction_int \\n  \\n  \\n  \\nsegment_perpay_web_p\\nroduction_tracks \\n  \\n  \\n  \\nfivetran_branch  \\n  \\n   \\n \\nfivetran_tiktok_ads  \\n  \\n  \\n  \\nfivetran_iterable  \\n  \\n  \\n  \\nperpay_marketing_data\\nmart_ext',\n",
       "  '108\\nHow do you handle upstream schema changes in your pipelines? \\nWhat is a Data Contract? Monte Carlo\\nWhat is a Data Contract? IBM\\nSchema Evolution on the Data Lakehouse Onehouse’s approach of handling upstream data changes - emphasis on backward-\\ncompatible queries\\nCrux Makes Dealing With Schema Changes Easy - are both proactive and reactive\\nInterfaces and Breaking Stuff - dbt’s founder shedding light on the importance of the conversation around the lack of contracts\\nSchema Evolution and Compatibility for Schema Registry on Confluent Platform | Confluent Documentation - offer a “Schema \\nRegistry” with the purpose of versioning schemas\\ndlthub \\n⭐ Schema Evolution | dlt Docs a well written out approach to schema evolution, and the tasks of numerical curation and business-\\ninfused structuring that data engineers have\\nGoogle Colab - DLT demo',\n",
       "  '55\\nGoogle Marketing GET API',\n",
       "  \"5: Log '[InvoiceMatchingV2] Removing invoices marked as delete from DB - table: po_invoice_overview \\n...'\\n6: update the perpay_accounting_datamart_ext.po_invoice_overview table\\n- Join the info from the sheet (from temp_int.invoice_matching_sheet_temp) onto \\nperpay_accounting_datamart_ext.po_invoice_overview where delete = 'Y' and save that to temp_int schema \\nas temp_int.po_invoice_overview\",\n",
       "  '217\\nAccounting Notes for AP Revamp\\nTable of Contents:\\nManual PO Matching Sheet\\nImportant Note\\nFields to Modify\\nWhat Enters the Sheet\\nOnboarding new vendors to the process\\n1: Setting up Gmail\\n2: Setting up Zapier\\n3: Update the list of vendors and their invoice types\\n4: Adding the vendor to Quickbooks\\n5: Setting up the code\\n6: Finally , DE will turn the correct zap on!\\nManual PO Matching Sheet\\nThis is the Manual PO Matching Sheet: Manual PO Matching \\nImportant Note\\nOverall, anything can be marked as Yes for ready_for_qb with a chosen_payment_option of Invoice Price as long as it has non-empty \\nvendor_name, invoice_number, and invoice_sku columns.\\nThis means any time ecomm_total is empty, you can still mark it as Yes as long as the invoice_sku is not empty. The reason this field would \\nbe empty (along with empty ecomm_po and ecomm_sku) is because the automated process failed to match the invoice line item with the',\n",
       "  'Creating a Step In Between\\nDBT’s Staging Layer\\nThis confluence page covers DBT’s implementation of a “source layer”.\\nIn this page, they describe its purpose:\\nThe purpose of staging models (in our convention) is just to clean up and standardize the raw data coming from the warehouse, so \\nwe have consistency when we use them in downstream models.\\nIn our dbt project, we’ll place them in a staging folder, and prefix filenames with stg_ for easy identification (so our Zendesk chat \\nlog would be stg_zendesk_chats, which is based on the raw zendesk.chats source table).\\n1. \\nThey’re typically a one-to-one reflection of each of our raw sources, and we do really light transformations at the staging layer. We \\nwill very rarely join data models at the staging layer, but instead will perform transformations like:\\nField type casting (from FLOAT to INT, STRING to INT, etc), to get columns into the proper type for downstream joins or \\nreporting\\nRenaming columns for readability',\n",
       "  '36\\n \\nNotice also that the radius has been templated. This lets us to control the radius when the tests are run which will prevent the tests from \\nbreaking if someone decides to increase or decrease the radius for reporting later. This is a good practice for building tests around queries \\nwith configurable parameters.\\nStep 3: Mocking the data and writing tests\\nTips for mocking the data\\nKeep it simple. It should be easy to quickly calculate what the expected outputs should be given the mock inputs.\\nInclude examples that represent all expected behavior and edge cases. Leave comments indicating the reason for including certain \\ninputs wherever it would be helpful to do so. Remember that you might need to modify the mock data later if requirements change and \\nyou don’t want to accidentally delete an example that looked superfluous but was actually there to test an important edge case.',\n",
       "  '148\\nAMZ-B07NQNS9CR|B07NQNS9CR\\nIssue: The Child SKU has multiple parent SKUs (parent SKU of top row is extended).\\nBBY-BB20840922-- this one has not resolved since 12/13/2022\\nIssue: The parent SKU BBY-BB20840922 has a parent SKU PS-MX3X2LLA7c2eb440921\\ne\\nb44b6eb3-\\n5b47-46df-\\n9cd5-\\nc3a82fe083b43414d14e-\\na3eb-4db8-\\n9b9d-\\n7c2eb440921\\neAMZ-\\nB084LMTR98DH-2007813 Amazon - \\nAutoD&H 2020-04-03 \\n15:36:02.0237\\n602021-01-13 \\n20:30:11.1905\\n57\\n3a600a47-\\n95e9-4b67-\\n8d39-\\n346ee2562cb\\n1cea3e3e6-\\n111b-4bd5-\\nb5a2-\\n20bd62a8475\\n5AMZ-\\nB07NQNS9C\\nR|B07NQNS9\\nCRPPA-\\nB07NQNS9C\\nR|B07NQNS9\\nCRAmazon - \\nAutoPerpay - \\nAmazon2020-04-03 \\n15:36:03.7739\\n142020-04-01 \\n13:25:24.7782\\n01\\n3a600a47-\\n95e9-4b67-\\n8d39-\\n346ee2562cb\\n162916a69-\\n6608-41f0-\\nb806-\\ne54ae49ec7b\\n5AMZ-\\nB07NQNS9C\\nR|B07NQNS9\\nCRPPA-\\nB07NQNS9C\\nRAmazon - \\nAutoPerpay - \\nAmazon2020-04-03 \\n15:36:03.7739\\n142020-04-23 \\n14:01:08.5576\\n07child_id parent_id child_sku parent_sku child_vendor parent_vendo\\nrchild_created parent_create\\nd\\nfe2b1bcf-\\nd088-4be8-\\n8c5a-',\n",
       "  '423\\nWIP\\nThese projects are partially complete.',\n",
       "  '119\\nCreating DAG Tasks\\nHistorically we have used the helper methods contained in dag_helpers.py to create tasks within DAG files. These helper methods use \\nthe Airflow-provided Python operator to execute SQL against the database which aren’t fully compatible with the column-level lineage \\ncomponent of DataHub. In order to get the most out of DataHub we are moving towards packaging the SQL code we execute against our \\ndatabase in the airflow provided SQLExecuteQueryOperator.\\nIn using the SQLExecuteQueryOperator we lose the flexibility to execute multiple SQL statements in the same task. Since creating a new \\nproduction table often involves several SQL statements this means that what previously may have been a single task becomes several. To \\nkeep everything organized we encapsulate all of these tasks in a single task group:\\nTempTestCommitTaskGroup',\n",
       "  \"Interviews- Questions565\\n      Onboarding & Offboarding568\\n           DE Onboarding569\\n                M1 Chip Workarounds For Docker/Airflow571\\n           Offboarding572\\n                Sylvia's Transition573\\n                     Calculating AWS Costs574\\n                     Segment Data Ingestion (via S3)576\\n                     Accounts Payable Updates579\\n                          Roadmap for Completion580\\n                          Automated CSV --> Parquet in AWS Console581\\n                          Lambda function to convert excel --> csv file types587\\n                     Accounts that need new owners589\\n                     Random Resources / Knowledge Transfer590\\n                     Credit Card593\\n                          Lifetime walkthrough of a card user594\\n                Derya's Transition597\\n      Continuing Education598\\n           Analytics Stack Reporting599\\n           Data Modeling Pipelines & DBT600\\n           Airflow & Kubernetes601\",\n",
       "  '215',\n",
       "  'accounts per borrower. \\nI split the fees up entirely- do we want to group them further & if so, how (shipping fees, one-time fees (although this \\ncould include shipping), interest fees, monthly fees)? → resolved talking with JD- decided to split shipping into its own \\ncategory and leave all else. additionally added a column called other to capture anything outside of the prior statuses. \\nAlso updated the dependencies (test file)\\nreformatted\\nupdated dependencies from logic changes\\nCard borrower underwriting\\nConfluence documentation is not correct → fixed\\nWe can beef this up over time as we learn what is important. Otherwise good for now – although it would be good to doucment \\nthat the different columns actually refer to\\nreformatted\\nfrom the old documentation (which was largely incorrect), there was a note saying all application data before 07/04 is \\nmissing/unreliable- check if this is still true. If not, remove the note\\n \\nCard card data model  ✅  PR is up',\n",
       "  'have been processed\\na. This is also used for logic to figure out the start-date to run the service incase there is an outage or error\\nData Definitions:\\nUpdates table (updatesperpayYYYYMMDD), data type is reference to pandas dataframe (type=update specific values, type=paid_off \\nspecific values)\\nVariable Name: Data Type: Description:',\n",
       "  \"332\\nLater, they send us \\nanother webhook \\nto recreate the \\ndeleted \\ntransaction, but we \\njust create a new \\none with the same \\ntransaction info.\\nProcessing errored \\ntransaction \\nwebhooks\\nDeserve sent us \\ntransaction \\nwebhooks several \\ndays after when \\nthe transaction \\noccurred\\n4 transaction ID: 205676There is latency between when a \\ntransaction is reported settled and \\nwhen it is marked as settled in our \\ndatabase.4,584 transactions (~4% of \\nsettled transactions)a lot of \\ntransactions in the \\nsample data are \\nmissing transaction \\nsnapshots due to a \\nbug that was \\nrecently resolved \\n(no missing \\nsnapshots after the \\n1/11/23 release!). \\nThis lead to the \\nmajority of \\ntransactions on the \\nlist having only a \\nSETTLED \\nsnapshot without a \\ncorresponding \\nPENDING \\nsnapshot in the db.\\nthere is no \\nsignificant delay \\n(no more than a \\ncouple seconds) \\nfrom webhook \\ningestion, meaning \\nthat the 'latency' \\nwe observe is due \\nto webhooks not \\nbeing sent \\nimmediately from \\nDeserve.\",\n",
       "  '248\\nRequest for Perpay Plus Feature Enrollment Cancellation\\nBefore updating the Perpay Plus status of a user we much check where they are in the reporting pipeline because that status dictates the \\navailable actions. The Perpay Plus statuses can be found here.\\nThere will be Trello cards created by Ops of users requesting to cancel Perpay Plus. Most of these will be users who opted in to Perpay Plus \\non accident; we want to keep the option to opt back into Perpay Plus open if the user has not been already reported. There are two courses \\nof action:\\n1. If a user is already reported, we must cancel the enrollment.\\n2. If a user has not been reported, we can delete the enrollment as if it never happened\\nEngineering will ask DE to check if the user is reported. The action here is to run the query bat the bottom of this page. If there is data \\nreturned, advise the following action…\\nIf Data is returned for the borrower\\nEnrollment cancellation\\nNo Data is returned for the borrower',\n",
       "  'ones are fine. \\nIn summary,\\n[PROD] ERROR has occured!\\nDAG: dag_organizer\\nTask: data_model_full.Test_MarketingDailyDataModel_Domo\\nError: Bash command failed. The command returned a non-zero exit code 1.\\nFAILED \\n../../opt/***/dags/analytics/data_model_full/tests/test_marketing_daily_data_model_post_iterable.py::TestMarketingDailyDataModel::t\\nest_row_count_marketing_daily_data_model\\nUTM campaign problem\\nResult of a workflow change\\nIterable definition Iterable value Our definition (in the \\ntemp_dev_int.iterable_\\ndaily_insights tables)Our value Meaning',\n",
       "  'Accounting Commerce Loan Daily Rollforward397\\n                          DE Notes on Initial Loan Discrepancies400\\n                     Card Account Balance Daily Ledger402\\n                          Known Deserve Incidents407\\n                Flattened Merged Users408\\n                Withdrawal NACHA File Generator414\\n                Experian Account Review Ingestion417\\n                Deserve Clarity Underwriting SFTP Ingestion419\\n                Deserve Clarity Underwriting Ingestion421\\n           WIP423\\n                Optimizing DM Full424\\n                Pinwheel Investigation426\\n                Pinwheel Metric Layer428\\n                Migrate Data Models From Amplitude to Segment430\\n                Card Datamodels Finalization431\\n                Permissions Restructure432\\n                Adjusting for Cards in the Wild437\\n                Clarity Score Drift from FTBs459\\n                VARIANCES - Accounts Receivables Reconciliation461',\n",
       "  '50            case when h.borrower_id is null then 0 else 1 end as activated_ind,\\n51            round(f.current_balance/f.credit_limit, 2) as utilization,\\n52            case when k.borrower_id is null then 0 else 1 end as provisioned_ind,\\n53            case when m.borrower_id is null then 0 else 1 end as application_started_ind,\\n54            case when n.borrower_id is null then 0 else 1 end as submitted_card_application_ind,\\n55            \\'{\"dt\":\\' || case when d.first_card_enabled_ts is not null then concat(concat(\\'\"\\', to_char(d.first_card_enabled_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n56                                              else \\'null\\' end ||\\n57                \\', \"ind\": \\' || case when enabled_ind = 1 then \\'true\\' else \\'false\\' end || \\'}\\' as enabled,\\n58            \\'{\"dt\":\\' || case when m.created_ts is not null then concat(concat(\\'\"\\', to_char(m.created_ts, \\'YYYY-MM-DD\\')),\\'\"\\')\\n59                                              else \\'null\\' end ||',\n",
       "  'waiting until payments reach a completion status because of instances where a borrower initiates a payment before the due date and the \\npayment is not completed until after the due date. Previously in these cases, this borrower’s payment would be marked late, while this would \\nnot be the case under the new methodology.\\nPayments initiated after 2023-05-08 are sourced from Deserve SFTP data. In most cases, initiated payments can be identified by a status of \\nINITIATED. The one exception to this is that payments that do not go through the ACH holdout process will occasionally only ever hit a \\nCOMPLETED status without ever hitting an INITIATED state, so these also have to be picked up. Payments that only ever hit a COMPLETED \\nstatus will still carry with them a non-null initiated_at timestamp. \\nEnding Balance\\nThe result of the equation should produce the ending balance for the given day that is reported by Deserve.',\n",
       "  '123\\nCommon Errors\\nThis folder is to serve as the basis of knowledge to correct errors we see all the time. It includes known error messages, solutions, and \\nsome suggestions to keep these errors from occurring again! \\nThe inclusion of ( E r r o r ) in each page title under this folder is for search-bar optimization purposes. When you search a topic, it allows for an \\neasier distinction between these error handling pages and the generic documentation via title. Please include it when you add a new page to \\nthis folder.',\n",
       "  '367\\nSolution\\nA large cost associated with the selected queries was the practice of writing the interstitial tables to disk. It is much faster to build these \\ntables in memory which can be accomplished by using common table expressions (CTEs). Including a subquery within a JOIN statement \\nwould functionally accomplish the same thing as forming a CTE with a WITH clause, however CTEs were used as they are thought to be \\nmore readable, are able to be used within several JOIN statements in the same query, and for consistency within the codebase. All \\ninterstitial tables were either changed to CTEs where possible or removed entirely if unnecessary. One concern with turning all of the \\ninterstitial tables into CTEs is that the data would exceed the available working memory and have to be written to disk anyway. This would \\neffectively get rid of any time saved from using CTEs. To check whether a query includes any disk-based steps:\\n1. Run the query\\n2. Retrieve the Query ID from sql_qlog:',\n",
       "  \"loan table. Does anybody know why this is? This is also causing some variances because we were expecting the start of the loan's \\nhistory to have the same same balance as the amount of the loan.\\nResponses:\\n1. I see change to that part of the codebase on March 2019. Did a quick manual binary search on Loans, looks like the cutover is at 2019-\\n4-11 , ~19:36 UTC\\n2. This might be due to a change, or not backfilling the data somehow. Looks like one off cases, probably incorrect manual deletions. \\nBecause all of the cases are prior to 2023, we are leaving them as is for now.\\nstart_bal_loans.csv end_bal_loans.csv \\nVariance 11: Midnight Payment Crossover\\nNo questions associated with this variance. Talha found it when investigating a specific case I sent him amongst a list for variance 5. I \\ncreated this new variance to cover all other edge-case borrowers similar to this (although I doubt there’s many) and to keep track of its \\nprogress.\",\n",
       "  \"17       a .card_transaction_id as perpay_transaction_id ,\\n18       a .created as perpay_created_ts ,\\n19       a .amount as perpay_amount ,\\n20       a .tip_amount as perpay_tip_amount ,\\n21       a .status as perpay_status ,\\n22       a .type_category as perpay_type_category ,\\n23       a .type as perpay_type ,\\n24       a .transacted_at as perpay_transacted_at ,\\n25       a .settled_at as perpay_settled_at ,\\n26       datediff ('hour', settled_at , created ) as difference_in_hours\\n27from card_transaction_cte as a\\n28where a.status = 'SETTLED'  and difference_in_hours > 2 and a.type not in ('FEE');\",\n",
       "  \"fivetran_quickbooks_sandbox schema. This may be important to consider depending on what you're trying to test.\\nI like to copy the table into the fivetran_quickbooks schema (the schema actually utilized by the DAG) with select invoices to mimic the \\nSandbox QB info.\\nReset\\nA lot of times on staging, you’ll run the job, modify something in the sheet to see that it’s working, then want to reset. This process varies \\nbased on what you’re trying to do, but the most common form of reset goes as follows:\\ndelete the invoice from QB staging\\nif you want to delete a single one, you can find it, click the dropdown arrow to its right and hit delete\\nif you want to bulk delete for a vendor, you can mark that vendor as inactive, which essentially does the same (puts a suffix like \\n_deleted after each invoice). this allows you to recreate the vendor with a balance of $0.00\\ndelete the invoice histories\",\n",
       "  'keep spurious dups from being added to our database (Note, this will absolutely require our Data Dog integration to be complete so we \\nknow if any DMS task has stopped because of an error). These are only two examples, however, time is necessary to understand the \\nfull extent to which we can make our AWS infrastructure more specific to our needs.\\nRedshift Resource Optimization\\nAs stated above, our infrastructure is governed by mainly default settings. Additionally, our cluster uses around 55% to 60% of its CPU \\nwhile only around 0.1% of its available memory. This is woefully out of balance and results in slow processing times when running \\nqueries. Possible reasons for this behavior are non-optimized queries, sub-optimal sort/dist/etc keys, out of date table statistics, and so \\non. These result in the Redshift query engine not putting together the best graph for running queries and leads to all the stated',\n",
       "  'oje\\nctCard \\nInve\\nstiga\\ntions \\nProje\\nctDS \\nTrai\\nnin\\ngInt\\ner\\nn \\nPr\\noje\\nctsIden\\ntity \\nGra\\nph \\nInte\\nrns\\npubl\\nic.p\\naym\\nent_\\nrisk\\ndata\\npubl\\nic.a\\nchd\\netail\\npubl\\nic.u\\nser\\nper\\npay\\n_ge\\nnera\\nl_da\\ntam\\nart_i\\nnt.u\\nser_\\nattri\\nbute\\npubl\\nic.a\\nchd\\netail\\npubl\\nic.u\\nser',\n",
       "  '561',\n",
       "  'Data Model). \\nThere are multiple instances of >1500 users with different PIIs associated with the same device ID / Amplitude ID. We note that 53% of \\nAmplitude IDs map to more than one UUID.\\nFor example, for the 2000+ users associated with the same device ID 6f94186b-6e4e-4205-a711-3b9cef874fe5R , we see that:\\nThese users are distributed across the country\\nThe users signed up for Perpay at different dates\\nThe users are associated with different companies\\nThe distribution of long-term revenue associated with each of these users is even',\n",
       "  '+ international_transaction_fee\\n+ interest_charge\\n– interest_charge_adjustment\\n– cash_back\\n+ cash_back_adjustment\\n– refund\\n+ credit_balance_refund\\n– dispute_won\\n+ dispute_lost_withdrawn\\n– dispute_provisional_credit\\n– dispute_write_off\\n+ returned_payments\\n– completed_payments_pre_may_8_2023\\n– initiated_payments_post_may_8_2023\\n= ending_balance = ending_balance_calc\\nNuances\\nStarting Balance\\nWe first observe a card account for account reconciliation purposes on the first day that the card account has a balance reported by \\nDeserve. For this process data is pulled from both the web-hook and SFTP files. The first date from either of these data sources that \\nDeserve reports a balance is the day that observation of the card account begins in card account reconciliation. \\nThe starting balance on the first day of observation is equal to 0. For all subsequent days, the starting balance is set equal to the ending \\nbalance of the previous day.',\n",
       "  'price_special_from_date boolean Whether the product has a special price; \\ncurrently always False.\\nprice_special_to_date boolean Whether the product has a special price; \\ncurrently always False.\\nship_price double precision The shipping price of the product.Column Name Type Description',\n",
       "  \"2. Is the logic for the compliance or Metro2 check outdated or incorrect? \\n3. Is this a weird edge-case that we have never encountered before? '\\n4. Is there weird data in a public table? \\n5. Do we need to ask ops to reach out to the customer to update their account info? \\n6. The resolution for each case will look different, but these are some common sources of held-out borrowers\\nTouchbase Takeaways\\nOccasionally asks will come in from compliance to update how we are doing our credit reporting. These are disseminated at a biweekly \\ncredit reporting touchbase that the DE on rotation will be attending. KG and compliance will share the context for the asks, and you’ll have \\nan opportunity to answer any questions. This is also an opportunity to share any updates on the asks from the previous touchbase. The \\nmeeting notes keep a brief running log of what the current outstanding asks are:\",\n",
       "  '371\\nNew Process\\nPart 1: Invoice Ingestion\\nInvoices are received in PDF or Excel format to the email address invoices@perpay.com. \\n^ I don’t want to change this part of the process because it would require our vendors to switch their current setup, and there has \\nbeen reluctancy to change in the past. Furthermore, receiving invoices as attachments through the Gmail is manageable on our \\nside.\\nThe excel sheets are plopped into an S3 bucket through Zapier and loaded into Redshift automatically (via Glue). This will require an \\nupdate to the invoice_matching DAG. The Zaps are designed to not do any data cleaning, which is instead saved for later in the process \\nafter the invoices have been loaded into Redshift. One exception is Milor Group because the CSVs they send cannot be handled by the glue \\njob until they are cleaned.\\n^ There is no reason to copy and paste these into a Google sheet prior to pulling them into our db as we currently do. Plopping',\n",
       "  'dags/utils/lib/google_sheet_integration.py. It loops through the Non Parsed Partners sheet and calls the sheet_to_redshift \\nfunction to write its contents to the schema. The argument False is set to False so that the Non Parsed Partners google sheet \\ndoesn’t clear. If set to true, the sheets will be cleared; we never want this to happen, so we leave it as false)\\n3: We create empty external tables for the vendors, using the temp_int tables from step 2. These tables follow the same \\nnaming conventions as step 2, only they are in the perpay_accounting_datamart_ext schema. A resulting table looks like \\nperpay_accounting_datamart_ext.invoicing_unparsed_rymax.\\n- We don’t use these tables anywhere in our code\\n so these table creations are pointless and we should delete them \\nfrom the code\\n- Heads up, the tables will also throw a message every time explaining they won’t be recreated because they already \\nexist… this doesn’t matter because, again, we don’t use them anywhere',\n",
       "  \"508\\nthis aligns with our borrower_credits tracking for the examples above\\nResponse: If it is not a big lift, can you share the total number of occurrences and share the most recent ones (last 2-3 should work)? No \\nrush though, for tomorrow.\\nFollow-up: Total number of occurrences is 64, with the two most recent being\\nvar_5_recent_ex_all.csv \\nI also took out the ones that have additional factors (i.e. payments) in case you want to look at those, where this total comes to 52 and the \\ntwo most recent are\\nvar_5_recent_ex_credits_only.csv \\nResponse: n your list for the two most recent of 64 occurrences, I'm seeing that loan 4997554 shows an ending balance of 729.98 \\n(payment_loanprincipalbalancehistory row 18015222). Similarly, I see that Loan  4980990 has an ending principal balance of \\n325.40.I've looked a bit into loan 4860890 and don't yet understand the two principal balance reductions of $10.81 on 6/15. Will keep \\ndigging after lunch.\",\n",
       "  \"Appropriate levels of actions should be taken depending on the severity of the errors - not all errors should bring the process to a \\nscreeching halt, though some should\\nError logging should be such that the problem area is clearly delineated so that the issue can be triaged immediately without a ton of \\ncontext - maybe even by the commerce team\\nProgress should be saved if errors do occur - there shouldn't be the need to re-run the whole process after an error is corrected, the \\nprocess should just continue forward\\nPost POC\\nNeed to add in the handling for invoices that come in as CSVs. This will require Terraform to create S3 with the appropriate permissions \\nfor read/write of finance data\",\n",
       "  '2          then 1 else 0 end as deserve_employee_ind,\\n3       a.id as perpay_id,\\n4       a.created,\\n5       a.amount as perpay_amount,',\n",
       "  'shape and flow of the merge network made sense, the lead user was not the same as the last merged to account.\\nThe following are examples of cases (1), (2), and (4):\\nA cluster with cyclic merges.',\n",
       "  '74\\ncreated_at date The date the product was created.\\nbrand_name varchar(65) The brand name of the product.\\ntype_id varchar(22) The type of the product.\\ncurrent_status varchar(18) The status of the product; Enabled or \\nDisabled.\\nhas_parent_flag varchar(1) Whether the product has a parent; only \\nproducts where this is 0 will have views / \\napp started / repayment data below.\\navailability_flag varchar(1) Whether the product is available in search \\ncurrently.\\nsuppres_item_flag integer Whether the product should be suppressed \\nfrom marketing emails, set in Magento.\\ncount_views_last_7_days bigint Count product views in the last 7 days. \\nCounts for products with parents are rolled \\nup to the parent, and this field will be 0.\\ncount_views_last_14_days bigint Count product views in the last 14 days. \\nCounts for products with parents are rolled \\nup to the parent, and this field will be 0.\\ncount_views_last_30_days bigint Count product views in the last 30 days.',\n",
       "  '215',\n",
       "  '240\\nHere’s an example PR for reference.\\nRegardless of CSV or PDF……\\nupdate the constants.py file within the AP Revamp folder to include the name of the vendor as seen in QB. This is used in the \\naccounts_payable_int.qb_bills query to gather all vendor names from QB.\\n6.2: Testing new code\\nTest on stagginggg!!!\\nIf accounting performed steps 1-5, do a quick check to make sure the zaps and quickbooks vendor are set up correctly (mistakes \\nhappen, just double check so the process runs smoothly), and you are ready to test! Refer to the section above for staging tips.\\nMake sure that you add the new vendor name to Sandbox QB and check that the new invoice made it into the po_matches or \\npo_variances table (either into Sandbox QB or into the Manual PO Matching sheet).\\nRefer to this link for instructions on how to access Sandbox QB.\\n7: Update the vendors sheet\\nUpdate the Vendor Invoice Types and Terms Google Sheet in the AP Folder in the DE Drive.\\n8: Loop back to the accounting page',\n",
       "  '(12:00AM EST). This is two hours before the DAG kicks off.\\nIf you have trouble setting up the parse-invoice-lambda-function, here’s a page that might help.\\nZapier access keys can be found in 1pass under AWS Access Key- prod-zapier-external-user. These allow Zapier to connect \\nto the Amazon S3 PROD account.\\nVendor \\nNameInvoice \\nNumbe\\nrPO \\nNumbe\\nrInvoice \\nDateDue \\nDateModel \\nNo/SK\\nUTrackin\\ng \\nNumbe\\nrQuantit\\nyUnit \\nCostShippin\\ng \\nCharge\\nsDrop \\nShip \\nCharge\\nsFreight \\nCharge\\nsTotal \\nCost\\n(1) Retrieve attachment from invoices@perpay.com vendor label\\n(2) Only continue if the attachment is in the form of .csv\\n(3) *Depending on the vendor (right now I believe this only applies to Fragrance Net)* Use Python to append a hash (based off the \\ntimestamp) to the back of the file attachment name\\n(4) Send the file into the csv_outputs/ folder within the ana-stage-accounts-payable-s3-i192s S3 bucket\\nThe data is brought into a table named cleaned_csvs in the accounts_payable_int schema.',\n",
       "  '48),\\n49-- Sum the rows to capture the total amount of redeemed rewards\\n50rewards_redeemed_summed as (\\n51    SELECT\\n52        borrower_id ,\\n53        email ,\\n54        account_id ,\\n55        sum(rewards_redeemed_usd )::decimal(10,2) as total\\n56    FROM rewards_redeemed_unsummed\\n57    GROUP BY 1, 2, 3\\n58    ORDER BY email desc\\n59),\\n60-- Get the provisioned date for each borrower\\'s card. We need this to set the rewards multiplier\\n61-- of either 3% or 2% later on.\\n62card_provisioned_date AS (\\n63    SELECT distinct\\n64        a .borrower_id ,\\n65        b .created_ts:: date as provisioned_date\\n66    FROM\\n67        random_sample a\\n68        INNER JOIN card_int .borrower_fact b\\n69            ON a.borrower_id = b.borrower_id\\n70)\\n71-- Final statement. Pulls everything together and renames the columns to match the example sheet\\n72SELECT\\n73    a.account_id ,\\n74    a.email,\\n75    coalesce (b.payments , 0.00)::decimal(10,2) as \"Transaction Value\" ,\\n76    CASE',\n",
       "  'write a python shell job in glue documentation\\nAWS Glue job to import an excel file\\nConvert CSV / JSON files to Apache Parquet using AWS Glue\\n(Random) Helpful Articles:\\nHow to upgrade terraform to the latest version (& other helpful TF environment setup)\\nHow to create helpful admin views in Redshift\\nExample: v_space_used_per_tbl shows how much space each table takes up',\n",
       "  \"43        INNER JOIN random_sample b on a.account_id = b.deserve_card_account_id\\n44        LEFT JOIN public.borrower c on b.borrower_id = c.id\\n45        LEFT JOIN public.user d on c.user_id = d.id\\n46    WHERE\\n47        a .sftp_dt between [Quarter start dt] and [Quarter end dt]\\n48    ORDER BY d.email desc\\n49),\\n50-- Sum the rows to capture the total amount of redeemed rewards\\n51rewards_redeemed_summed as (\\n52    SELECT\\n53        borrower_id ,\\n54        email ,\\n55        account_id ,\\n56        round(sum(rewards_redeemed_usd ), 2)::decimal(10,2) as total\\n57    FROM rewards_redeemed_unsummed\\n58    GROUP BY 1, 2, 3\\n59    ORDER BY email desc\\n60),\\n61-- Get the provisioned date for each borrower's card. We need this to set the rewards multiplier\\n62-- of either 3% or 2% later on.\\n63card_provisioned_date AS (\\n64    SELECT distinct\\n65        a .borrower_id ,\\n66        b .created_ts:: date as provisioned_date\\n67    FROM\\n68        random_sample a\\n69        INNER JOIN card_int .borrower_fact b\",\n",
       "  'a. The DAG runs with the regenerate flag as True so the full raw table is rebuilt - this option allows the most recent raw table to be \\nconverted to Metro 2 without rebuilding (here)\\nb. FYI, all this really does is either copy the data from the previous step or use what is already existing in the raw data table - so if you \\nset regenerate as false, almost the same amount of work is performed',\n",
       "  'credits)Payment \\nTracking\\nborrower_credits Numeric(\\n38,2)Borrower credits applied to the loan balance \\non the observation datePayment \\nTracking\\nreturned_payments Numeric(\\n38,2)Payments returned on the observation day \\nthat will cause the loan balance to be \\nincremented by the returned amountPayment \\nTracking\\nrefunds Numeric(\\n38,2)Refunds for the loan applied to the loan \\nbalanceRefund \\nTracking\\nending_balance Numeric(\\n38,2)The balance resulting from all balance \\nmovement being applied to the day’s starting \\nbalanceCalculating \\nDaily \\nBalances\\ncurrent_balance Numeric(\\n38,2)The present loan balance. This value only \\npopulates on the observation row for the \\ncurrent dateLoan Level \\nVariances\\nvariance Numeric(\\n38,2)The difference between the ending balance \\nand the current balanceLoan Level \\nVariances\\npossible_partial_returned_p\\nayment_indInteger An indicator for loans that have had \\nassociated returned payments. This value only Loan Level \\nVariancesColumn Data \\nTypeDefinition Additional',\n",
       "  \"your function code to Lambda. The deployment package acts as the source bundle to run your function's code and dependencies (if \\napplicable) on Lambda. Lambda supports two types of deployment packages: container images and .zip file archives. \\nSince we only need 3 packages for our lambda function (pandas, openpyxl, and s3fs), it is easiest to use the .zip file archives. There is \\ngreat documentation about how to do this here.\\n \\nNote: Ran into dependency issues with NumPy and Pandas when trying to use .zip file archives. As a workaround, I used this workaround.\\n \\nThe zip package used for the accounts payable process can be found in the Data Engineering github repo. \\n \\n \\nExample workflow (for Rymax, Accounts payable)\\n \\nNew statement email in gmail \\n→ Triggers Zapier\\nZapier extracts attachement, dumps in s3 bucket (in the Rymax Excel folder)\\ns3://ana-stage-s3-accounts-payable-d3tcw/rymax_statements_excel/\\n→ Triggers Lambda function\",\n",
       "  'from user_id = 123 to user_id = 456 account review data will be pulled for user_id = 123. \\nKeeping the column names standardized to underwriting data sources would be helpful in leveraging current ETL processes that use \\nthose column names \\nTO DO: Premier File 1 reference_archive_int Account Review Premier Attributes\\nPremier File 2 reference_archive_int Account Review Premier Attributes\\nClear Credit Attributes reference_archive_int Account Review Clear Credit Attributes\\nClarity Riskdata perpay_risk_datamart_ext Underwriting Clarity Distributable \\nData Dictionary\\nClarity Retro Data reference_archive_int Underwriting Clarity Distributable \\nData DictionaryTable Name Schema Source Experian Data \\nDictionary File Name \\nmapping\\n15 Jun 2023, 02:48 PM_v0.csv',\n",
       "  '54    variance          varchar(4000),\\n55    ready_for_qb      varchar(4000),\\n56    chosen_payment_option varchar(4000),\\n57    invoice_qty       varchar(4000),\\n58    ecomm_qty         varchar(4000),\\n59    invoice_net_price varchar(4000),\\n60    ecomm_net_price   varchar(4000),\\n61    invoice_shipping  varchar(4000),\\n62    invoice_dropship  varchar(4000),\\n63    invoice_freight   varchar(4000),\\n64    invoice_po_number varchar(4000),\\n65    ecomm_po_number   varchar(4000),\\n66    invoice_sku       varchar(4000),\\n67    ecomm_sku         varchar(4000),\\n68    invoice_date      varchar(4000),\\n69    invoice_due_date  varchar(4000),\\n70    tracking_number   varchar(4000),\\n71    created_ts        timestamp with time zone encode az64,',\n",
       "  \"318\\n \\nPerpay Deserve Data Issue # 2: Latency\\nTransactions are pending that appear to be “settled” according to the deserve_daily_settled_transactions_report SFTP file. \\nThere are 736 transactions in the wrong status. This behavior is prevalent before 9/30, but persists. \\nExcluding transactions before 9/30, right now, there are 12 transactions in this state. \\nPerpay Deserve Data Issue # 3:\\n** appears to be resolved problem **\\nTransactions that appear in the SFTP deserve_daily_settled_transactions_report file are missing from the Perpay Deserve Data. \\n1select\\n2    case when a.account_id in ('149eaf20-8288-4ce3-87da-425b0ed06934' , 'bf085378-a8ad-4244-b921-9f926d1f7416' , '\\n3          then 1 else 0 end as deserve_employee_ind ,\\n4    b.status,\\n5    b.id as card_transaction_id ,\\n6    a.transaction_id ,\\n7    a.transaction_type ,\\n8    a.type_category ,\\n9    a.transaction_amount as sftp_amount ,\\n10    b.amount as core_amount ,\\n11    a.account_id ,\\n12    b.deserve_id ,\\n13    a.cleared_at ,\",\n",
       "  \"you don’t want to accidentally delete an example that looked superfluous but was actually there to test an important edge case.\\nErr toward being overly explicit. Tests are what help prevent us from releasing bugs. The intention needs to be extremely clear, there is \\nno room here for assumptions.\\n \\n3    p.property_id ,\\n4    ra.restaurant_id\\n5from {{ public_schema }} .properties p\\n6join {{ public_schema }} .restaurant_address ra\\n7    on p.state = ra.state\\n8where\\n9    -- flat earth\\n10    sqrt (pow(p.longitude - ra.longitude , 2) + pow(p.latitude - ra.latitude , 2)) < {{ distance }}\\n11\\n12create table {{ temp_schema }} .property_restaraunt_options as\\n13select\\n14    property_id ,\\n15    count(1) number_of_local_restaurants\\n16from {{ temp_schema }} .property_restaraunt_options_base\\n17group by\\n18    property_id\\n1test_user_data = pd.DataFrame ([\\n2    {'property_id' : 1, 'latitude' : 0, 'longitude' : 0, 'state': 'PA'},\\n3    # Same as property 1, but different state. Allows us to test state logic\",\n",
       "  \"apache-airflow-at-lyft-6e53bb8fccff\\nPandora uses Airflow in their stack, and uses Prometheus & Grafana to monitor its state. The shown Grafana dashboard tracks at the \\ntask level, looking at the number of queued tasks and tasks up for retry. It also monitors Airflow database size:  \\nhttps://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee\\nGojek, the Indonesian Uber, uses Airflow and uses InfluxDB and Telegraf to get metrics into Grafana for visualization. They monitor \\nheavily at the tasks level looking at number of queued and running tasks. (Side reading: there is less documentation on Airflow → \\nTelegraf connections then there is documentation on Airflow → Prometheus connections): https://blog.gojekengineering.com/how-we-\\nmonitor-apache-airflow-in-production-210f9bff9e71\\nTechnical Resources\\nDocker compose example with all the containers wired up with Airflow: https://github.com/ankxyz/airflow-prometheus-grafana-exampl\\neCan't find link\",\n",
       "  \"Adjusting for Cards in the Wild437\\n                Clarity Score Drift from FTBs459\\n                VARIANCES - Accounts Receivables Reconciliation461\\n                     Card Report Definitions462\\n                     Card Report Variances476\\n                     Core Report Context483\\n                     Core Report Definitions489\\n                     Core Report Variances492\\n                     Commerce Report Definitions496\\n                     Commerce Report Variances502\\n                     Tying out Engineering's Account Balance Flow514\\n                Deposit, Payment, and WithdrawalRequest Updates519\\n                Iterable DAG Revamp523\",\n",
       "  '118\\n119-- Make the switch - uses renaming to make the switch super fast\\n120create table census_source_int.iterable_customer_data_temporary as select * from users_herr.iterable_customer_data_save;\\n121-- depreciate the current/outdated table\\n122ALTER TABLE census_source_int.iterable_customer_data RENAME TO iterable_customer_data_depreciated;\\n123-- promote the temporary table to current table\\n124ALTER TABLE census_source_int.iterable_customer_data_temporary RENAME TO iterable_customer_data;\\n125-- drop the depreciated table\\n126DROP TABLE if exists census_source_int.iterable_customer_data_depreciated;',\n",
       "  \"7    sum(amount_redeemed) as sum_amount_redeemed\\n8from offers_borrowercreditamounthistory\\n9where borrower_id = 2427261 and loan_id = 4860890 and status = 'used'\\n10group by borrower_id, loan_id;\\n11\\n12-- Case 2\\n13select\\n14    borrower_id,\\n15    loan_id,\\n16    sum(starting_balance) as sum_starting_balance,\\n17    sum(ending_balance) as sum_ending_balance,\\n18    sum(amount_redeemed) as sum_amount_redeemed\",\n",
       "  '6.1: Accounting for the invoice in code logic\\nYou now have the invoice type and the vendor name. If you can, grab an example invoice from the invoices@perpay.com email.\\nWe only really need to modify the ingestion process for the new vendor. Depending on whether they’re CSV or PDF invoices, things differ in \\ncomplexity.\\nIf the vendor is supplying CSV invoices….\\nJust make sure that the layout is the same as the template mentioned above:\\nAnd make sure that the vendor name matches that which was set up in QB. If it doesn’t modify the cleaned_csvs table query to \\naccommodate, or ask accounting to change it in QB.\\nThe code will handle the information within it as long as there’s no weird typos\\nIf the vendor is supplying PDF invoices….\\nupdate cleaned_pdf_summaries! You’ll want to make sure that the fields are being pulled in correctly for the new invoice, so this step is',\n",
       "  '112\\n(4) Table aliases\\nWhen you have a query with multiple tables, use aliases to reference them in a concise, clean manner! Employ those aliases in the SELECT statements, so that the database and reader can tell exactly \\nwhich column belongs to which table.\\nIf you have columns with the same name across multiple tables, you will need to explicitly reference them with either the table name or alias unless you want an error.\\n \\nEfficiency\\n(1) Join tables using ON and JOIN\\nIt’s possible to join tables using a , but you should avoid that! Instead, use join as you specify what the tables are joined on and the type of join\\n(2) SELECT columns, not stars where possible\\nBe as specific as you can in grabbing columns. \\nIt’s okay to use * when first exploring tables or getting all data from another table, but if you do, you should LIMIT your results.\\n(3) Use EXISTS instead of IN (and NOT EXISTS instead of NOT IN)\\nIf you just need to verify the existence of a value in a table, prefer EXISTS to IN.',\n",
       "  \"Data Engineering Home7\\n      Datahub: Perpay's Data Catalog8\\n           Usage and Contribution9\\n           Rollout Plan12\\n           How To Guide13\\n           Current Documentation Additions14\\n           (WIP) DataHub Utility15\\n           DataHub Context and Learnings17\\n      Data Platform Infrastructure19\\n           Current Analytics Architecture20\\n           DE Group Objectives25\\n                wip - 202326\\n                2023 OKRs28\\n                2023 Q1 OKRs29\\n                2023 Q2 OKRs30\\n                2023 Q3 OKRs31\\n                2023 Q4 OKRs32\\n                Platform Improvements To-Do list33\\n                     Using the Unittest library for Data Quality Tests35\\n                     Platform and Infrastructure Performance Notes38\\n           Cost Breakdown40\\n           3rd Party Platforms44\\n           Third-Party API and Reporting Integrations46\\n                Automated Audience Management47\\n                DataX Reporting Service49\",\n",
       "  '125',\n",
       "  ...],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a174195-a535-4720-9d8e-24684b7fb31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-huggingface) (0.24.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-huggingface) (0.2.22)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-huggingface) (4.43.0)\n",
      "Requirement already satisfied: filelock in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.85)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hongkaiwang/.pyenv/versions/3.12.4/envs/venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cae76-a6ca-4302-8758-f7897395aab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd272-d156-43a4-acb5-500f1239e37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
